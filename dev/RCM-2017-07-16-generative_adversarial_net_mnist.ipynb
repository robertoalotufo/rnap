{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "from IPython.core.pylabtools import figsize\n",
    "from __future__ import print_function\n",
    "\n",
    "# import json\n",
    "# import matplotlib\n",
    "# s = json.load(open(\"../../styles/bmh_matplotlibrc.json\"))\n",
    "# matplotlib.rcParams.update(s)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import (SGD, \n",
    "                              RMSprop, \n",
    "                              Adam, \n",
    "                              Adadelta, \n",
    "                              Adagrad)\n",
    "\n",
    "sys.path.append('../src')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter)\n",
    "\n",
    "os.makedirs('../../models',exist_ok=True)\n",
    "np.set_printoptions(precision=3, linewidth=120, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend:        tensorflow\n",
      "Data format:    channels_last\n",
      "Available GPUS: []\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "K.set_floatx('float32')\n",
    "\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "print('Available GPUS:', get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print('X_train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               101136    \n",
      "=================================================================\n",
      "Total params: 114,064\n",
      "Trainable params: 114,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,609\n",
      "Trainable params: 100,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_10 (Model)             (None, 784)               114064    \n",
      "_________________________________________________________________\n",
      "model_11 (Model)             (None, 1)                 100609    \n",
      "=================================================================\n",
      "Total params: 214,673\n",
      "Trainable params: 214,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_trainable(net, val=True):\n",
    "    net.trainable = val\n",
    "    for layer in net.layers:\n",
    "        layer.trainable = val\n",
    "\n",
    "# Generator\n",
    "G_in = Input(shape=(100,))\n",
    "G_h1 = Dense(128, activation='relu')(G_in)\n",
    "G_out = Dense(784, activation='sigmoid')(G_h1)\n",
    "G = Model(G_in, G_out)\n",
    "G.summary()\n",
    "             \n",
    "# Discriminator\n",
    "D_in = Input(shape=(784,))\n",
    "D_h1 = Dense(128, activation='relu')(D_in)\n",
    "D_out = Dense(1, activation='sigmoid')(D_h1)\n",
    "D = Model(D_in, D_out)\n",
    "D.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "D.summary()\n",
    "\n",
    "N_in = Input(shape=(100,))\n",
    "N_img = G(N_in)\n",
    "N_out = D(N_img)\n",
    "GAN = Model(N_in, N_out)\n",
    "GAN.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "GAN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = dict(g=[], d=[])\n",
    "\n",
    "def trainIt(nepochs, batch_size=32, pint=10):\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "        \n",
    "        # Real images\n",
    "        real_images = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
    "        \n",
    "        # Fake images\n",
    "        fake_images = G.predict(np.random.uniform(0, 1, size=[batch_size, 100]))\n",
    "        \n",
    "        # Train discriminator these images\n",
    "        X = np.vstack((real_images, fake_images))\n",
    "        y = np.zeros([2*batch_size])\n",
    "        y[:batch_size] = 1\n",
    "        y[batch_size:] = 0\n",
    "        \n",
    "        make_trainable(D, True)\n",
    "        d_loss  = D.train_on_batch(X, y)\n",
    "        losses[\"d\"].append(d_loss)\n",
    "    \n",
    "        # Train Generator-Discriminator stack on input noise\n",
    "        noise_tr = np.random.uniform(0, 1, size=[batch_size, 100])\n",
    "        y2 = np.ones([batch_size])\n",
    "        \n",
    "        make_trainable(D, False)\n",
    "        g_loss = GAN.train_on_batch(noise_tr, y2)\n",
    "        losses[\"g\"].append(g_loss)\n",
    "        \n",
    "        if i%pint == 0:\n",
    "#             plot_loss(losses)\n",
    "#             plot_gen()\n",
    "            print('{:6d}: G_loss = {:.5f};  D_loss = {:.5f}'.format(i, g_loss, d_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0: G_loss = 0.21071;  D_loss = 1.01728\n",
      "  1000: G_loss = 0.21057;  D_loss = 1.01533\n",
      "  2000: G_loss = 0.21142;  D_loss = 1.03005\n",
      "  3000: G_loss = 0.21083;  D_loss = 1.03026\n",
      "  4000: G_loss = 0.21065;  D_loss = 1.01812\n",
      "  5000: G_loss = 0.21058;  D_loss = 1.01678\n",
      "  6000: G_loss = 0.21078;  D_loss = 1.01973\n",
      "  7000: G_loss = 0.21067;  D_loss = 1.01802\n",
      "  8000: G_loss = 0.21064;  D_loss = 1.01748\n",
      "  9000: G_loss = 0.21036;  D_loss = 1.01363\n",
      " 10000: G_loss = 0.21030;  D_loss = 1.01209\n",
      " 11000: G_loss = 0.21021;  D_loss = 1.01085\n",
      " 12000: G_loss = 0.21053;  D_loss = 1.01141\n",
      " 13000: G_loss = 0.20931;  D_loss = 0.99842\n",
      " 14000: G_loss = 0.20936;  D_loss = 0.99931\n",
      " 15000: G_loss = 0.20937;  D_loss = 0.99957\n",
      " 16000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 17000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 18000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 19000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 20000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 21000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 22000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 23000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 24000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 25000: G_loss = 0.20937;  D_loss = 0.99967\n",
      " 26000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 27000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 28000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 29000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 30000: G_loss = 0.20937;  D_loss = 0.99967\n",
      " 31000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 32000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 33000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 34000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 35000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 36000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 37000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 38000: G_loss = 0.20937;  D_loss = 0.99966\n",
      " 39000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 40000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 41000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 42000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 43000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 44000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 45000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 46000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 47000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 48000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 49000: G_loss = 0.20938;  D_loss = 0.99970\n",
      " 50000: G_loss = 0.20938;  D_loss = 0.99966\n",
      " 51000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 52000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 53000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 54000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 55000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 56000: G_loss = 0.20937;  D_loss = 0.99962\n",
      " 57000: G_loss = 0.20937;  D_loss = 0.99962\n",
      " 58000: G_loss = 0.20937;  D_loss = 0.99962\n",
      " 59000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 60000: G_loss = 0.20938;  D_loss = 0.99965\n",
      " 61000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 62000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 63000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 64000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 65000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 66000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 67000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 68000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 69000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 70000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 71000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 72000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 73000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 74000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 75000: G_loss = 0.20938;  D_loss = 0.99964\n",
      " 76000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 77000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 78000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 79000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 80000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 81000: G_loss = 0.20938;  D_loss = 0.99962\n",
      " 82000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 83000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 84000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 85000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 86000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 87000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 88000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 89000: G_loss = 0.20938;  D_loss = 0.99964\n",
      " 90000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 91000: G_loss = 0.20935;  D_loss = 0.99969\n",
      " 92000: G_loss = 0.20937;  D_loss = 0.99965\n",
      " 93000: G_loss = 0.20938;  D_loss = 0.99965\n",
      " 94000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 95000: G_loss = 0.20937;  D_loss = 0.99964\n",
      " 96000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 97000: G_loss = 0.20937;  D_loss = 0.99963\n",
      " 98000: G_loss = 0.20938;  D_loss = 0.99963\n",
      " 99000: G_loss = 0.20938;  D_loss = 0.99963\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainIt(100000, pint=1000)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAosAAAI2CAYAAAA4vsK8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsggAQPR/g5Jaik3YgxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f8d6f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_images = G.predict(np.random.uniform(0, 1, size=[20, 100])).reshape(20, 28, 28)\n",
    "\n",
    "figsize(11, 10)\n",
    "for i, img in enumerate(fake_images):\n",
    "    plot.subplot(4, 5, i+1)\n",
    "    plot.imshow(img, cmap=plot.cm.gray)\n",
    "    plot.axis('off')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
