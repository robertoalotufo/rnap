{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística usando Softmax NMIST no Keras\n",
    "\n",
    "Este notebook procura reproduzir o exemplo de Regressão Logística usando Softmax,\n",
    "já feito utilizando programação matricial. \n",
    "Neste notebook utiliza-se o Keras, utilizando o mesmo modelo e mesmos parâmetros\n",
    "\n",
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Keras also offers a collection of datasets that can be used to train and test the model. The MNIST set is a part of the available datasets and can be loaded as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist \n",
    "# the data, shuffled and split between train and test sets \n",
    "(X_train_orig, y_train), (X_test_orig, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping and normalizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n. amostras treinamento: 60000 n. classes: 10 n. de atributos: 784\n",
      "X_train_orig shape original: (60000, 28, 28)\n",
      "X_train novo shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "k = nb_classes = 10\n",
    "(n,height,width) = X_train_orig.shape  # deve ser 60000 amostras de 28 x 28 pixels\n",
    "m = input_dim = height * width\n",
    "print 'n. amostras treinamento:',n,'n. classes:',k,'n. de atributos:',m\n",
    "print 'X_train_orig shape original:', X_train_orig.shape\n",
    "X_train = X_train_orig.reshape(n, input_dim)/255. \n",
    "print 'X_train novo shape:', X_train.shape\n",
    "X_test = X_test_orig.reshape(10000, input_dim)/255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando apenas 500 para treinamento e 100 para teste\n",
    "\n",
    "Para ficar igual ao exemplo usando programação matricial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:500]\n",
    "y_train = y_train[:500]\n",
    "X_test  = X_test[:100]\n",
    "y_test  = y_test[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils \n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print y_train[0:5]\n",
    "print Y_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras.optimizers import SGD\n",
    "output_dim = nb_classes = 10 \n",
    "model = Sequential() \n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax')) \n",
    "batch_size = 500\n",
    "nb_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "\n",
    "Excelente blog sobre otimização:\n",
    "\n",
    "http://sebastianruder.com/optimizing-gradient-descent/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.380618659258\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr= 0.5, decay=0e-6, momentum=0., nesterov=False)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy') \n",
    "#model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=0, validation_data=(X_test, Y_test)) \n",
    "loss = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print 'loss:',loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7ad50a890>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+s4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7ad88e950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s     \n",
      "\n",
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(X,Y):\n",
    "    Y_hat = model.predict_classes(X)\n",
    "    accuracy = sum(Y_hat == Y)/(float(len(Y)))\n",
    "    return accuracy\n",
    "\n",
    "train_ac = getAccuracy(X_train,y_train)\n",
    "test_ac  = getAccuracy(X_test,y_test)\n",
    "print \n",
    "print 'Training Accuracy: ', train_ac\n",
    "print 'Test Accuracy: ', test_ac\n",
    "# usual 0.868 e 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/100 [========>.....................] - ETA: 0s(100,)\n",
      "y_hat:  [7 2 1 0 4 1 9 9 2 9 0 6 9 0 1 3 7 7 3 4]\n",
      "y_test: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1  2   3   4  5  6   7  8  9\n",
       "row_0                                  \n",
       "0      8   0  0   0   0  0  0   0  0  0\n",
       "1      0  14  0   0   0  0  0   0  0  0\n",
       "2      0   0  7   0   0  1  1   1  0  0\n",
       "3      0   0  1  10   0  1  0   0  0  0\n",
       "4      0   0  0   0  12  1  0   0  0  0\n",
       "5      0   0  0   1   0  3  0   0  1  0\n",
       "6      0   0  0   0   0  0  9   0  0  0\n",
       "7      0   0  0   0   0  1  0  12  0  2\n",
       "8      0   0  0   0   0  0  0   0  1  0\n",
       "9      0   0  0   0   2  0  0   2  0  9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model.predict_classes(X_test)\n",
    "print y_hat.shape\n",
    "print 'y_hat: ',y_hat[:20]\n",
    "print 'y_test:',y_test[:20]\n",
    "print y_test.shape\n",
    "pd.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
