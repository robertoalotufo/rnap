{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import (SGD, \n",
    "                              RMSprop, \n",
    "                              Adam, \n",
    "                              Adadelta, \n",
    "                              Adagrad)\n",
    "\n",
    "sys.path.append('../src')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter)\n",
    "\n",
    "import os\n",
    "os.makedirs('../../models',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend:        tensorflow\n",
      "Data format:    channels_last\n",
      "Available GPUS: ['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "# K.set_image_data_format('channels_first')\n",
    "K.set_floatx('float32')\n",
    "\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "print('Available GPUS:', get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model_name, model_build_fun, model_build_args, Xtra, ytra, Xval=None, yval=None, \n",
    "                  reset=False, opt='rmsprop', batch_size=60, nepochs=50, patience=50, \n",
    "                  nr_seed=20170512, ploss=1.0):\n",
    "\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    nr.seed = nr_seed    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        print(\"[INFO] creating model...\")\n",
    "        model = model_build_fun(*model_build_args)\n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot)\n",
    "\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])    \n",
    "\n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "\n",
    "    model.summary()\n",
    "    past_epochs = cb.get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs ...\".format(tr_epochs))\n",
    "    try:\n",
    "        h = model.fit(Xtra, ytra, batch_size=60, epochs=tr_epochs, verbose=vv, \n",
    "                      validation_data=(Xval, yval), callbacks=[cb])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return model, cb\n",
    "\n",
    "\n",
    "def test_network(model_name, Xtest, ytest, batch_size=64):\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    print('Model from epoch {}'.format(histo.best_epoch))\n",
    "    print(\"[INFO] evaluating in the test data set ...\")\n",
    "    loss, accuracy = model.evaluate(Xtest, ytest, batch_size=batch_size, verbose=1)\n",
    "    print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '../../datasets/'\n",
    "GLOVE_DIR     = BASE_DIR + 'glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '20_newsgroup/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS        = 20000\n",
    "EMBEDDING_DIM       = 100\n",
    "VALIDATION_SPLIT    = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []         # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []        # list of label ids\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000) 19999\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')\n",
    "\n",
    "targets = to_categorical(np.asarray(labels))\n",
    "\n",
    "# data = data[:5000]\n",
    "# labels = labels[:5000]\n",
    "\n",
    "print('Shape of data tensor:', data.shape, data.max())\n",
    "print('Shape of label tensor:', targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq. length min/max/mean/median: 0 27442 266.008101215 150.0\n",
      "Word indexes min/max: 0 19999\n",
      "\n",
      "  881: in article healta 183 saturn wwc edu healta saturn wwc edu tammy r healy writes i was asked to post list of the sda church's basic beliefs the sda church has always been reluctant to a creed in the usual sense of word however the powers that be in the\n",
      "\n",
      " 1929: we are looking for gl source code which was developed by silicon graphics sgi we would like to compile it on sun and hp 9000 if there is anyone already supporting gl on hp and sun please respond also please respond if anyone knows where the source code is available\n",
      "\n",
      " 1230: howdy all i was wondering if people could e mail me their opinions on the various graphics viewers available for ms windows 3 x i'm working on a project to set up our scanner and write documentation on how to use it and it would be nice to have a\n",
      "\n",
      " 1643: in article dundee ac uk mcs dundee ac uk writes hi the subject says it all is there a pd viewer for gl files for x thanks have you tried it's out there on several ftp sites not sure which but archie can find it i'm sure it works ok\n",
      "\n",
      " 4350: the is a 3 4 week but they are shipping the opinions expressed are not necessarily those of the university of north carolina at chapel hill the campus office for information technology or the experimental bulletin board service internet launchpad unc edu or 152 2 22 80\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9462265400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = np.array([len(seq) for seq in sequences])\n",
    "print('Seq. length min/max/mean/median:', nn.min(), nn.max(), nn.mean(), np.median(nn))\n",
    "\n",
    "print('Word indexes min/max:', data.min(), data.max(), end='\\n\\n')\n",
    "\n",
    "plot.hist(nn, bins=500)\n",
    "plot.xlim(0, 1000)\n",
    "\n",
    "i2w = dict([(v, k) for k, v in word_index.items()])\n",
    "\n",
    "for i in nr.randint(5000, size=5):\n",
    "    seq = data[i]\n",
    "    print('{:5d}: {}\\n'.format(i, ' '.join([i2w[x] for x in seq[:50] if x > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15998, 1000) (15998, 20)\n",
      "(3999, 1000) (3999, 20)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "targets = targets[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = targets[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = targets[-num_validation_samples:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix: (20000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix: ', end='')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= MAX_NB_WORDS:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "i2w = dict([(v, k) for k, v in word_index.items()])\n",
    "\n",
    "for i in range(1, MAX_NB_WORDS):\n",
    "    word = i2w[i]\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "embeddings_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = embedding_matrix.sum(1)\n",
    "aa[aa==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_name = '../../models/word_embeddings_1'\n",
    "\n",
    "def build_net(embedding_matrix=None, input_len=None, trainable=False):\n",
    "    num_words, embedding_dim = embedding_matrix.shape\n",
    "    \n",
    "    seq_input = Input(shape=(input_len,), dtype='int32')\n",
    "    embedding_layer = Embedding(num_words, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=input_len, trainable=trainable)(seq_input)\n",
    "\n",
    "    x = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "    model = Model(seq_input, preds)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 97, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 93, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               753792    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,984,596\n",
      "Trainable params: 984,596\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "[INFO] training for 93 epochs ...\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/93\n",
      "9s - loss: 1.9500 - acc: 0.3998 - val_loss: 1.5887 - val_acc: 0.4864\n",
      "Epoch 2/93\n",
      "8s - loss: 1.4873 - acc: 0.5140 - val_loss: 1.4925 - val_acc: 0.5119\n",
      "Epoch 3/93\n",
      "8s - loss: 1.2433 - acc: 0.5883 - val_loss: 1.3548 - val_acc: 0.5699\n",
      "Epoch 4/93\n",
      "9s - loss: 1.0294 - acc: 0.6521 - val_loss: 1.4137 - val_acc: 0.5624\n",
      "Epoch 5/93\n",
      "9s - loss: 0.8642 - acc: 0.7002 - val_loss: 1.4460 - val_acc: 0.5826\n",
      "Epoch 6/93\n",
      "9s - loss: 0.7410 - acc: 0.7418 - val_loss: 1.4430 - val_acc: 0.5901\n",
      "Epoch 7/93\n",
      "9s - loss: 0.6413 - acc: 0.7762 - val_loss: 1.4921 - val_acc: 0.5969\n",
      "Epoch 8/93\n",
      "9s - loss: 0.5542 - acc: 0.8072 - val_loss: 1.5512 - val_acc: 0.5981\n",
      "Epoch 9/93\n",
      "9s - loss: 0.4817 - acc: 0.8332 - val_loss: 1.6628 - val_acc: 0.5849\n",
      "Epoch 10/93\n",
      "9s - loss: 0.4226 - acc: 0.8565 - val_loss: 1.6491 - val_acc: 0.6072\n",
      "Epoch 11/93\n",
      "9s - loss: 0.3631 - acc: 0.8742 - val_loss: 1.8488 - val_acc: 0.6074\n"
     ]
    }
   ],
   "source": [
    "build_params = (embedding_matrix, MAX_SEQUENCE_LENGTH, False)\n",
    "fit_params = {\n",
    "    'opt':             'adam',     #SGD(lr=0.01, momentum=0.9, nesterov=True), \n",
    "    'batch_size':      64, \n",
    "    'nepochs':         100, \n",
    "    'patience':        10,\n",
    "    'nr_seed':         20170601,\n",
    "    'ploss':           0.0,\n",
    "    'reset':           False,\n",
    "}\n",
    "\n",
    "train_network(model_name, build_net, build_params, x_train, y_train, x_val, y_val, **fit_params);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from epoch 6\n",
      "[INFO] evaluating in the test data set ...\n",
      "3999/3999 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "[INFO] accuracy on the test data set: 32.26% [3.01098]\n"
     ]
    }
   ],
   "source": [
    "test_network(model_name, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
