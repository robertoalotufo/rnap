{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word2Vec</a></div><div class=\"lev2 toc-item\"><a href=\"#Preâmbulo\" data-toc-modified-id=\"Preâmbulo-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preâmbulo</a></div><div class=\"lev2 toc-item\"><a href=\"#Preparando-o-dataset\" data-toc-modified-id=\"Preparando-o-dataset-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Preparando o dataset</a></div><div class=\"lev3 toc-item\"><a href=\"#Buscando-o-texto-dos-livros-e-definindo-os-rótulos\" data-toc-modified-id=\"Buscando-o-texto-dos-livros-e-definindo-os-rótulos-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Buscando o texto dos livros e definindo os rótulos</a></div><div class=\"lev3 toc-item\"><a href=\"#Representando-as-palavras-através-de-índices-inteiros\" data-toc-modified-id=\"Representando-as-palavras-através-de-índices-inteiros-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Representando as palavras através de índices inteiros</a></div><div class=\"lev3 toc-item\"><a href=\"#Palavras-características-de-cada-livro\" data-toc-modified-id=\"Palavras-características-de-cada-livro-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Palavras características de cada livro</a></div><div class=\"lev2 toc-item\"><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2Vec</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preâmbulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import (SGD, \n",
    "                              RMSprop, \n",
    "                              Adam, \n",
    "                              Adadelta, \n",
    "                              Adagrad)\n",
    "\n",
    "sys.path.append('../src')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter)\n",
    "\n",
    "os.makedirs('../../models',exist_ok=True)\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend:        tensorflow\n",
      "Data format:    channels_first\n",
      "Available GPUS: []\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "# K.set_image_data_format('channels_first')\n",
    "K.set_floatx('float32')\n",
    "\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "print('Available GPUS:', get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando o texto dos livros e definindo os rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  87788  Fernando_Sabino    O_Bom_Ladrão\n",
      " 511681  Fernando_Sabino    O_Encontro_Marcado\n",
      " 328288  Fernando_Sabino    O_Grande_Mentecapto\n",
      " 180313  Fernando_Sabino    O_Menino_no_Espelho\n",
      " 427711  Jorge_Amado        Capitães_de_Areia\n",
      "1030735  Jorge_Amado        Dona_flor_seus_dois_maridos\n",
      " 828417  Jorge_Amado        Gabriela\n",
      "1001226  Jorge_Amado        Tereza_Batista_Cansada_de_Guerra\n",
      " 372459  Machado_de_Assis   Dom_Casmurro\n",
      " 411043  Machado_de_Assis   Esaú_e_Jacó\n",
      " 352965  Machado_de_Assis   Memórias_Póstumas_de_Brás_Cubas\n",
      " 443778  Machado_de_Assis   Quincas_Borba\n",
      "\n",
      "Labels:\n",
      "     0: Machado_de_Assis\n",
      "     1: Fernando_Sabino\n",
      "     2: Jorge_Amado\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../datasets/livros/'\n",
    "\n",
    "autores = [\n",
    "    'Fernando_Sabino', \n",
    "    'Jorge_Amado',\n",
    "    'Machado_de_Assis',\n",
    "]\n",
    "\n",
    "book_text = []\n",
    "book_author = []\n",
    "book_title = []\n",
    "for aut in autores:\n",
    "    for fn in glob.glob(data_dir + aut + '*.txt'):\n",
    "        author, book = os.path.basename(fn).split('__')\n",
    "        txt = open(fn).read().replace('\\x97', '')\n",
    "        book_text.append(txt)\n",
    "        book_author.append(author)\n",
    "        book_title.append(book[:-4])\n",
    "        print('{:7d}  {:18s} {}'.format(len(txt), author, book[:-4]))\n",
    "\n",
    "author_list = list(set(book_author))\n",
    "n_labels = len(author_list)\n",
    "n_books = len(book_title)\n",
    "book_label = [author_list.index(a) for a in book_author]\n",
    "print('\\nLabels:')\n",
    "for i, autor in enumerate(author_list):\n",
    "    print('    {:2d}: {}'.format(i, autor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representando as palavras através de índices inteiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51557 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(book_text)\n",
    "sequences = tokenizer.texts_to_sequences(book_text)\n",
    "\n",
    "w2i = tokenizer.word_index\n",
    "i2w = dict([(v, k) for k, v in w2i.items()])\n",
    "\n",
    "i2w_vec = np.array([i2w[i+1] for i in range(len(i2w))])\n",
    "\n",
    "print('Found %s unique tokens.' % len(w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fernando Sabino: O Bom Ladrão -- 15139 palavras\n",
      "tentação de abraçá la esquecer tudo que havia passado subir também mas alguma coisa me dizia que o meu lugar era embaixo que eu era apenas uma testemunha um espectador o lado passivo do seu mistério doze por essa ocasião voltei a minas passei uns dias na casa de meu pai ele andava doente e cada vez mais solitário na sua viuvez minha mãe morreu quando nasci e ele acabou se casando com minha madrinha que me criou ao perder também a segunda mulher abandonou a carreira de advogado do interior mandou me estudar no rio e desgostoso veio terminar\n",
      "\n",
      "Fernando Sabino: O Encontro Marcado -- 87226 palavras\n",
      "algumas mesas em torno da pista de danças uma orquestra homens bebendo cerveja mulheres espalhadas pela sala não tinha nada de mais como numa festa qualquer aquelas eram as mulheres as famosas mulheres da “ zona ” seriam todas prostitutas nenhu ma delas estava nua nem sumariamente vestida procediam como qualquer mulher procederia apenas quando dançavam requebravam se apertando o parceiro dando passos ousados arranjavam fregueses certamente — quanto cobrariam moravam em pensões por perto talvez algumas estivessem ali só para se divertir a semi escuridão do ambiente não permitia ver bem suas feições arranjadas de maneira a parecerem be\n",
      "\n",
      "Fernando Sabino: O Grande Mentecapto -- 56091 palavras\n",
      "mais — era o que lhe perguntavam ao que viramundo respondia — apenas movimentar tomar a trocar por mais aves e ovos o candidato oficial um velho professor de nome praxedes borba gato natural ninguém sabia de onde homem sisudo que arrotava sabenças mas cujo nome se deslustrava na condição de pau mandado do governador ladisbão começou a ficar apreensivo com aquela situação não podia deixar de tomar conhecimento da pândega que empolgava toda a cidade e numa de suas manifestações públicas que em geral eram bem privadas verberou a atitude das duas correntes políticas locais tradicionalmente inimigas nunca se\n",
      "\n",
      "Fernando Sabino: O Menino no Espelho -- 31938 palavras\n",
      "barriga olhos arregalados de repente como se fosse de mola dava um salto no ar em direção à cara de um todos se espalhavam assustados — cuidado que se ela mija no seu olho você fica cego — é só sapo que faz isso perereca não mija não se não fosse o tição conter com mão firme o barbante que a prendia pela cintura ninguém segurava a perereca e ele a guardava no bolso do uniforme onde ela ficava se mexendo a idéia era botá la dentro da bolsa que dona risoleta deixava em cima da mesa enquanto dava aula\n",
      "\n",
      "Jorge Amado: Capitães de Areia -- 78337 palavras\n",
      "ouviu o choro de zé fuinha joão grande falava – eu sempre tive contigo bala sou teu amigo mas ela é uma menina fui eu e professor que trouxe ela eu s ou teu amigo mas se tu vier eu te mato é uma menina ninguém faz mal a ela – a gente te derruba e depois – disse volta seca – cala a boca gritou pedro bala joão grande continuou – o pai dela a mãe dela morreu de bexiga a gente encontrou el a não tinha onde dormir a gente trouxe ela não é uma puta é uma\n",
      "\n",
      "Jorge Amado: Dona flor seus dois maridos -- 175457 palavras\n",
      "às suas custas eis porque dona norma convocou não só dona gisa letrada e sabichona amiga do peito como também quis ouvir seu zé sampaio e nele se apoiar pensara de começo em tia lita e tio pôrto encontrando se em nazareth das farinhas ou no rio e mãe e os demais parentes de dona flor mas convieram ela e a viúva na inutilidade da presença dos bons velhos nos debates preliminares do caso se chegassem ao mom ento solene do noivado ai sim convocariam tia lita em seu jardim tio pôrto em suas paisagens coloridas para ouvirem do pretendente\n",
      "\n",
      "Jorge Amado: Gabriela -- 140073 palavras\n",
      "de conforto mas rico de arte gabriela sorria gostava de ouvir mas na tarde em que melk espancara malvina josué vira o rosto triste glória triste pela moça surrada triste por josué abandonado triste por mesma em sua solidão renovada escreveu lhe em seguida um bilhete passou junto à janela e ali o deixou algumas noites depois entrava ele quando o silêncio envolvia a praça e últimos notívagos haviam se recolhido pela pesada porta entreaberta uma boca esmagou sua boca uns braços cercaram seus ombros magros arrastar am no para dentro esqueceu malvina seu amor eterno imortal quando a aurora\n",
      "\n",
      "Jorge Amado: Tereza Batista Cansada de Guerra -- 174570 palavras\n",
      "gostar é fácil acontece quando menos se espera um olhar uma palavra um gesto e o fogo lavra queimando peito e boca difícil é esquecer a saudade consome o vivente amor não é espinho que se arranca tumor que se rasga é dor rebelde e pertinaz matando por dentro lá vai tereza envolta na manta espa nhola no rumo de casa difícil de lágrimas em vez de chorar fica de olhos secos ardidos 32 jorge amado alguém marcha em sua direção com pressa tereza imagina tratar se de homem à cata de mulher dama para conduzi la ao vaticano pela\n",
      "\n",
      "Machado de Assis: Dom Casmurro -- 66969 palavras\n",
      "a mão conservei me à porta a ver se ao longe ainda olharia para trás mas não olhou que amigo é esse tamanho perguntou alguém de uma janela ao pé não é preciso dizer que era capitu são coisas que se adivinham na vida como nos livros sejam romances sejam histórias verdadeiras era capitu que nos espreitara desde algum tempo por dentro da veneziana e agora abrira inteiramente a janela e aparecera viu as nossas despedidas tão rasgadas e afetuosas e quis saber quem era que me merecia tanto é o escobar disse eu indo pôr me embaixo da janela\n",
      "\n",
      "Machado de Assis: Esaú e Jacó -- 72640 palavras\n",
      "passou imediatamente o que ele fez foi colocar a coroa que levava no lado correspondente aos pés da defunta para não a irmanar com a outra que estava do lado da cabeça não viu não adivinhou sequer que pedro naturalmente pararia um instante para voltar a cara e mandar um derradeiro olhar à moça enterrada assim foi mas quando pedro deu com o irmão no mesmo lugar que ele os olhos no chão teve também o seu impulso de ir buscá lo e trazê lo daquela cova sagrada preferiu esconder se e esperar os gestos de piedade quaisquer que fossem\n",
      "\n",
      "Machado de Assis: Memórias Póstumas de Brás Cubas -- 61492 palavras\n",
      "nu de habitações salvo o velho palacete do alto onde era a capela pois um domingo ao descer com nhã loló pelo braço não sei que fenômeno se deu que fui deixando aqui dois anos ali quatro logo adiante cinco de maneira que quando cheguei abaixo estava com vinte anos apenas tão lépidos como tinham sido agora se querem saber em que circunstância se deu o fenômeno basta lhes ler este capítulo até o fim vínhamos da missa ela o pai e eu no meio do morro achamos um grupo de homens damasceno que vinha ao pé de nós percebeu\n",
      "\n",
      "Machado de Assis: Quincas Borba -- 76423 palavras\n",
      "pelos moradores das casas porque a gesticulação diminuía ou mudava de feitio não se dirigia à parede à suposta imperatriz mas era ainda imperador caminhava parava murmurava sem grandes gestos sonhando sempre sempre sempre envolvido naquele véu através do qual todas as coisas eram outras contrárias e melhores cada lampião tinha um aspecto de cada esquina uma feição de reposteiro rubião seguia direito à sala do trono para receber um embaixador qualquer mas o paço era interminável cumpria atravessar muitas salas e galerias verdade é que sobre tapetes e por entre altos e robustos das gentes que o viam e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_sequences = []\n",
    "for i, seq in enumerate(sequences):\n",
    "    k = nr.randint(len(seq) - 100)\n",
    "    word_sequences.append([i2w[x] for x in seq])\n",
    "    print('{}: {} -- {} palavras'.format(book_author[i], book_title[i], len(seq)).replace('_', ' '))\n",
    "    print(' '.join([i2w[x] for x in seq[k:k+100]]), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palavras características de cada livro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['que' 'de' 'garcia' 'brígida' 'norberto' 'abotoaduras' '–' 'isabel']\n",
      " ['eduardo' 'marciano' '“' 'vítor' 'gerlane' 'mauro' 'térsio' '—']\n",
      " ['tibério' 'dionísio' 'barbeca' 'ladisbão' '—' 'mentecapto' 'geraldo' 'viramundo']\n",
      " ['pastoff' 'alzira' 'odnanref' 'birica' 'anairam' 'hindemburgo' 'gerson' '—']\n",
      " ['ester' 'dalva' 'barandão' 'almiro' 'dora' '–' 'pirulito' 'trapiche']\n",
      " ['marilda' 'dinorá' 'teodoro' 'pelancchi' 'gisa' 'mirandão' 'rozilda' 'vadinho']\n",
      " ['arminda' 'malvina' 'mundinho' 'amâncio' 'fulgêncio' 'tonico' 'gabriela' 'nacib']\n",
      " ['libório' 'vavá' 'almério' 'januário' '—' 'dóris' 'justiniano' 'tereza']\n",
      " ['protonotário' 'bentinho' 'cabral' 'sancha' 'pádua' 'justina' 'capitu' 'escobar']\n",
      " ['cabocla' 'perpétua' 'excia' 'custódio' 'aires' 'natividade' 'cláudia' 'flora']\n",
      " ['loló' 'eusébia' 'cubas' 'quincas' 'sabina' 'cotrim' 'marcela' 'virgília']\n",
      " ['cristiano' 'fernanda' 'quincas' 'tonica' 'camacho' 'benedita' 'sofia' 'rubião']]\n"
     ]
    }
   ],
   "source": [
    "tfidf = tokenizer.sequences_to_matrix(sequences, mode='tfidf')\n",
    "ww = np.argsort(tfidf, axis=1)[:, -8:]\n",
    "print(i2w_vec[ww-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-09 07:32:46,899 : INFO : collecting all words and their counts\n",
      "2017-06-09 07:32:46,901 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-09 07:32:47,105 : INFO : collected 49999 word types from a corpus of 1036355 raw words and 12 sentences\n",
      "2017-06-09 07:32:47,106 : INFO : Loading a fresh vocabulary\n",
      "2017-06-09 07:32:47,192 : INFO : min_count=3 retains 21357 unique words (42% of original 49999, drops 28642)\n",
      "2017-06-09 07:32:47,194 : INFO : min_count=3 leaves 1000153 word corpus (96% of original 1036355, drops 36202)\n",
      "2017-06-09 07:32:47,316 : INFO : deleting the raw counts dictionary of 49999 items\n",
      "2017-06-09 07:32:47,318 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-06-09 07:32:47,320 : INFO : downsampling leaves estimated 763603 word corpus (76.3% of prior 1000153)\n",
      "2017-06-09 07:32:47,321 : INFO : estimated required memory for 21357 words and 100 dimensions: 27764100 bytes\n",
      "2017-06-09 07:32:47,415 : INFO : resetting layer weights\n",
      "2017-06-09 07:32:47,906 : INFO : training model with 3 workers on 21357 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-09 07:32:48,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-09 07:32:48,452 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-09 07:32:48,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-09 07:32:48,459 : INFO : training on 5181775 raw words (600000 effective words) took 0.5s, 1092340 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model = Word2Vec(word_sequences, min_count=3, size=100, window=5, max_vocab_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-09 07:32:50,190 : INFO : saving Word2Vec object under ../../models/wv_pt_br, separately None\n",
      "2017-06-09 07:32:50,192 : INFO : not storing attribute syn0norm\n",
      "2017-06-09 07:32:50,193 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bom', 'ladrão', '“não', 'adianta', 'saber', 'de', 'quem', 'é', 'a', 'culpa']\n",
      "[-0.035 -0.162 -0.206  0.009 -0.146  0.403 -0.261 -0.029  0.106  0.183  0.669 -0.725  0.572  0.438 -0.098 -0.133 -0.033\n",
      " -0.432  0.291 -0.258 -0.44   0.217  0.073  0.19  -0.111 -0.448 -0.292  0.1    0.107 -0.139  0.214 -0.152  0.027 -0.142\n",
      "  0.048  0.239  0.152  0.415  0.082 -0.295  0.215  0.024  0.655  0.379 -0.19   0.353  0.588 -0.463  0.2    0.201 -0.443\n",
      "  1.055  0.009  0.123  0.614  0.07  -0.244  0.162  0.262  0.294 -0.051  0.382 -0.063  0.211  0.019  0.033 -0.57   0.255\n",
      "  0.134  0.022  0.199  0.557  0.116  0.154 -0.397 -0.187  0.049 -0.062 -0.247  0.064  0.341 -0.022  0.474  0.044  0.52\n",
      "  0.288 -0.163 -0.186 -0.135  0.493 -0.373 -0.2   -0.443 -0.073 -0.409 -0.124 -0.448 -0.03   0.389 -0.448]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-09 07:32:50,725 : INFO : saved ../../models/wv_pt_br\n"
     ]
    }
   ],
   "source": [
    "model_fn = '../../models/wv_pt_br'\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n",
    "print(model.wv['bom'])\n",
    "len(vocab)\n",
    "\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-09 07:33:32,725 : INFO : loading Word2Vec object from ../../models/wv_pt_br\n",
      "2017-06-09 07:33:32,931 : INFO : loading wv recursively from ../../models/wv_pt_br.wv.* with mmap=None\n",
      "2017-06-09 07:33:32,932 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-06-09 07:33:32,933 : INFO : setting ignored attribute cum_table to None\n",
      "2017-06-09 07:33:32,934 : INFO : loaded ../../models/wv_pt_br\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bom', 'ladrão', '“não', 'adianta', 'saber', 'de', 'quem', 'é', 'a', 'culpa']\n",
      "[-0.035 -0.162 -0.206  0.009 -0.146  0.403 -0.261 -0.029  0.106  0.183  0.669 -0.725  0.572  0.438 -0.098 -0.133 -0.033\n",
      " -0.432  0.291 -0.258 -0.44   0.217  0.073  0.19  -0.111 -0.448 -0.292  0.1    0.107 -0.139  0.214 -0.152  0.027 -0.142\n",
      "  0.048  0.239  0.152  0.415  0.082 -0.295  0.215  0.024  0.655  0.379 -0.19   0.353  0.588 -0.463  0.2    0.201 -0.443\n",
      "  1.055  0.009  0.123  0.614  0.07  -0.244  0.162  0.262  0.294 -0.051  0.382 -0.063  0.211  0.019  0.033 -0.57   0.255\n",
      "  0.134  0.022  0.199  0.557  0.116  0.154 -0.397 -0.187  0.049 -0.062 -0.247  0.064  0.341 -0.022  0.474  0.044  0.52\n",
      "  0.288 -0.163 -0.186 -0.135  0.493 -0.373 -0.2   -0.443 -0.073 -0.409 -0.124 -0.448 -0.03   0.389 -0.448]\n"
     ]
    }
   ],
   "source": [
    "model_1 = Word2Vec.load(model_fn)\n",
    "\n",
    "vocab_1 = list(model_1.wv.vocab.keys())\n",
    "print(vocab_1[:10])\n",
    "print(model_1.wv['bom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
