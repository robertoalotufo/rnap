{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:00:45.270833",
     "start_time": "2017-08-12T21:00:45.254573"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "import keras.layers\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "sys.path.append('../src')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:53:26.098159",
     "start_time": "2017-08-12T20:53:25.935040"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend:        tensorflow\n",
      "Data format:    channels_last\n",
      "Available GPUS: []\n",
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_floatx('float32')\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "print('Available GPUS:', get_available_gpus())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:53:27.963722",
     "start_time": "2017-08-12T20:53:27.904219"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model, X_train, y_train, Xval, yval, \n",
    "                  model_name = None,\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  opt='rmsprop', batch_size=60, nepochs=100, patience=10, nr_seed=20170522, \n",
    "                  reset=False, ploss=1.0):\n",
    "\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot)\n",
    "        \n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "        cb.patience = patience\n",
    "\n",
    "    past_epochs = cb.get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        import matplotlib.pyplot as plot\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs...\".format(tr_epochs))\n",
    "    try:\n",
    "        h = model.fit(X_train, y_train, batch_size=60, epochs=tr_epochs, verbose=0, \n",
    "                      validation_data=(Xval, yval), callbacks=[cb])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return model, cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:40:30.517622",
     "start_time": "2017-08-12T21:40:30.007078"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (50000, 32, 32, 3) (50000, 1)\n",
      "test: (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#y_train = y_train.ravel()\n",
    "#y_test = y_test.ravel()\n",
    "print('train:',X_train.shape, y_train.shape)\n",
    "print('test:',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:40:31.395331",
     "start_time": "2017-08-12T21:40:31.383688"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n classes: 10\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(set(y_train.ravel()))\n",
    "print('n classes:', nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:40:33.484841",
     "start_time": "2017-08-12T21:40:32.744720"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:40:34.400039",
     "start_time": "2017-08-12T21:40:34.393232"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing = True\n",
    "if testing:\n",
    "    n_samples = 500\n",
    "    X_train = X_train[:n_samples]\n",
    "    y_train = y_train[:n_samples]\n",
    "    n_samples_test = 100\n",
    "    X_test = X_test[:n_samples_test]\n",
    "    y_test = y_test[:n_samples_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:28:04.968519",
     "start_time": "2017-08-12T21:28:04.961673"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_oh = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_oh = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a Keras implementation of Huang et al.'s [DenseNet](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "Our motivation behind studying DenseNet is because of how well it works with limited data.\n",
    "\n",
    "DenseNet beats state-of-the-art results on CIFAR-10/CIFAR-100 w/ and w/o data augmentation, but the performance increase is most pronounced w/o data augmentation.\n",
    "\n",
    "Compare to FractalNet, state-of-the-art on both datasets:\n",
    "* CIFAR-10: ~ 30 % performance increase w/ DenseNet\n",
    "* CIFAR-100: ~ 30 % performance increase w/ DenseNet\n",
    "\n",
    "That increase is motivation enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is a DenseNet?\n",
    "\n",
    "Put simply, DenseNet is a Resnet where we replace addition with concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in broad terms, a Resnet is a Convnet that uses residual block structures.\n",
    "\n",
    "These \"blocks\" work as follows:\n",
    "* Let L<sub>t</sub> be the input layer to block\n",
    "* Perform conv layer transformations/activations on L<sub>t</sub>, denote by f(<sub>t</sub>)\n",
    "* Call output layer of block L<sub>t+1</sub>\n",
    "* Define L<sub>t+1</sub> = f(L<sub>t</sub>)+ L<sub>t</sub>  \n",
    "    * That is, total output is the conv layer outputs plus the original input\n",
    "* We call residual block b.c. f(L<sub>t</sub>)=L<sub>t+1</sub> - L<sub>t</sub>, the residual\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the difference w/ DenseNet is instead of adding L<sub>t</sub> to L<sub>t+1</sub>, it is being concatenated.\n",
    "\n",
    "As with Resnet, DenseNet consists of multiple blocks.\n",
    "Therefore, there is a recursive relationship across blocks:\n",
    "* Block B<sub>i</sub> takes as input the ouput of block B<sub>i-1</sub> concatenated with the input of B<sub>i-1</sub>\n",
    "* The input to B<sub>i-1</sub> is the ouput of block B<sub>i-2</sub> concatenated with the input of B<sub>i-2</sub>\n",
    "* So on and so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of filters added to each layer needs to be monitored, given that the input space for each block keeps growing.\n",
    "\n",
    "Huang et al. calls the # of filters added at each layer the *growth rate*, and appropriately denotes this number with the related letter *k*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some helper functions for piecing together our network using Keras' Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These components should all be familiar to you:\n",
    "* Relu activation\n",
    "* Dropout regularization\n",
    "* Batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:59:14.089819",
     "start_time": "2017-08-12T20:59:14.083570"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x): return Activation('relu')(x)\n",
    "def dropout(x, p): return Dropout(p)(x) if p else x\n",
    "def bn(x): return BatchNormalization(axis=-1)(x)\n",
    "def relu_bn(x): return relu(bn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer:\n",
    "* L2 Regularization\n",
    "* 'same' border mode returns same width/height\n",
    "* Pass output through Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:59:15.285989",
     "start_time": "2017-08-12T20:59:15.280389"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, nf, sz, wd, p):\n",
    "    x = Conv2D(nf, (sz, sz), kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(wd))(x)\n",
    "    return dropout(x,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ConvBlock as sequence:\n",
    "* Batchnorm\n",
    "* ReLU Activation\n",
    "* Conv layer (conv w/ Dropout)\n",
    "\n",
    "The authors also use something called a *bottleneck* layer to reduce dimensionality of inputs. \n",
    "\n",
    "Recall that the filter space dimensionality grows at each block. The input dimensionality will determine the dimensionality of your convolution weight matrices, i.e. # of parameters.\n",
    "\n",
    "At size 3x3 or larger, convolutions can become extremely costly and # of parameters can increase quickly as a function of the input feature (filter) space. Therefore, a smart approach is to reduce dimensionality of filters by using a 1x1 convolution w/ smaller # of filters before the larger convolution.\n",
    "\n",
    "Bottleneck consists of:\n",
    "* 1x1 conv\n",
    "* Compress # of filters into growth factor `nf` * 4\n",
    "* Batchnorm -> ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:59:16.378290",
     "start_time": "2017-08-12T20:59:16.371212"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, nf, bottleneck=False, p=None, wd=0):\n",
    "    x = relu_bn(x)\n",
    "    if bottleneck: x = relu_bn(conv(x, nf * 4, 1, wd, p))\n",
    "    return conv(x, nf, 3, wd, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the dense block:\n",
    "* Take given input `x`\n",
    "* Pass through a conv block for output `b`\n",
    "* Concatenate input `x` and conv block output `b`\n",
    "* Set concatenation as new input `x` for next block\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:59:17.485694",
     "start_time": "2017-08-12T20:59:17.478080"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block(x, nb_layers, growth_rate, bottleneck=False, p=None, wd=0):\n",
    "    if bottleneck: nb_layers //= 2\n",
    "    for i in range(nb_layers):\n",
    "        b = conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        x = keras.layers.concatenate([x,b],axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As typical for CV architectures, we'll do some pooling after computation.\n",
    "\n",
    "We'll define this unit as the transition block, and we'll put one between each dense block.\n",
    "\n",
    "Aside from BN -> ReLU and Average Pooling, there is also an option for filter *compression* in this block. This is simply feature reduction via 1x1 conv as discussed before, where the new # of filters is a percentage of the incoming # of filters.\n",
    "\n",
    "Together with bottleneck, compression has been shown to improve performance and computational efficiency of DenseNet architectures. (the authors call this DenseNet-BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T20:59:18.590552",
     "start_time": "2017-08-12T20:59:18.584426"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_block(x, compression=1.0, p=None, wd=0):\n",
    "    nf = int(x.get_shape().as_list()[-1] * compression)\n",
    "    x = relu_bn(x)\n",
    "    x = conv(x, nf, 1, wd, p)\n",
    "    return AveragePooling2D((2, 2), strides=(2, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the DenseNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined all the building blocks (literally) to put together a DenseNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nb_classes: number of classes\n",
    "- img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
    "- depth: total number of layers \n",
    "    - Includes 4 extra non-block layers\n",
    "        - 1 input conv, 3 output layers\n",
    "- nb_block: number of dense blocks (generally = 3). \n",
    "    - NOTE: Layers / block are evenly allocated. Therefore nb_block must be a factor of (Depth - 4)\n",
    "- growth_rate: number of filters to add per dense block\n",
    "- nb_filter:  initial number of filters\n",
    "- bottleneck: add bottleneck blocks\n",
    "- Compression: Filter compression factor in transition blocks.\n",
    "- p: dropout rate\n",
    "- wd: weight decay\n",
    "- activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'. Note that if sigmoid is used, classes must be 1.\n",
    "\n",
    "Returns: keras tensor with nb_layers of conv_block appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From start to finish, this generates:\n",
    "* Conv input layer\n",
    "* Alternate between Dense/Transition blocks `nb_block` times, ommitting Transition block after last Dense block\n",
    "    * Each Dense block has `(Depth-4)/nb_block` layers\n",
    "* Pass final Dense block to BN -> ReLU\n",
    "* Global Avg Pooling\n",
    "* Dense layer w/ desired output activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:02:42.957027",
     "start_time": "2017-08-12T21:02:42.936862"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_net(nb_classes, img_input, depth=40, nb_block=3, \n",
    "     growth_rate=12, nb_filter=16, bottleneck=False, compression=1.0, p=None, wd=0, activation='softmax'):\n",
    "    \n",
    "    assert activation == 'softmax' or activation == 'sigmoid'\n",
    "    assert (depth - 4) % nb_block == 0\n",
    "    nb_layers_per_block = int((depth - 4) / nb_block)\n",
    "    nb_layers = [nb_layers_per_block] * nb_block\n",
    "\n",
    "    x = conv(img_input, nb_filter, 3, wd, 0)\n",
    "    for i,block in enumerate(nb_layers):\n",
    "        x = dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        if i != len(nb_layers)-1:\n",
    "            x = transition_block(x, compression=compression, p=p, wd=wd)\n",
    "\n",
    "    x = relu_bn(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Dense(nb_classes, activation=activation, kernel_regularizer=l2(wd))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it out on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T21:41:09.007154",
     "start_time": "2017-08-12T21:40:48.718730"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_345 (Conv2D)          (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_339 (Bat (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_346 (Conv2D)          (None, 32, 32, 48)        816       \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_340 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_347 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_162 (Concatenate (None, 32, 32, 28)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_341 (Bat (None, 32, 32, 28)        112       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 32, 32, 28)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_348 (Conv2D)          (None, 32, 32, 48)        1392      \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_342 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_349 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_163 (Concatenate (None, 32, 32, 40)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_343 (Bat (None, 32, 32, 40)        160       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 32, 32, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_350 (Conv2D)          (None, 32, 32, 48)        1968      \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_344 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_351 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_164 (Concatenate (None, 32, 32, 52)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_345 (Bat (None, 32, 32, 52)        208       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 32, 32, 52)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_352 (Conv2D)          (None, 32, 32, 48)        2544      \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_346 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_353 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_343 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_165 (Concatenate (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_347 (Bat (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_354 (Conv2D)          (None, 32, 32, 48)        3120      \n",
      "_________________________________________________________________\n",
      "dropout_344 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_348 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_355 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_345 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_166 (Concatenate (None, 32, 32, 76)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_349 (Bat (None, 32, 32, 76)        304       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 32, 32, 76)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_356 (Conv2D)          (None, 32, 32, 48)        3696      \n",
      "_________________________________________________________________\n",
      "dropout_346 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_350 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_357 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_347 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_167 (Concatenate (None, 32, 32, 88)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_351 (Bat (None, 32, 32, 88)        352       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 32, 32, 88)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_358 (Conv2D)          (None, 32, 32, 48)        4272      \n",
      "_________________________________________________________________\n",
      "dropout_348 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_352 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_359 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_349 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_168 (Concatenate (None, 32, 32, 100)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_353 (Bat (None, 32, 32, 100)       400       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 32, 32, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_360 (Conv2D)          (None, 32, 32, 48)        4848      \n",
      "_________________________________________________________________\n",
      "dropout_350 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_354 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_361 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_351 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_169 (Concatenate (None, 32, 32, 112)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_355 (Bat (None, 32, 32, 112)       448       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 32, 32, 112)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_362 (Conv2D)          (None, 32, 32, 48)        5424      \n",
      "_________________________________________________________________\n",
      "dropout_352 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_356 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_363 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_353 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_170 (Concatenate (None, 32, 32, 124)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_357 (Bat (None, 32, 32, 124)       496       \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 32, 32, 124)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_364 (Conv2D)          (None, 32, 32, 48)        6000      \n",
      "_________________________________________________________________\n",
      "dropout_354 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_358 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_358 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_365 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_355 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_171 (Concatenate (None, 32, 32, 136)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_359 (Bat (None, 32, 32, 136)       544       \n",
      "_________________________________________________________________\n",
      "activation_359 (Activation)  (None, 32, 32, 136)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_366 (Conv2D)          (None, 32, 32, 48)        6576      \n",
      "_________________________________________________________________\n",
      "dropout_356 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_360 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_360 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_367 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_357 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_172 (Concatenate (None, 32, 32, 148)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_361 (Bat (None, 32, 32, 148)       592       \n",
      "_________________________________________________________________\n",
      "activation_361 (Activation)  (None, 32, 32, 148)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_368 (Conv2D)          (None, 32, 32, 48)        7152      \n",
      "_________________________________________________________________\n",
      "dropout_358 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_362 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_362 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_369 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_359 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_173 (Concatenate (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_363 (Bat (None, 32, 32, 160)       640       \n",
      "_________________________________________________________________\n",
      "activation_363 (Activation)  (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_370 (Conv2D)          (None, 32, 32, 48)        7728      \n",
      "_________________________________________________________________\n",
      "dropout_360 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_364 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_364 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_371 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_361 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_174 (Concatenate (None, 32, 32, 172)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_365 (Bat (None, 32, 32, 172)       688       \n",
      "_________________________________________________________________\n",
      "activation_365 (Activation)  (None, 32, 32, 172)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_372 (Conv2D)          (None, 32, 32, 48)        8304      \n",
      "_________________________________________________________________\n",
      "dropout_362 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_366 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_366 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_373 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_363 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_175 (Concatenate (None, 32, 32, 184)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_367 (Bat (None, 32, 32, 184)       736       \n",
      "_________________________________________________________________\n",
      "activation_367 (Activation)  (None, 32, 32, 184)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_374 (Conv2D)          (None, 32, 32, 48)        8880      \n",
      "_________________________________________________________________\n",
      "dropout_364 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_368 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_368 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_375 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_365 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_176 (Concatenate (None, 32, 32, 196)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_369 (Bat (None, 32, 32, 196)       784       \n",
      "_________________________________________________________________\n",
      "activation_369 (Activation)  (None, 32, 32, 196)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_376 (Conv2D)          (None, 32, 32, 48)        9456      \n",
      "_________________________________________________________________\n",
      "dropout_366 (Dropout)        (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_370 (Bat (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_370 (Activation)  (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_377 (Conv2D)          (None, 32, 32, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_367 (Dropout)        (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_177 (Concatenate (None, 32, 32, 208)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_371 (Bat (None, 32, 32, 208)       832       \n",
      "_________________________________________________________________\n",
      "activation_371 (Activation)  (None, 32, 32, 208)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_378 (Conv2D)          (None, 32, 32, 104)       21736     \n",
      "_________________________________________________________________\n",
      "dropout_368 (Dropout)        (None, 32, 32, 104)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 16, 16, 104)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_372 (Bat (None, 16, 16, 104)       416       \n",
      "_________________________________________________________________\n",
      "activation_372 (Activation)  (None, 16, 16, 104)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_379 (Conv2D)          (None, 16, 16, 48)        5040      \n",
      "_________________________________________________________________\n",
      "dropout_369 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_373 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_373 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_380 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_370 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_178 (Concatenate (None, 16, 16, 116)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_374 (Bat (None, 16, 16, 116)       464       \n",
      "_________________________________________________________________\n",
      "activation_374 (Activation)  (None, 16, 16, 116)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_381 (Conv2D)          (None, 16, 16, 48)        5616      \n",
      "_________________________________________________________________\n",
      "dropout_371 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_375 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_375 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_382 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_372 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_179 (Concatenate (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_376 (Bat (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_376 (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_383 (Conv2D)          (None, 16, 16, 48)        6192      \n",
      "_________________________________________________________________\n",
      "dropout_373 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_377 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_377 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_384 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_374 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_180 (Concatenate (None, 16, 16, 140)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_378 (Bat (None, 16, 16, 140)       560       \n",
      "_________________________________________________________________\n",
      "activation_378 (Activation)  (None, 16, 16, 140)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_385 (Conv2D)          (None, 16, 16, 48)        6768      \n",
      "_________________________________________________________________\n",
      "dropout_375 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_379 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_379 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_386 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_376 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_181 (Concatenate (None, 16, 16, 152)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_380 (Bat (None, 16, 16, 152)       608       \n",
      "_________________________________________________________________\n",
      "activation_380 (Activation)  (None, 16, 16, 152)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_387 (Conv2D)          (None, 16, 16, 48)        7344      \n",
      "_________________________________________________________________\n",
      "dropout_377 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_381 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_381 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_388 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_378 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_182 (Concatenate (None, 16, 16, 164)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_382 (Bat (None, 16, 16, 164)       656       \n",
      "_________________________________________________________________\n",
      "activation_382 (Activation)  (None, 16, 16, 164)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_389 (Conv2D)          (None, 16, 16, 48)        7920      \n",
      "_________________________________________________________________\n",
      "dropout_379 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_383 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_383 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_390 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_380 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_183 (Concatenate (None, 16, 16, 176)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_384 (Bat (None, 16, 16, 176)       704       \n",
      "_________________________________________________________________\n",
      "activation_384 (Activation)  (None, 16, 16, 176)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_391 (Conv2D)          (None, 16, 16, 48)        8496      \n",
      "_________________________________________________________________\n",
      "dropout_381 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_385 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_385 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_392 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_382 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_184 (Concatenate (None, 16, 16, 188)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_386 (Bat (None, 16, 16, 188)       752       \n",
      "_________________________________________________________________\n",
      "activation_386 (Activation)  (None, 16, 16, 188)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_393 (Conv2D)          (None, 16, 16, 48)        9072      \n",
      "_________________________________________________________________\n",
      "dropout_383 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_387 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_387 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_394 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_384 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_185 (Concatenate (None, 16, 16, 200)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_388 (Bat (None, 16, 16, 200)       800       \n",
      "_________________________________________________________________\n",
      "activation_388 (Activation)  (None, 16, 16, 200)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_395 (Conv2D)          (None, 16, 16, 48)        9648      \n",
      "_________________________________________________________________\n",
      "dropout_385 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_389 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_389 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_396 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_386 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_186 (Concatenate (None, 16, 16, 212)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_390 (Bat (None, 16, 16, 212)       848       \n",
      "_________________________________________________________________\n",
      "activation_390 (Activation)  (None, 16, 16, 212)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_397 (Conv2D)          (None, 16, 16, 48)        10224     \n",
      "_________________________________________________________________\n",
      "dropout_387 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_391 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_391 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_398 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_388 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_187 (Concatenate (None, 16, 16, 224)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_392 (Bat (None, 16, 16, 224)       896       \n",
      "_________________________________________________________________\n",
      "activation_392 (Activation)  (None, 16, 16, 224)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_399 (Conv2D)          (None, 16, 16, 48)        10800     \n",
      "_________________________________________________________________\n",
      "dropout_389 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_393 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_393 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_400 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_390 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_188 (Concatenate (None, 16, 16, 236)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_394 (Bat (None, 16, 16, 236)       944       \n",
      "_________________________________________________________________\n",
      "activation_394 (Activation)  (None, 16, 16, 236)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_401 (Conv2D)          (None, 16, 16, 48)        11376     \n",
      "_________________________________________________________________\n",
      "dropout_391 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_395 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_395 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_402 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_392 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_189 (Concatenate (None, 16, 16, 248)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_396 (Bat (None, 16, 16, 248)       992       \n",
      "_________________________________________________________________\n",
      "activation_396 (Activation)  (None, 16, 16, 248)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_403 (Conv2D)          (None, 16, 16, 48)        11952     \n",
      "_________________________________________________________________\n",
      "dropout_393 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_397 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_397 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_404 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_394 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_190 (Concatenate (None, 16, 16, 260)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_398 (Bat (None, 16, 16, 260)       1040      \n",
      "_________________________________________________________________\n",
      "activation_398 (Activation)  (None, 16, 16, 260)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_405 (Conv2D)          (None, 16, 16, 48)        12528     \n",
      "_________________________________________________________________\n",
      "dropout_395 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_399 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_399 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_406 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_396 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_191 (Concatenate (None, 16, 16, 272)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_400 (Bat (None, 16, 16, 272)       1088      \n",
      "_________________________________________________________________\n",
      "activation_400 (Activation)  (None, 16, 16, 272)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_407 (Conv2D)          (None, 16, 16, 48)        13104     \n",
      "_________________________________________________________________\n",
      "dropout_397 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_401 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_401 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_408 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_398 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_192 (Concatenate (None, 16, 16, 284)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_402 (Bat (None, 16, 16, 284)       1136      \n",
      "_________________________________________________________________\n",
      "activation_402 (Activation)  (None, 16, 16, 284)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_409 (Conv2D)          (None, 16, 16, 48)        13680     \n",
      "_________________________________________________________________\n",
      "dropout_399 (Dropout)        (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_403 (Bat (None, 16, 16, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_403 (Activation)  (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_410 (Conv2D)          (None, 16, 16, 12)        5196      \n",
      "_________________________________________________________________\n",
      "dropout_400 (Dropout)        (None, 16, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_193 (Concatenate (None, 16, 16, 296)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_404 (Bat (None, 16, 16, 296)       1184      \n",
      "_________________________________________________________________\n",
      "activation_404 (Activation)  (None, 16, 16, 296)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_411 (Conv2D)          (None, 16, 16, 148)       43956     \n",
      "_________________________________________________________________\n",
      "dropout_401 (Dropout)        (None, 16, 16, 148)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 8, 8, 148)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_405 (Bat (None, 8, 8, 148)         592       \n",
      "_________________________________________________________________\n",
      "activation_405 (Activation)  (None, 8, 8, 148)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_412 (Conv2D)          (None, 8, 8, 48)          7152      \n",
      "_________________________________________________________________\n",
      "dropout_402 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_406 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_406 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_413 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_403 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_194 (Concatenate (None, 8, 8, 160)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_407 (Bat (None, 8, 8, 160)         640       \n",
      "_________________________________________________________________\n",
      "activation_407 (Activation)  (None, 8, 8, 160)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_414 (Conv2D)          (None, 8, 8, 48)          7728      \n",
      "_________________________________________________________________\n",
      "dropout_404 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_408 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_408 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_415 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_405 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_195 (Concatenate (None, 8, 8, 172)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_409 (Bat (None, 8, 8, 172)         688       \n",
      "_________________________________________________________________\n",
      "activation_409 (Activation)  (None, 8, 8, 172)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_416 (Conv2D)          (None, 8, 8, 48)          8304      \n",
      "_________________________________________________________________\n",
      "dropout_406 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_410 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_410 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_417 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_407 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_196 (Concatenate (None, 8, 8, 184)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_411 (Bat (None, 8, 8, 184)         736       \n",
      "_________________________________________________________________\n",
      "activation_411 (Activation)  (None, 8, 8, 184)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_418 (Conv2D)          (None, 8, 8, 48)          8880      \n",
      "_________________________________________________________________\n",
      "dropout_408 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_412 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_412 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_419 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_409 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_197 (Concatenate (None, 8, 8, 196)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_413 (Bat (None, 8, 8, 196)         784       \n",
      "_________________________________________________________________\n",
      "activation_413 (Activation)  (None, 8, 8, 196)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_420 (Conv2D)          (None, 8, 8, 48)          9456      \n",
      "_________________________________________________________________\n",
      "dropout_410 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_414 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_414 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_421 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_411 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_198 (Concatenate (None, 8, 8, 208)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_415 (Bat (None, 8, 8, 208)         832       \n",
      "_________________________________________________________________\n",
      "activation_415 (Activation)  (None, 8, 8, 208)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_422 (Conv2D)          (None, 8, 8, 48)          10032     \n",
      "_________________________________________________________________\n",
      "dropout_412 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_416 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_416 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_423 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_413 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_199 (Concatenate (None, 8, 8, 220)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_417 (Bat (None, 8, 8, 220)         880       \n",
      "_________________________________________________________________\n",
      "activation_417 (Activation)  (None, 8, 8, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_424 (Conv2D)          (None, 8, 8, 48)          10608     \n",
      "_________________________________________________________________\n",
      "dropout_414 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_418 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_418 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_425 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_415 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_200 (Concatenate (None, 8, 8, 232)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_419 (Bat (None, 8, 8, 232)         928       \n",
      "_________________________________________________________________\n",
      "activation_419 (Activation)  (None, 8, 8, 232)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_426 (Conv2D)          (None, 8, 8, 48)          11184     \n",
      "_________________________________________________________________\n",
      "dropout_416 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_420 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_420 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_427 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_417 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_201 (Concatenate (None, 8, 8, 244)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_421 (Bat (None, 8, 8, 244)         976       \n",
      "_________________________________________________________________\n",
      "activation_421 (Activation)  (None, 8, 8, 244)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_428 (Conv2D)          (None, 8, 8, 48)          11760     \n",
      "_________________________________________________________________\n",
      "dropout_418 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_422 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_422 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_429 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_419 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_202 (Concatenate (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_423 (Bat (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_423 (Activation)  (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_430 (Conv2D)          (None, 8, 8, 48)          12336     \n",
      "_________________________________________________________________\n",
      "dropout_420 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_424 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_424 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_431 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_421 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_203 (Concatenate (None, 8, 8, 268)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_425 (Bat (None, 8, 8, 268)         1072      \n",
      "_________________________________________________________________\n",
      "activation_425 (Activation)  (None, 8, 8, 268)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_432 (Conv2D)          (None, 8, 8, 48)          12912     \n",
      "_________________________________________________________________\n",
      "dropout_422 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_426 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_426 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_433 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_423 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_204 (Concatenate (None, 8, 8, 280)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_427 (Bat (None, 8, 8, 280)         1120      \n",
      "_________________________________________________________________\n",
      "activation_427 (Activation)  (None, 8, 8, 280)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_434 (Conv2D)          (None, 8, 8, 48)          13488     \n",
      "_________________________________________________________________\n",
      "dropout_424 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_428 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_428 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_435 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_425 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_205 (Concatenate (None, 8, 8, 292)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_429 (Bat (None, 8, 8, 292)         1168      \n",
      "_________________________________________________________________\n",
      "activation_429 (Activation)  (None, 8, 8, 292)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_436 (Conv2D)          (None, 8, 8, 48)          14064     \n",
      "_________________________________________________________________\n",
      "dropout_426 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_430 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_430 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_437 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_427 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_206 (Concatenate (None, 8, 8, 304)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_431 (Bat (None, 8, 8, 304)         1216      \n",
      "_________________________________________________________________\n",
      "activation_431 (Activation)  (None, 8, 8, 304)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_438 (Conv2D)          (None, 8, 8, 48)          14640     \n",
      "_________________________________________________________________\n",
      "dropout_428 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_432 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_432 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_439 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_207 (Concatenate (None, 8, 8, 316)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_433 (Bat (None, 8, 8, 316)         1264      \n",
      "_________________________________________________________________\n",
      "activation_433 (Activation)  (None, 8, 8, 316)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_440 (Conv2D)          (None, 8, 8, 48)          15216     \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_434 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_434 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_441 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_208 (Concatenate (None, 8, 8, 328)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_435 (Bat (None, 8, 8, 328)         1312      \n",
      "_________________________________________________________________\n",
      "activation_435 (Activation)  (None, 8, 8, 328)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_442 (Conv2D)          (None, 8, 8, 48)          15792     \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_436 (Bat (None, 8, 8, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_436 (Activation)  (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_443 (Conv2D)          (None, 8, 8, 12)          5196      \n",
      "_________________________________________________________________\n",
      "dropout_433 (Dropout)        (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "concatenate_209 (Concatenate (None, 8, 8, 340)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_437 (Bat (None, 8, 8, 340)         1360      \n",
      "_________________________________________________________________\n",
      "activation_437 (Activation)  (None, 8, 8, 340)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 340)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3410      \n",
      "=================================================================\n",
      "Total params: 781,470\n",
      "Trainable params: 757,958\n",
      "Non-trainable params: 23,512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,32,3)\n",
    "img_input = Input(shape=input_shape)\n",
    "x = create_dense_net(10, img_input, depth=100, nb_filter=16, compression=0.5, \n",
    "                     bottleneck=True, p=0.2, wd=1e-4)\n",
    "\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-13T00:41:16.874Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4kAAAGDCAYAAACGKlUaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8VeW59//PlRDmMMigILTgUTEyyKBoj7UGp4NUcKiK\nVmuxtbRUa+1pfX7o06dqq6f21Cr1VItY0WMrKmJR2+LYEqeqRRCRwakaK4NMMiQQhiTX74+1ErYh\n2Xsn2TtrJfv7fr32K9lr2t+sewf2lXut+zZ3R0RERERERAQgL+oAIiIiIiIiEh8qEkVERERERKSW\nikQRERERERGppSJRREREREREaqlIFBERERERkVoqEkVERERERKSWikQRkRTMrMTM3MymRPDaHj4G\ntfRrS7TM7Pqw7e+LOouIiOQWFYkiElsJxZmbWbWZbTSzZ8xsTIaOPyU8dkkmjpclvw4f2xu7o5l9\n1cxeDM/bTjNbbmbfTLFPBzP7tZn9y8x2m9laM7vPzA6oZ9u7EtrnrITlA8xsjpl9Er7u38xsRJ19\ne5nZrHCb3Wb2kZldnrC+NOHYNY8/N/Yc5Bozu9LM3jezCjPbbmavm9n5TTzWTxPO/X9nOmtbZWb3\n1nn/FiesG1TP+7rmcX2SY+aFfzRYHf6+LDWzCXW2GWdmi8xsl5mtM7P/NrN24breZvYXMysP9x2V\nsN+lZvaxmXXJ/NkQkdaqXdQBRETS8AKwFDgeOBU4FDgk0kQtxN2vasbupxGcp6eBvgTn7ndmtsHd\n/9TAPtcAVwJbgP8FTge+Dlj4FQAzOwOYClSS8H+JmeUDC4DhBO32L+BC4Dkz+zd3LzOzDsBfgaOA\nt4DHgQMJ2jVRGTA74fmKRv78uWgwsBx4BhgKfAl40MyWuPv7jTzWRQnfX2hm0929OkM5G8XM2rl7\nZRSv3QRfIHhf9wPa11m3neCPPjUMuBzIB5K1z/8BrgNKgYeAycATZnaUu68ws88DT4bHeRg4Brga\nqCL4nb4G+A/g98AZwCzgGDPrAdwMXOHuO5r484pIW+Tueuihhx6xfAAlgANXhc+Hh88daB8u60zw\nIed9YAewBDgr4RinAovDddvC9ecAUxKOVfMoTZFjSvi8gOBD19vhcVcBPwDywvWDgKcICq0K4B3g\nhnBdT+ARYBOwC/gQuCvJOajJNih8fhXwT2B3eIwSYEgD+x5dc57q/By3J3m934fb3BI+vyJ8/nzC\nNn2A9QQFXGm4/qxw3dDw+W6gIFz2WLjsh+Hzb4bP/1ZzzurJUdpQezSwfao2qWnvl4DbgK3AGuCi\nJMdsR/DhfFV4zJXA1IT114fHfASYk7DNyXXO1e8IiuXtwKvA+Dqv8X2Cwm5neF5/Uuf4DwP3AOUE\n7/NT0jwnFv6cDpzUyN+9fw/32wCsru8YwITwfG4Jf7a/JKw7jqBQ3RjmfpXgd7WYOr9r7P/7dV/4\n/C7gWWBPuN/F4fktC5e9C3y3TqavEfy+lwGfhsfoFJ6HKmBAuF17gn8PqmuW1TnOAcCMJI+xaZzD\nmnNfnGSbieE260j4Xa3nfbgp3G5MuOxn4fP7wuczwuf/Ez4/NHxeDnQl+MPNW+G6O4Cd4ff/AzzX\nmPeGHnrokRsP9SSKSGtwtpkNJvjgCvBnd98Tfn8PcAFB8bcQmAT80cxOcvcS4F7gIIIP8ZXACGAY\nQRH3LEERuQaYR/ChMh03EfyVfh3BX/UnAbcCHYGfAzcS/NX+WYIi8N+AY8N9fwicS/CheSnw+YSf\nKykzO5SgwNkU/lzdCD6M9yMoRD/D3V+vs6imV2N1kpeZCZwJfMPMCgkKgZ3ALxO2uZvgw+f3CXpM\nEu0KvxYAI83sn+zrITwq/Hpy+LUd8GHYm/EC8D13L0041sFmVhYe8+/A1e7+bgO5U7VJjeMJzsM/\nCNr+LjP7k7vXdznvz4DpBOd2Tpj7LjPb7e7/m7DdVwja+iWC3tsnzOwQggLpCYI2egN4nuC9+hcz\nO8Hd/w7cAFxL8MeER8PzNqROjvMICurlBO+j2cDnGjgPmNl44MsE7/XuYa6XGtq+AReHX58gOP+X\nExRgfwtf41TgL+E2TwKfEPReYWbDCAq/DsCLBOfvS+zfq5bKVIJz9geCIvTzwAfhsq4E5/03ZvaG\nu79iZt8i6CGrAuYTnNND3b3CzB4EvgN8FfhvgqKzG8EfP+r7fehG8P5uyFKC91Bz1VwpcGfCv2l1\nDQR6ERS0S8JlNb/bI8OvoxKXu/v7ZrYV6EHw+7cKOM3MHiJ4368ws5EEf7AZnYGfQ0TamqirVD30\n0EOPhh7s62FIfFQD14Tr+4TLqgj+Ij4jYZ+Hwm3WExQ05xJ8+M4D8sN1U8JtS9LMMYWgd6Y8fH5i\nuP7M8Pna8PnD4fNrCD68dUx4zV+E62YAYwk+7OYnee3ankSgKPz+TYJipKZXpMH9E47zn+G+7wHd\nkmzXk6BgTjznf2NfT+ZlBMX2v4fPS0noSQyXPVRPuznwTLj+mYR2ewBYFD5/I+EYi8McdxH0njlB\nD2rHejKn0yY1bb05bI+C8Odw4OgGjlkWrp8dttcT4fNXw22uD58vSdjvjXDZ5WH7enicLuH628Jl\nc+q8xtkJx2hX5/jLw20HJ5zL3kna8PqE7SoI3ofWiN+7Avb1XJ0BnBR+vw3oFG7z53DZr+vJ/Ztw\n3eMJ6/IJfveKSb8n8fk6udoT/B5fF57Hd8Ltrg3XLw+f/6CeTGPCdcvqZJya7nlpwr9fSXsS2Xdl\nRAXQJ8lxjqt5HyUsOyVc9kn4/O3w+VcStqnpAR4P9CboTSwnKHBHAy8T/Hs0geB3cBXhv6166KGH\nHupJFJHW4AfuPsPMDifoUfovM3uZ4MMVBB8+r6izT03v1bcJesEeCZ9vDrd9qIlZ+gA1AzysCr++\nHX7tZ2btCT6kDyDoifovgksv/4egp2sGQY/adwl6KqqAh83sa57ifi93X2Vm1xHcM/g0gJm9Q/DB\neXlD+4UDYlxH0Atzstffa1ZjJkEPzZ3Aj8LXuhmYS1D0XERQLFxrZhDc6wjwf82ss7vPIbgH8SGC\nD+abCXpC/pOgZ42Er0+5+0Vm1ofgssaRZvY5d/8XQeHmYf4ewFqC+ytHAa/UyZxOm9RY5e67wuPu\nIOgx6lrPeeidsPzSOuvq3jv5dp3vRxK0f83P+bHvu9+rZtvP13mNV2sO4Pvfe7fU3T3sGarRlaCQ\n24+7X29mNwJHAn8ieA/+i6AgT8fpBD1X5cBzBMX05nDZmQRtOzhJ7vrWVQGE75m68hvI8fc6z/9E\n8MeRuvoked3K8OtiM1tK8B4bQXCZ5x72/bvwGeFATT9pIBfAHHdvbk9iTS/iHHffmGS79eHXzmaW\nF/47UfO++SRhmyF89r1cu427byIoBgEws68TvAcvIPidmUFw3v5kZq+7+7NN/JlEpI3Q6KYi0mp4\ncKnhuvDp4QS9WBB82Ovj7ubuRtDjcHa47kl3P4zgA/m5BB90bwrXVYVfG/Nv4UaCyy8Bjgi/1lwe\nuM6DS8Y+cPfjCS71G0twGeuPzGwg8Km7jwcKCYrFFQSXwB2f6oXDQWFucvfeBB/wfhG+9g8a2D7P\nzO4kKBDfAI4PC7DEbY4IHx3CRUPDr6+7ewX7LqkrqtmF4H6tL4ePTuHyownaBIJ7ER9z9/9H0BNY\n8+H0ufDrsiQ/ZrmZ9SK4TK4+9RXS6bRJjcQCzJPk2ERwjyHAUQnvrTyCnzXREfV8v5p978+BZta5\nTq6PwtcoD5/XXI5MzYiU9WROlrdm30IIiiN3X8a+ovTwcH1BQps39L6vudS0K8EfYvYS/N4krvsw\nSe761uVZUCHWnNPCmjzse9/UtTth/x7sKxC/RNAOT9asTiMTBJemA9xCcLnuU+6+pYHXrrnctKHH\nkQ3sl5bwDyNfDZ/OqLOuc9g+h4WLPib4NySP4A8vEF7aS3BVAQS9gxD8e0O4b3eC8/2ZAXHMrDvB\nJbc/JLhUvQtBQb4w3GQkIpLz1JMoIq3B2RbME3g4wf2E1cA/3H2jmc0FzgdeM7NnCT7MnkDQI3Y9\n8IaZlRL0pAwMj1fTI/Nx+HVMWEy94e53JwsS9ujU9LLNMbOnCO5/g+ASNoA7zWwIweVw+QQFahVB\nQTDdzCYR3Mu3h+AyUgh651IZGP6cLxD0vNUUllsb2P5nwDSC8/VG+NoA77t7TdaanrdRBB80XyYo\nFG82s+MILmuD8J42dy9OfIHw3H6e4HLJx8LF94YfRNcC4wh63t5gX0/WLIJ7/cab2QPsKxL+7O6f\nhlMG/NnM/krQU3IyQTG6MjzOZ6TZJo0SHvMOgoFrnjGzPxEUTccR3BM3JWHzo8zs6fD7kQQF66ME\nbfQaQdHyopmtIOhldYJ70NzMbie4J/EBM6u5J7GKfcVYY60zs78R3Gd7GMG5qya4ZxLgYPa1eU/q\nvHfMrBtBLxsEl4HWvC+7hsf6j7DAuZ3gjwTfD++VXU9QuIwg+N27DDjTgull3gO+SDDq57vh+TnA\nzO4nuF+4pjc6mR3sG4TleoJ7FE+us82vCd5bvzSzfycocPsT3IMHwb2Nv0x4PqehF/Pg3th6uz1T\nMbNbCH7na/4wMN2COVZvdveaov07BJc9/9Xd697XO5agYNsG9HD3SjP7FcEftx4Jf//PJ3if1Nwr\nfFt4zG+Hv3tjw+V3uHs5n/UzYLm7Pxy25R6CPzitDdfvd3+ziOSgqK931UMPPfRo6MH+9yRuJxiY\n4byEbboSDEzyHsEAG2sIBq04Llx/O8G9bBUEH7oWAqPCdXkEhcv28Ph/TpFjSvi8PfBj9n3gfZug\nQKm57/CbBAVXWbj+LWByuG4iQeGwNcz7LsGALQ2dg8R7Eg8guORuLcEHuw3Ag0DPBva9r875q3mU\n1HP8keHzQoLC6iOCnpy1BFNhHNjAa5Sy/z2J08N22ENQ5M2sm5GgoHgxbJc14TY9wnUDwnb5V3iO\nVgP3AwOTnKdUbTKlnp891T1jBQRF4sow53qCAY9OD9dfH+7/UHiOakZVPS3hGH0J7mn8OHw//AM4\nI2F9O4LLDpONbnpf+LxH4vuhgcyPhedzN0EPa0md1xuUcIwe9ez/jXDdGsLRaRN+V2ruDf1euGwC\nwR8VtlL/6KbPsq9H9lWgc7jua+HxNwC/Jbh8uL57Eq+vk+0cgvflDoKBmx4It5uRsE3N6KblhKOb\n1jnGH9h3n2inLP27VUr9v3fFCe+rteGyM+rZvzhctzVhWT5BcVfze/Vm3X0JiubXw7b/hKDHtKDO\nNiMI3stFCcsuIfgd20JQaKd9/6oeeujRdh/mnvLqFREREakj4V7P/3X3KdGmkXSY2WSCov737n5J\n1HlEROJKl5uKiIhImxZeRnsZQU8jBD2YIiLSgKwNXGNmA81soZmtNLMVZrbffEMWuN3M3jezZWY2\nOmHdeDN7J1w3PVs5RUREpM07APgVweW217h73RFyRUQkQdYuNzWzfkA/d18Sjra2mOCelZUJ20wA\nvkdwX8OxBPMtHRuO4Pcuwc3lqwnm77kwcV8RERERERHJvKz1JLr7OndfEn5fRnAz/8F1NjsTuN8D\nrwI9wuJyLMHoex94MHT5Q+G2IiIiIiIikkUtMk9iOHT9KIIR/RIdzL4h6CHoNTw4yXIRERERERHJ\noqwPXGNmXQnmi7rK3bdn4fhTgakAHTt2HPO5z30u0y8hGVJdXU1eXov8XUKaSG0Ub2qf+FMbxZ/a\nKN7UPvGnNoq/d999d5O792nOMbJaJJpZAUGB+IC7/7GeTdawb3JrCObGWkMwh1B9y/fj7rMIJs9l\nyJAh/s47mgM2rkpKSiguLo46hiShNoo3tU/8qY3iT20Ub2qf+FMbxZ+ZfdTcY2RzdFMD7gFWufut\nDWz2BHBJOMrpccA2d19HMFDNYWY22MzaAxeE2ybVLj8/Q+mzo2JbGRXbyqKOkZJyioiIiIjkrmz2\nJB5PMB/RW2a2NFx2LfA5AHefCSwgGNn0fWAncGm4rtLMrgCeBvKB2e6+ItUL5se863v3jgoAOnUv\njDhJcsopIiIiIpK7slYkuvtLgKXYxoHLG1i3gKCIFBERERERkRaS9YFrREREREQk3vbu3cvq1avZ\ntWtX0u26d+/OqlWrWiiVJNOxY0cGDBhAQUFBxo+tIlFEREREJMetXr2awsJCBg0aRDC0SP3Kysoo\nLNStPlFzdzZv3szq1asZPHhwxo8f75v4REREREQk63bt2kWvXr2SFogSH2ZGr169Uvb8NlWb6knc\nvXdv1BGS6tG/b9QR0qKcIiIiIrlHBWLrks32Uk+iiIiIiIhEauvWrdx5551N2nfChAls3bo16TY/\n+clPeO6555p0/FzUporEuM+TuHPrdnZu3R51jJSUU0RERERaUrIisbKyMum+CxYsoEePHkm3+elP\nf8opp5zS5Hy5pk0ViXGfJ3HPzl3s2Zmd64YzSTlFREREpCVNnz6df/7zn4wcOZKrr76akpISTjjh\nBCZNmsSRRx4JwFlnncWYMWMYOnQos2bNqt130KBBbNq0idLSUoqKivjWt77F0KFDOe2006ioCObV\nnjJlCvPmzavd/rrrrmP06NEMHz6ct99+G4CNGzdy6qmnMnToUC677DI+//nPs2nTpv2ydu3alauv\nvpqhQ4dyyimn8I9//IPi4mIOOeQQnnjiCQBWrFjB2LFjGTlyJCNGjOC9994D4A9/+EPt8m9/+9tU\nVVVl76Q2Q5u6J1FERERERJpn0PS/ZOW4pTd/ucF1N998M8uXL2fp0qUAlJSUsGTJEpYvX147eufs\n2bM54IADqKio4JhjjuErX/kKvXr1+sxx3nvvPR588EHuvvtuzj//fB599FEuvvji/V6vd+/eLFmy\nhDvvvJNbbrmF3/3ud9xwww2cdNJJXHPNNTz11FPcc8899WbdsWMHJ510Er/85S85++yz+fGPf8yz\nzz7LypUr+frXv86kSZOYOXMm3//+97nooovYs2cPVVVVrFq1iocffpiXX36ZgoICvvvd7/LAAw9w\nySWXNPWUZo2KRBERERERiZ2xY8d+ZnqH22+/nfnz5wPw8ccf89577+1XJA4ePJiRI0cCMGbMGEpL\nS+s99jnnnFO7zR//+EcAXnrppdrjjx8/np49e9a7b/v27Rk/fjwAw4cPp0OHDhQUFDB8+PDa1/vC\nF77ATTfdxOrVqznnnHM47LDD+Otf/8rixYs55phjAKioqKBv33gOxKgiUUREREREaiXr8WvJeRK7\ndOlS+31JSQnPPfccr7zyCp07d6a4uLje6R86dOhQ+31+fn7t5aYNbZefn5/ynse6CgoKakcWzcvL\nqz1WXl5e7bG++tWvcuyxx/KXv/yFCRMmcNddd+HufP3rX+fnP/95o14vCvG+ia+R3KNOkJyZtYqh\nhZVTRERERFpSYWEhZWVlDa7ftm0bPXv2pHPnzrz99tu8+uqrGc9w/PHHM3fuXACeeeYZtmzZ0uRj\nffDBBxxyyCFceeWVnHnmmSxbtoyTTz6ZefPmsWHDBgA+/fRTPvroo4xkz7Q2VSTuqYz3PInd+/Wh\ne78+UcdISTlFREREpCX16tWL448/nmHDhnH11Vfvt378+PFUVlZSVFTE9OnTOe644zKe4brrruOZ\nZ55h2LBhPPLIIxx00EFN7jWdO3cuw4YNY+TIkSxfvpxLLrmEI488khtvvJHTTjuNESNGcOqpp7Ju\n3boM/xSZYR737rdGGDJkiL/zzjtRx5AGlJSUUFxcHHUMSUJtFG9qn/hTG8Wf2ije1D7RWbVqFUVF\nRSm3a8nLTVva7t27yc/Pp127drzyyitMmzatdiCduKqv3cxssbsf3Zzjtql7Etvlx/vH2bllGwCd\ne3aPOElyyikiIiIiueZf//oX559/PtXV1bRv356777476kiRiXdV1Uj5efG+P21PxW4AOtc/UFJs\nKKeIiIiI5JrDDjuMN954I+oYsdCm7kkUERERERGR5lGRKCIiIiIiIrVUJIqIiIiIiEitNnVPYtwH\nas3Lax01uXKKiIiIiOSuNvUpO+7zJHY7qDfdDuoddYyUlFNERERE4q5r164ArF27lnPPPbfebYqL\ni3n99deTHmfGjBns3Lmz9vmECRPYunVr5oK2Qm2qSBQRERERkdzSv39/5s2b1+T96xaJCxYsoEeP\nHpmI1mq1qSKxIObzJO74dCs7Po3/XyWUU0RERERa0vTp07njjjtqn19//fXccsstlJeXc/LJJzN6\n9GiGDx/O448/vt++paWlDBs2DICKigouuOACioqKOPvss6moqKjdbtq0aRx99NEMHTqU6667DoDb\nb7+dtWvXMm7cOMaNGwfAoEGD2LRpEwC33norw4YNY9iwYcyYMaP29YqKivjWt77F0KFDOe200z7z\nOjWmTJnCtGnTOO644zjkkEMoKSnhG9/4BkVFRUyZMgWAqqoqpkyZwrBhwxg+fDi33XYbAP/85z8Z\nP348Y8aM4YQTTuDtt99u7ilulHhXVY2UF/N5Evfu2hN1hLQop4iIiEgOu757g6sKm3XcbQ2umjx5\nMldddRWXX345AHPnzuXpp5+mY8eOzJ8/n27durFp0yaOO+44Jk2ahFn9n/t/+9vf0rlzZ1atWsWy\nZcsYPXp07bqbbrqJAw44gKqqKk4++WSWLVvGlVdeya233srChQvp3fuztzEtXryYe++9l9deew13\n59hjj+XEE0+kZ8+evPfeezz44IPcfffdnH/++Tz66KNcfPHF++XZsmULr7zyCk888QSTJk3i5Zdf\n5ne/+x3HHHMMS5cupaqqijVr1rB8+XKA2stcp06dysyZMznssMN47bXX+O53v8vf/va3xp3vZmhT\nRaKIiIiIiLQ+o0aNYsOGDaxdu5aNGzfSs2dPBg4cyN69e7n22mt54YUXyMvLY82aNaxfv56DDjqo\n3uO88MILXHnllQCMGDGCESNG1K6bO3cus2bNorKyknXr1rFy5crPrK/rpZde4uyzz6ZLly4AnHPO\nObz44otMmjSJwYMHM3LkSADGjBlDaWlpvceYOHEiZsbw4cM58MADGT58OABDhw6ltLSUE088kQ8+\n+IDvfe97fPnLX+a0006jvLycv//975x33nm1x9m9e3f6JzMDVCSKiIiIiMg+SXr8ysrKKCxsVn9i\ng8477zzmzZvHJ598wuTJkwF44IEH2LhxI4sXL6agoIBBgwaxa9euRh/7ww8/5JZbbmHRokX07NmT\nKVOmNOk4NTp06FD7fX5+fr2XmyZul5eX95l98vLyqKyspGfPnrz55ps8/fTTzJw5k7lz5zJjxgx6\n9OjB0qVLm5yvudrUPYkiIiIiItI6TZ48mYceeoh58+bV9qJt27aNvn37UlBQwMKFC/noo4+SHuNL\nX/oSc+bMAWD58uUsW7YMgO3bt9OlSxe6d+/O+vXrefLJJ2v3KSwspKysbL9jnXDCCTz22GPs3LmT\nHTt2MH/+fE444YRM/bgAbNq0ierqar7yla9w4403smTJErp168bgwYN55JFHAHB33nzzzYy+bipt\nqicx7vMk5rfLjzpCWpRTRERERFra0KFDKSsr4+CDD6Zfv34AXHTRRUycOJHhw4dz9NFHc8QRRyQ9\nxrRp07j00kspKiqiqKiIMWPGAHDUUUcxatQojjjiCAYOHMjxxx9fu8/UqVMZP348/fv3Z+HChbXL\nR48ezZQpUxg7diwAl112GaNGjWrw0tKmWLNmDZdeeinV1dUA/PznPweCHtRp06Zx4403snfvXi64\n4AKOOuqojL1uKuZxr6waYciQIf7OO+9EHUMaUFJSQnFxcdQxJAm1UbypfeJPbRR/aqN4U/tEZ9Wq\nVRQVFaXcLpuXm0rj1dduZrbY3Y9uznF1uamIiIiIiIjUalNFYtznSSzftIXyTVuijpGScoqIiIiI\n5K54V1WNFPd5Eiv37I06QlqUU0REREQkd7WpnkQRERERERFpnqz1JJrZbOAMYIO7D6tn/dXARQk5\nioA+7v6pmZUCZUAVUNncGy9FREREREQkPdnsSbwPGN/QSnf/pbuPdPeRwDXA8+7+acIm48L1KhBF\nRERERERaSNaKRHd/Afg05YaBC4EHm/ua1TGfziO/oB35BfG/DVQ5RURERCTuunbtCsDatWs599xz\n692muLiY119/PelxZsyYwc6dO2ufT5gwga1bt2YuaAot/XrpyOo8iWY2CPhzfZebJmzTGVgNHFrT\nk2hmHwLbCC43vcvdZyXZfyowFaBPnz5j5s6dm7H8klnl5eW1v8wST2qjeFP7xJ/aKP7URvGm9olO\n9+7dOfTQQ1NuV1VVRX5+fgskSq1fv36sW7cu6TYTJkzgxhtvZPTo0Q1uM2zYMJ5//nl69eqV6YhZ\n9/7777Nt27bPLBs3blyz50mMQzfMRODlOpeaftHd15hZX+BZM3s77JncT1hAzgIYMmSIawLW+NIE\nufGnNoo3tU/8qY3iT20Ub2qf6KxatYrCwsKU25WVlaW1XWNNnz6dgQMHcvnllwNw/fXX07VrV77z\nne9w5plnsmXLFvbu3cuNN97ImWeeWbtfYWEhpaWlnHHGGSxfvpyKigouvfRS3nzzTY444gj27NlD\nly5dKCwsZNq0aSxatIiKigrOPfdcbrjhBm6//XbWrVvHxIkT6d27NwsXLmTQoEG8/vrr9O7dm1tv\nvZXZs2cDcNlll3HVVVdRWlrK6aefzhe/+EX+/ve/c/DBB/P444/TqVOnz/xMU6ZMoVOnTrzxxhts\n2LCB2bNnc//99/PKK69w7LHHct999wHUvl55eXlax03UsWNHRo0aleHWiMfophdQ51JTd18Tft0A\nzAfGpnOggnZxqHkbVrbxU8o2pnsFbnSUU0RERCS3bV27Yb9HxbYyALy6ut71u8p2AFBdVbXfulQm\nT55M4hWBc+fOZfLkyXTs2JH58+ezZMkSFi5cyA9/+EOSXQn529/+ls6dO7Nq1SpuuOEGFi9eXLvu\npptu4vXXX2fZsmU8//zzLFu2jCuvvJL+/fuzcOFCFi5c+JljLV68mHvvvZfXXnuNV199lbvvvps3\n3ngDgPecWrpoAAAgAElEQVTee4/LL7+cFStW0KNHDx599NF682zZsoVXXnmF2267jUmTJvGDH/yA\nFStW8NZbb7F06dL9tk/3uNkWaZFoZt2BE4HHE5Z1MbPCmu+B04Dl6Rwvz+I9T2LV3kqq9lZGHSMl\n5RQRERGRljRq1Cg2bNjA2rVrefPNN+nZsycDBw7E3bn22msZMWIEp5xyCmvWrGH9+vUNHueFF17g\n4osvBmDEiBGMGDGidt3cuXMZPXo0o0aNYsWKFaxcuTJpppdeeomzzz6bLl260LVrV8455xxefPFF\nAAYPHszIkSMBGDNmDKWlpfUeY+LEiZgZw4cP58ADD2T48OHk5eUxdOjQevdJ97jZls0pMB4EioHe\nZrYauA4oAHD3meFmZwPPuPuOhF0PBOZbUPC1A+a4+1PZyikiIiIiIp/Vo3/fepeXlZVheXkNrgfI\ny89Pur4h5513HvPmzeOTTz5h8uTJADzwwANs3LiRxYsXU1BQwKBBg9i1a1ejj/3hhx9yyy23sGjR\nInr27MmUKVOadJwaHTp0qP0+Pz+fioqKpNvl5eV9Zp+8vDwqK/fv7Ej3uNmWzdFNL3T3fu5e4O4D\n3P0ed5+ZUCDi7ve5+wV19vvA3Y8KH0Pd/aZsZRQRERERkXiYPHkyDz30EPPmzeO8884DYNu2bfTt\n25eCggIWLlzIRx99lPQYX/rSl5gzZw4Ay5cvZ9myZQBs376dLl260L17d9avX8+TTz5Zu09hYSFl\nZWX7HeuEE07gscceY+fOnezYsYP58+dzwgknZOrHjbV438QnIiIiIiI5YejQoZSVlXHwwQfTr18/\nAC666CImTpzI8OHDOfroozniiCOSHmPatGlceumlFBUVUVRUxJgxYwA46qijGDVqFEcccQQDBw7k\n+OOPr91n6tSpjB8/vvbexBqjR49mypQpjB0bDI9y2WWXMWrUqMguAW1JWZ0Co6UNO3KoL1+5IuoY\nDSrftAWArr17RpwkuWzlzPSIZa3lfLYmGlUu3tQ+8ac2ij+1UbypfaKzatUqioqKUm6XrdFNpWnq\nazczaxNTYGTM3qp4D2LSWooZ5RQRERERyV1xmAJDREREREREYqJNFYnt2xVEHSGpsg2bKduwOeoY\nKSmniIiIiEjualOXm8Z8mkSqKquijpAW5RQRERHJPe6Oxf0DtdTK5tgybaonUUREREREGq9jx45s\n3rw5q4WHZI67s3nzZjp27JiV47epnkQREREREWm8AQMGsHr1ajZu3Jh0u127dmWtMJHG6dixIwMG\nDMjKsVUkioiIiIjkuIKCAgYPHpxyu5KSEkaNGtUCiSRKbapIrK6Od/d4Qcf2UUdIi3KKiIiIiOSu\nNlUkxn2exC4H9Ig6QlqUU0REREQkd2ngGhEREREREanVporEuM+TuP2TTWz/ZFPUMVJSThERERGR\n3NWmLjeN+7Qu1dXVUUdIi3KKiIiIiOSuNtWTKCIiIiIiIs2jIlFERERERERqqUgUERERERGRWm3q\nnsSqmM+T2L5Th6gjpEU5RURERERyV5sqEitjPk9i557do46QFuUUEREREcldutxUREREREREarWp\nIjHu8yRuW7eRbes2Rh0jJeUUEREREcldbepy07jPk+ge73smayiniIiIiEjualM9iSIiIiIiItI8\nKhJFRERERESklopEERERERERqdWm7kmsqq6OOkJS7Tt3jDpCWpRTRERERCR3takisbKqKuoISXXu\n0S3qCGlRThERERGR3KXLTUVERERERKRWmyoSOxTEe57ErWs3sHXthqhjpKScIiIiIiK5q00ViSIi\nIiIiItI8KhJFRERERESklopEERERERERqZW1ItHMZpvZBjNb3sD6YjPbZmZLw8dPEtaNN7N3zOx9\nM5uerYwiIiIiIiLyWdmcAuM+4DfA/Um2edHdz0hcYGb5wB3AqcBqYJGZPeHuK1O9YNznSezQpVPU\nEdKinCIiIiIiuStrRaK7v2Bmg5qw61jgfXf/AMDMHgLOBFIWiXGfJ7FT98KoI6RFOUVEREREcpe5\ne/YOHhSJf3b3YfWsKwb+SNBbuAb4kbuvMLNzgfHuflm43deAY939igZeYyowFaBv375jHn744Sz8\nJJlhZgBk85xnQrZylpeX07Vr14wdr7Wcz9Yk020kmaX2iT+1UfypjeJN7RN/aqP4Gzdu3GJ3P7o5\nx8jm5aapLAE+5+7lZjYBeAw4rLEHcfdZwCyAEcOGeXFxcUZDZlLNnH49+veNOEly2cpZUlJCJtun\ntZzP1iTTbSSZpfaJP7VR/KmN4k3tE39qo9wQ2eim7r7d3cvD7xcABWbWm6BXcWDCpgPCZSIiIiIi\nIpJlkRWJZnaQhdcLmtnYMMtmYBFwmJkNNrP2wAXAE1HlFBERERERySVZu9zUzB4EioHeZrYauA4o\nAHD3mcC5wDQzqwQqgAs8uLms0syuAJ4G8oHZ7r4iWzlFRERERERkn2yObnphivW/IZgio751C4AF\n2cglIiIiIiIiDYty4JqMq6yK9zyJHQu7RB0hLcopIiIiIpK72lSRWFUd73kSW0tRo5wiIiIiIrkr\nsoFrsqFm3ry4qq6qoroq3oUsKKeIiIiISC5rU0Vi+3bx7hjdvn4z29dvjjpGSsopIiIiIpK72lSR\nKCIiIiIiIs2jIlFERERERERqqUgUERERERGRWioSRUREREREpFa8R3pppMqYj3TZqVvXqCOkRTlF\nRERERHJX0iLRzEancYy97v5WhvI0S1V1ddQRkurQtXPUEdKinCIiIiIiuStVT+LzwCIg2QSEg4FB\nmQrUHHGfJ7GqshKA/JhP1aGcIiIiIiK5K9Wn60XuflKyDczsbxnM0yxxnyexbMOnAPTo3zfiJMkp\np4iIiIhI7ko6cE2qAjHdbURERERERKR1aNTopmbW1cxGm1mPbAUSERERERGR6CQtEs3szoTvvwis\nBH4FvGVmE7KcTURERERERFpYqpv4jkv4/mfAWe6+xMwOAeYCC7KWTERERERERFpcY0Z66e7uSwDc\n/QMza9Slqi0h9vMkdi+MOkJalFNEREREJHelKhKPMLNlBFNgDDKznu6+JSwQ22c/XuPEfp7ELp2i\njpAW5RQRERERyV2pisSiOs93hF8PAH6S+TjNE/d5Eiv37AWgXfuCiJMkp5wiIiIiIrkraZHo7h/V\nXWZmvdx9E/DHrKVqorjPk1i+aQsQ/3n9lFNEREREJHelGt30ZjPrHX5/tJl9ALxmZh+Z2YktklBE\nRERERERaTKrBZ74c9hoC/BKY7O6HAqcSTIUhIiIiIiIibUiqIrGdmdVcw9nJ3RcBuPu7QIesJhMR\nEREREZEWl6pIvBNYYGYnAU+Z2a/N7EQzuwFYmv14IiIiIiIi0pJSDVzzP2b2FjANODzc/nBgPnBj\n9uM1zt6Yz5PYuUe3qCOkRTlFRERERHJXyuFA3b0EKMl6kgyojvk8ie07d4w6QlqUU0REREQkd6Us\nEs3sEOAcYCBQBbwLzHH37VnO1mh5cZ8ncfceANp1aB9xkuSUU0REREQkd6WaAuNK4C6gI3AMwWA1\nA4FXzaw46+kaqSDu8yRu3kr55q1Rx0hJOUVEREREcleqqupbwEh3rzKzW4EF7l5sZncBjwOjsp5Q\nREREREREWkyq0U1hXyHZAegK4O7/AgqyFUpERERERESikaon8XfAIjN7DTgB+AWAmfUBPs1yNhER\nEREREWlhqabA+LWZPQcUAb9y97fD5RuBL7VAPhEREREREWlB6UyBsQJYUfPczCa5+xOp9jOz2cAZ\nwAZ3H1bP+ouA/w8woAyY5u5vhutKw2VVQKW7H53OD7O3Mt7zJHY5oHvUEdKinCIiIiIiuStpkWhm\n59RdBNxhZu0A3P2PSXa/D/gNcH8D6z8ETnT3LWZ2OjALODZh/Th335QsX13VHu95Egs6dog6QlqU\nU0REREQkd6XqSXwYeBrYQFAgAnQBJgIONFgkuvsLZjYoyfq/Jzx9FRiQOm5yeZbOODzR2btrNxD/\n4kY5RURERERyl7l7wyvNjgFuBua5+2/DZR+6++C0Dh4UiX+u73LTOtv9CDjC3S+reQ1gG8Hlpne5\n+6wk+04FpgIUHT5kzJ13zUwnWiQG9+0PwIcb1kacJLls5SwvL6dr164ZO15rOZ+tSabbSDJL7RN/\naqP4UxvFm9on/tRG8Tdu3LjF6d6u15CkRSKAmeUB3wPOIriH8CF3PyStg6dRJJrZOOBO4Ivuvjlc\ndrC7rzGzvsCzwPfc/YVUrzdi2DBftnx5OtEisXXtBgB69O8bcZLkspWzpKSE4uLijB2vtZzP1iTT\nbSSZpfaJP7VR/KmN4k3tE39qo/gzs2YXiSmvz3T3anf/NXAR8KPmvFhdZjaCYJqNM2sKxPA114Rf\nNwDzgbGZfF0RERERERGpX9o38bn7Wnc/P91exFTM7HME9zR+zd3fTVjexcwKa74HTgPi2z0oIiIi\nIiLShqQa3fQQ4MfAWoJ7E28DvgCsAq5299Ik+z4IFAO9zWw1cB1QAODuM4GfAL2AO80M9k11cSAw\nP1zWDpjj7k81+ScUERERERGRtKUa3fQ+4EGgO8EIpPcCPyXo3ZsNnNTQju5+YbIDh4PUXFbP8g+A\no1Lkqtfeysqm7NZiuvbqEXWEtCiniIiIiEjuSlUkFiaMavpdd/9VuPweM7siu9EarzrFIDxRa9eh\nfdQR0qKcIiIiIiK5K9U9idVmdng4FUZnMzsawMwOBfKznq6R8vLiPU/inp272LNzV9QxUlJOERER\nEZHclaon8f8AfwKqCabAuMbMjgK6Ad/KcrZGK8iPXd36GTu3bgegfeeOESdJTjlFRERERHJX0iLR\n3f8KDElY9JKZ9Qa2uHtVVpOJiIiIiIhIi2v09Znuvsndq8zsoGwEEhERERERkeg05ya+ezKWQkRE\nRERERGKhyUWiu385k0FEREREREQkeqkGrmlV9sR9nsTePaOOkBblFBERERHJXUl7Es1suJm9amYf\nm9ksM+uZsO4f2Y/XOB73eRLbF9CufUHUMVJSThERERGR3JXqctPfAtcDw4F3CUY3/bdwXew+nefH\nfJ7E3Tsq2L2jIuoYKSmniIiIiEjuSnW5aaG7PxV+f4uZLQaeMrOvAbHrtmsX83kSK7aVAdChS6eI\nkySnnCIiIiIiuSvlPYlm1t3dtwG4+0Iz+wrwKHBAtsOJiIiIiIhIy0p1feYvgKLEBe6+DDgZ+GO2\nQomIiIiIiEg0kvYkuvucBpb/C/hWVhKJiIiIiIhIZOI90ouIiIiIiIi0KM2T2IIK+7aO2ziVU0RE\nREQkd6WaJ/FCM+vVUmGaK+7zJOa3a0d+u/jX5copIiIiIpK7Un3C/hzwiJkVAH8FngT+4TGtxmI/\nT2L5TgA6dO0ccZLklFNEREREJHclrarc/RfufhIwAXgT+AawxMzmmNklZnZgS4RMV+znSdxeTsX2\n8qhjpKScIiIiIiK5K61r9dy9DJgfPjCzI4HTgfuB/8haOhEREREREWlRTbqhy91XAiuBX2U2joiI\niIiIiEQp3jfxiYiIiIiISItSkSgiIiIiIiK10rrcNByg5uDw6Rp3X5+9SE0X93kSux3YOmYTUU4R\nERERkdyVtEg0s5HATKA7sCZcPMDMtgLfdfclWc7XKDGdmaNWXsxHX62hnCIiIiIiuStVT+J9wLfd\n/bXEhWZ2HHAvcFSWcjVJfl68i4ZdZTsA6FjYJeIkySmniIiIiEjuSnVPYpe6BSKAu78KxO6Tebv8\neN9iuatsR21hE2fKKSIiIiKSu1L1JD5pZn8hmA/x43DZQOAS4KlsBhMREREREZGWl7RIdPcrzex0\n4EwSBq4B7nD3BdkOJyIiIiIiIi0r5eim7v4k8GQLZBEREREREZGIpRrdtB3wTeAsPtuT+Dhwj7vv\nzW48ERERERERaUmpRnr5PTASuAGYED5uIBjV9A/JdjSz2Wa2wcyWN7DezOx2M3vfzJaZ2eiEdePN\n7J1w3fR0f5jde+Nds3Y/qDfdD+oddYyUlFNEREREJHelutx0jLsfXmfZauBVM3s3xb73Ab8hGPSm\nPqcDh4WPY4HfAseaWT5wB3Bq+FqLzOwJd1+Z4vViz/LiPfpqDeUUEREREcldqT5lf2pm55lZ7XZm\nlmdmk4EtyXZ09xeAT5NsciZwvwdeBXqYWT9gLPC+u3/g7nuAh8JtU2oX88nVK7aVUbGtLOoYKSmn\niIiIiEjuSlUkXgCcC6w3s3fD3sP1wDnhuuY4mH3TakDQa3hwkuUp5ce8Z2n3jgp276iIOkZKyiki\nIiIikrtSTYFRCkwGMLNe4bLN2Y+VPjObCkwFKDp8CCUlJdEGSmJw3/4ALH033lfOZitneXl5Rtun\ntZzP1iTTbSSZpfaJP7VR/KmN4k3tE39qo9yQcgqMGjXFoZn9l7tfm4HXXgMMTHg+IFxW0MDyhnLN\nAmYBjBg2zIuLizMQLTu2rt0AQPGRdW/zjJds5SwpKSGT7dNazmdrkuk2ksxS+8Sf2ij+1EbxpvaJ\nP7VRbkg1BcbtdRcBXzOzrgDufmUzXvsJ4Aoze4hg4Jpt7r7OzDYCh5nZYILi8ALgq814HRERERER\nEUlTqp7Es4HngWcICkQIirbFqQ5sZg8CxUBvM1sNXEfQS4i7zwQWEEyp8T6wE7g0XFdpZlcATwP5\nwGx3X9Gon0pERERERESaJFWReCTwM2A88CN3X2tm17n7/6Y6sLtfmGK9A5c3sG4BQRHZKHGfJ7FH\n/75RR0iLcoqIiIiI5K5UA9eUAVeZ2RjgATP7C6lHRBUREREREZFWKq2Cz90XAycBFcBLWU3UDHGf\nJ3Hn1u3s3Lo96hgpKaeIiIiISO5Ku1cwnPT+Dne/OJuBmiPu8yTu2bmLPTt3RR0jJeUUEREREcld\nTa6qzOytTAYRERERERGR6KWaAuOchlYBB2U+joiIiIiIiEQp1eimDwMPAF7Puo6ZjyMiIiIiIiJR\nSlUkLgNucffldVeY2SnZidR0Xl8pGyNmlnqjGFBOEREREZHclapIvApoaPjIszOcpdn2VMZ7nsTu\n/fpEHSEtyikiIiIikrtSzZP4YpJ1r2c+joiIiIiIiESp0aObmtnfshEkE9rlp+oYjdbOLdvYuWVb\n1DFSUk4RERERkdyVanTTZXUXAYfXLHf3EdkK1hT5efG+R21PxW4AOveMOEgKyikiIiIikrtSdb2V\nEtyTeCNQQVAkvghMzG4sERERERERiULSy03dfRLwKDALOMrdS4G97v6Ru3/UAvlERERERESkBaW8\nJ9Hd5wOnA8Vm9jjQPuupREREREREJBJpjfTi7juA/zSzo4AvZDdS08V9nsS8vEaPExQJ5RQRERER\nyV2NGg7U3d8E3sxSlmaL+zyJ3Q7qHXWEtCiniIiIiEjuanJXjJktyWQQERERERERiV6Ti0R3H53J\nIJlQEPN5End8upUdn26NOkZKyikiIiIikrviXVU1Ul7M50ncu2tP1BHSopwiIiIiIrkraU+imQ00\ns4fM7EUzu9bMChLWPZb9eCIiIiIiItKSUl1uOhsoAb4H9AOeN7Ne4brPZzGXiIiIiIiIRCDV5aZ9\n3H1m+P33zOxi4AUzmwTEfMIJERERERERaaxURWKBmXV0910A7v4HM/sEeBrokvV0jRT3eRLz2+VH\nHSEtyikiIiIikrtSFYm/A44Fnq9Z4O7Pmdl5wH9nM1hTxH2exMK+vVJvFAPKKSIiIiKSu5IWie5+\nWwPL3wBOzUoiERERERERiUyq0U2npjpAOtu0lLjPk1i+aQvlm7ZEHSMl5RQRERERyV2pqqrpZrYp\nyXoDvg/Mylykpov7PImVe+J9OWwN5RQRERERyV2pisTngYkptnk2Q1lEREREREQkYqnuSby0pYKI\niIiIiIhI9JLekygiIiIiIiK5Jd4jvTRSdcwnSswvaB2nWzlFRERERHJXm/qUvbeyMuoISRX2OSDq\nCGlRThERERGR3JXW5aZm9n0z62aBe8xsiZmdlsZ+483sHTN738ym17P+ajNbGj6Wm1mVmR0Qris1\ns7fCda83/kcTERERERGRxkr3nsRvuPt24DSgJ/A14OZkO5hZPnAHcDpwJHChmR2ZuI27/9LdR7r7\nSOAa4Hl3/zRhk3Hh+qPTCVnQLt4do2UbP6Vs46epN4yYcoqIiIiI5K50i8SaCQgnAL939xUJyxoy\nFnjf3T9w9z3AQ8CZSba/EHgwzTz1yrN4z5NYtbeSqr3xviQWlFNEREREJJeZpzHYi5ndCxwMDAaO\nAvKBEncfk2Sfc4Hx7n5Z+PxrwLHufkU923YGVgOH1vQkmtmHwDagCrjL3Wc18DpTgakARYcPGXPn\nXTNT/jxRGdy3PwAfblgbcZLkspWzvLycrl27Zux4reV8tiaZbiPJLLVP/KmN4k9tFG9qn/hTG8Xf\nuHHjFqd7JWZD0r0+85vASOADd99pZr2ATM6hOBF4uc6lpl909zVm1hd41szedvcX6u4YFo+zAEYM\nG+bFxcUZjJVZW9duAKD4yMMjTpJctnKWlJSQyfZpLeezNcl0G0lmqX3iT20Uf2qjeFP7xJ/aKDek\ne7mpE9xXeGX4vAvQMcU+a4CBCc8HhMvqcwF1LjV19zXh1w3AfILLV0VERERERCSL0i0S7wS+QHDf\nIEAZwaA0ySwCDjOzwWbWnqAQfKLuRmbWHTgReDxhWRczK6z5nmDAnOWpQlZXx3uexHbtC2jXviDq\nGCkpp4iIiIhI7kr3ctNj3X20mb0B4O5bwsKvQe5eaWZXAE8T3MM4291XmNl3wvU1Nw+eDTzj7jsS\ndj8QmG/BQDTtgDnu/lSqkHur4j2ISdfePaOOkBblFBERERHJXekWiXvDKS0cwMz6ANWpdnL3BcCC\nOstm1nl+H3BfnWUfEAyQIyIiIiIiIi0o3ctNbye4L7Cvmd0EvAT8V9ZSNVH7dvG+9LBsw2bKNmyO\nOkZKyikiIiIikrvS6kl09wfMbDFwMsH8iGe5+6qsJmuCmE+TSFVlVdQR0qKcIiIiIiK5K62eRDP7\nN+BDd7+DYACZU82sR1aTiYiIiIiISItL93LTR4EqMzsUuItgaos5WUslIiIiIiIikUi3SKx290rg\nHOA37n410C97sURERERERCQKjRnd9ELgEmBiuCx2o8TEfZ7Ego5JZw2JDeUUEREREcld6RaJlwLf\nAW5y9w/NbDDw++zFapq4z5PY5YDWcRuncoqIiIiI5K50RzddCVwJYGY9gUJ3/0U2g4mIiIiIiEjL\nS3d00xIz62ZmBwBLgLvN7NbsRmu8uM+TuP2TTWz/ZFPUMVJSThERERGR3JXuwDXd3X07wcA197v7\nscAp2YvVNHGfJ7G6uprq6uqoY6SknCIiIiIiuSvdIrGdmfUDzgf+nMU8IiIiIiIiEqF0i8SfAk8D\n/3T3RWZ2CPBe9mKJiIiIiIhIFNIduOYR4JGE5x8AX8lWKBEREREREYlGugPXDDCz+Wa2IXw8amYD\nsh2usapiPk9i+04daN+pQ9QxUlJOEREREZHcle7lpvcCTwD9w8efwmWxUhnzeRI79+xO557do46R\nknKKiIiIiOSudIvEPu5+r7tXho/7gD5ZzCUiIiIiIiIRSLdI3GxmF5tZfvi4GNiczWBNEfd5Eret\n28i2dRujjpGScoqIiIiI5K50i8RvEEx/8QmwDjgXmJKlTE0W93kS3R33eN83CcopIiIiIpLL0ioS\n3f0jd5/k7n3cva+7n4VGNxUREREREWlz0u1JrM9/ZiyFiIiIiIiIxEJzisSYX9wpIiIiIiIijdWu\nGfvG7mawqurqqCMk1b5zx6gjpEU5RURERERyV9Ii0czKqL8YNKBTVhI1Q2VVVdQRkurco1vUEdKi\nnCIiIiIiuStpkejuhS0VRERERERERKLXnHsSY6dDQbznSdy6dgNb126IOkZKyikiIiIikrvaVJEo\nIiIiIiIizaMiUURERERERGqpSBQREREREZFaKhJFRERERESkVnPmSYyduM+T2KFL7GYNqZdyioiI\niIjkrjZVJMZ9nsRO3VvHjCLKKSIiIiKSu3S5aQvy6mo85r2doJwiIiIiIrksq0WimY03s3fM7H0z\nm17P+mIz22ZmS8PHT9Ldtz5xnydx2yeb2PbJpqhjpKScIiIiIiK5K2uXm5pZPnAHcCqwGlhkZk+4\n+8o6m77o7mc0cV8RERERERHJoGz2JI4F3nf3D9x9D/AQcGYL7CsiIiIiIiJNlM2Baw4GPk54vho4\ntp7t/t3MlgFrgB+5+4pG7IuZTQWmAhQdPoSSkpLmJ8+SwX37A7D03Xh3iGYrZ3l5eUbbp7Wcz9Yk\n020kmaX2iT+1UfypjeJN7RN/aqPcEPXopkuAz7l7uZlNAB4DDmvMAdx9FjALYMSwYV5cXJzxkJmy\nde0GAIqPPDziJMllK2dJSQmZbJ/Wcj5bk0y3kWSW2if+1EbxpzaKN7VP/KmNckM2i8Q1wMCE5wPC\nZbXcfXvC9wvM7E4z653OvvWprIr3SJcdC7tEHSEtyikiIiIikruyWSQuAg4zs8EEBd4FwFcTNzCz\ng4D17u5mNpbgHsnNwNZU+9anqjre8yS2lqJGOUVEREREclfWikR3rzSzK4CngXxgtruvMLPvhOtn\nAucC08ysEqgALnB3B+rdN9VrmlmWfprMqK4Kiti8/PyIkySnnCIiIiIiuSur9yS6+wJgQZ1lMxO+\n/w3wm3T3TaV9u6hvsUxu+/rNAPTo3zfiJMkpp4iIiIhI7srmFBgiIv9/e/ceJFlZ3nH8+2N2F4Hl\nppSgXAJJUMRE0VDgLcli1AKkwEQrQnklWgRKvKQKFZOUsZJ/jEksg0ERFcUIQmLEbFkoGi/RlJJw\nCYIg6ApryQbZcFtYWHZ3lid/9NlOO07P9LLT02e6v5+qqTn9vu8580y/8/bpZ87b55UkSdISY5Io\nSQe60mkAABC4SURBVJIkSeoySZQkSZIkdZkkSpIkSZK62n2nlx00va3dS2DsttfKUYcwEOOUJEmS\nJtdYJYnbHnts1CHMadeVu486hIEYpyRJkjS5xmq6advXSdw2Pc226elRhzEv45QkSZIm11gliW1f\nJ/Gh9ffx0Pr7Rh3GvIxTkiRJmlxjlSRKkiRJknaOSaIkSZIkqcskUZIkSZLUZZIoSZIkSepq951e\ndlDr10nce89RhzAQ45QkSZIm11glia1fJ3GP3UYdwkCMU5IkSZpcYzXdtO3rJE5v2cr0lq2jDmNe\nxilJkiRNrrFKEtu+TuLGe+5n4z33jzqMeRmnJEmSNLnGKkmUJEmSJO0ck0RJkiRJUpdJoiRJkiSp\nyyRRkiRJktTV7ju97KCtLV8ncfd99hp1CAMxTkmSJGlyjVWS+FjL10lcsfsTRh3CQIxTkiRJmlxj\nNd10l7avk7h5C9Obt4w6jHkZpyRJkjS5xipJXN72dRLvfYCN9z4w6jDmZZySJEnS5BqrJFGSJEmS\ntHNMEiVJkiRJXSaJkiRJkqQuk0RJkiRJUle77/Syg7ZOt3udxD2euPeoQxiIcUqSJEmTa6ySxMeq\n3eskLn/CrqMOYSDGKUmSJE2usZpuukva/etsfXQzWx/dPOow5mWckiRJ0uRqd1a1g5Yvmxp1CHN6\n+L4NPHzfhlGHMS/jlCRJkibXUJPEJMcnuS3JmiTnzlL/miQ3JrkpyXeTPLunbm1TfkOSa4cZpyRJ\nkiSpY2ifSUwyBZwPvBS4E7gmyeqquqWn2R3A71bV/UlOAC4Eju2pP66q7hlWjJIkSZKkXzTMK4nH\nAGuq6vaq2gJcBpzS26CqvltV9zcPrwYOGmI8kiRJkqR5pKqGc+DkVcDxVfXm5vHrgGOr6uw+7c8B\njuhpfwewAdgGfKyqLuyz3xnAGQDPeNrTf+sjH7tgwX+XhXLYk58KwB3r/2fEkcxtWHFu3LiRlStX\nLtjxlsrzuZQsdB9pYdk/7WcftZ991G72T/vZR+133HHHXVdVR+/MMVqxBEaS44A3AS/qKX5RVa1L\n8mTga0lurapvz9y3SR4vBHjGEUfUqlWrFiPkx2V68xYAfuXIp404krkNK85vfetbLGT/LJXncylZ\n6D7SwrJ/2s8+aj/7qN3sn/azjybDMKebrgMO7nl8UFP2C5I8C/gEcEpV3bu9vKrWNd/XA1fQmb46\np8eGdFV0oSzbdQXLdl0x6jDmZZySJEnS5BpmkngNcHiSw5KsAE4FVvc2SHII8AXgdVX1o57yPZLs\nuX0beBnwg/l+4C67tHtFjy2PPMqWRx4ddRjzMk5JkiRpcg1tumlVTSc5G7gKmAIuqqqbk5zZ1F8A\nvBd4EvCRJADTzfzZ/YErmrJlwKVV9ZX5fubyqXavk/jIAw8CsGL3J4w4krkZpyRJkjS5hvqZxKq6\nErhyRtkFPdtvBt48y363A8+eWS5JkiRJGq52z8+UJEmSJC0qk0RJkiRJUpdJoiRJkiSpqxXrJC6U\nLdPTow5hTiv323fUIQzEOCVJkqTJNVZJYrV9ncQVy0cdwkCMU5IkSZpcYzXddKrl6yRufngTmx/e\nNOow5mWckiRJ0uQaqyuJy1q+TuKmDQ8BsOseu404krkZpyRJkjS52n3pTZIkSZK0qEwSJUmSJEld\nJomSJEmSpC6TREmSJElS11jduKbt6yTu+eQnjjqEgRinJEmSNLnGKkls+zqJU8uWxtNtnJIkSdLk\nGqvppq1fJ3HjI2ze+Miow5iXcUqSJEmTa6wuxbR+ncQHNwKw68rdRxzJ3IxTkiRJmlztvvQmSZIk\nSVpUJomSJEmSpC6TREmSJElSl0miJEmSJKlrrG5c0/Z1Evfa/0mjDmEgxilJkiRNrrFKEtu+TuIu\nLb/76nbGKUmSJE2usZpuOrVLu5OGRx96mEcfenjUYczLOCVJkqTJNVZJ4rKpdv86SyWpMU5JkiRp\ncrU7q5IkSZIkLSqTREmSJElSl0miJEmSJKnLJFGSJEmS1DVWS2Bs3rp11CHMae8D9ht1CAMxTkmS\nJGlyjVWS2HbZZWlcuDVOSZIkaXKN1bvsZS1fXH3ThofYtOGhUYcxL+OUJEmSJtdYJYlTLb+ytPnh\nTWx+eNOow5iXcUqSJEmTa6hZVZLjk9yWZE2Sc2epT5Lzmvobkzx30H0lSZIkSQtvaElikingfOAE\n4EjgtCRHzmh2AnB483UG8NEd2FeSJEmStMCGeSXxGGBNVd1eVVuAy4BTZrQ5BfhMdVwN7JPkKQPu\nK0mSJElaYMNMEg8Eftbz+M6mbJA2g+wrSZIkSVpgS34JjCRn0JmqCrA5yQ9GGY/mtB9wz6iD0Jzs\no3azf9rPPmo/+6jd7J/2s4/a7+k7e4BhJonrgIN7Hh/UlA3SZvkA+wJQVRcCFwIkubaqjt65sDUs\n9k/72UftZv+0n33UfvZRu9k/7WcftV+Sa3f2GMOcbnoNcHiSw5KsAE4FVs9osxp4fXOX0+cBG6rq\nrgH3lSRJkiQtsKFdSayq6SRnA1cBU8BFVXVzkjOb+guAK4ETgTXAI8Dpc+07rFglSZIkSR1D/Uxi\nVV1JJxHsLbugZ7uAtwy67wAu3NEYtajsn/azj9rN/mk/+6j97KN2s3/azz5qv53uo3TyNEmSJEmS\nhvuZREmSJEnSErPkksQkxye5LcmaJOfOUp8k5zX1NyZ57ijinFRJDk7yzSS3JLk5ydtnabMqyYYk\nNzRf7x1FrJMsydokNzXP/y/dActxNDpJnt4zNm5I8mCSd8xo4xhaZEkuSrK+d5mlJE9M8rUkP26+\n79tn3znPW1oYffrob5Lc2ryOXZFknz77zvmaqJ3Xp3/el2Rdz2vZiX32dQwtgj59dHlP/6xNckOf\nfR1DQ9bvPfawzkVLarppkingR8BLgTvp3AX1tKq6pafNicBb6dwQ51jg76vq2BGEO5GSPAV4SlVd\nn2RP4DrgFTP6aBVwTlWdNKIwJ16StcDRVTXrOkeOo3ZoXvPWAcdW1U97ylfhGFpUSX4H2Ah8pqp+\noyn7AHBfVb2/OeHuW1XvnrHfvOctLYw+ffQy4BvNDfH+GmBmHzXt1jLHa6J2Xp/+eR+wsar+do79\nHEOLZLY+mlH/d3RWIvjLWerW4hgaqn7vsYE3MoRz0VK7kngMsKaqbq+qLcBlwCkz2pxC54+7qupq\nYJ/mSdUiqKq7qur6Zvsh4IfAgaONSo+D46gdfg/4SW+CqNGoqm8D980oPgW4uNm+mM7JeqZBzlta\nALP1UVV9taqmm4dX01l3WSPQZwwNwjG0SObqoyQB/hD43KIGpa453mMP5Vy01JLEA4Gf9Ty+k19O\nQAZpo0WQ5FDgOcB/zlL9gmb6z5eTPHNRAxNAAf+W5LokZ8xS7zhqh1Ppf0J2DI3e/s3avgA/B/af\npY1jqT3+CPhyn7r5XhM1PG9tXssu6jNNzjHUDr8N3F1VP+5T7xhaRDPeYw/lXLTUkkQtEUlWAv8C\nvKOqHpxRfT1wSFU9C/gw8MXFjk+8qKqOAk4A3tJMMVGLJFkBnAz88yzVjqGWaZZ0Wjqf35gwSf4M\nmAYu6dPE18TR+Cjwq8BRwF3A3402HM3hNOa+iugYWiRzvcdeyHPRUksS1wEH9zw+qCnb0TYaoiTL\n6fzxXlJVX5hZX1UPVtXGZvtKYHmS/RY5zIlWVeua7+uBK+hMQ+jlOBq9E4Drq+rumRWOoda4e/s0\n7Ob7+lnaOJZGLMkbgZOA11SfGzEM8JqoIaiqu6tqW1U9Bnyc2Z93x9CIJVkG/AFweb82jqHF0ec9\n9lDORUstSbwGODzJYc1/2U8FVs9osxp4fTqeR+cDtnfNPJCGo5mz/kngh1X1wT5tDmjakeQYOn+H\n9y5elJMtyR7NB55JsgfwMuAHM5o5jkav739tHUOtsRp4Q7P9BuBfZ2kzyHlLQ5LkeOBdwMlV9Uif\nNoO8JmoIZnzW/feZ/Xl3DI3eS4Bbq+rO2SodQ4tjjvfYQzkXLdv5kBdPc3eys4GrgCngoqq6OcmZ\nTf0FwJV07si4BngEOH1U8U6oFwKvA27K/98m+U+BQ6DbR68CzkoyDWwCTu33310Nxf7AFU2OsQy4\ntKq+4jhqj+Yk+1Lgj3vKevvHMbTIknwOWAXsl+RO4C+A9wP/lORNwE/p3NSBJE8FPlFVJ/Y7b43i\ndxh3ffroPcCuwNea17yrq+rM3j6iz2viCH6Fsdanf1YlOYrO9Li1NK95jqHRmK2PquqTzPL5eMfQ\nSPR7jz2Uc9GSWgJDkiRJkjRcS226qSRJkiRpiEwSJUmSJEldJomSJEmSpC6TREmSJElSl0miJEmS\nJKnLJFGS1DpJtiW5oefr3AU89qFJ5l3DK8mnk9yxffmRJGcmuamJ5z+SHDnAMT6Z5PtJbkzy+SQr\nm/JVSTb0/H7vnbHfVJL/TvKlPsc9Isn3kmxOcs6MuuOT3JZkzXzPW5JXN+1m/TmSpMm0pNZJlCRN\njE1VddSogwDeWVWfb7YvbdapJMnJwAeB4+fZ/0+q6sFmnw8CZ9NZ0wrgO1V1Up/93g78ENirT/19\nwNuAV/QWJpkCzqezzuadwDVJVlfVLbMdpKouT3I3cM5s9ZKkyeSVREnSkpFkbZIPNFf0/ivJrzfl\nhyb5RnPF7utJDmnK909yRXM17/tJXtAcairJx5PcnOSrSXab72dvT/Yae9BZAHygfdJZZXq3QfZJ\nchDwcuATcxx3fVVdA2ydUXUMsKaqbq+qLcBlwCnNcd+W5JbmObpsvjgkSZPLJFGS1Ea7zZhu+uqe\nug1V9ZvAPwAfaso+DFxcVc8CLgHOa8rPA/69qp4NPBe4uSk/HDi/qp4JPAC8cpCgkrwlyU+AD9C5\nkre9/IY59vkU8HPgiCbO7V7QJGxfTvLMnvIPAe8CHhskphkOBH7W8/jOpgzgXOA5zXN05uM4tiRp\nQpgkSpLaaFNVHdXzdXlP3ed6vj+/2X4+cGmz/Y/Ai5rtFwMfBaiqbVW1oSm/o6q2J3bXAYcOElRV\nnV9Vvwa8G/jznvK+U2Or6nTgqXSmj25Pdq8HDmkStg8DXwRIchKwvqquGySeHXQjcEmS1wLTQzi+\nJGlMmCRKkpaa6rO9Izb3bG9jxz+jfxkzPg84l6ra1uzzyubxg1W1sdm+ElieZD/ghcDJSdY27V+c\n5LM7ENc64OCexwc1ZdCZwno+nSuq1yTxvgSSpFmZJEqSlppX93z/XrP9XeDUZvs1wHea7a8DZ0H3\njqF7P94fmuTwnocvB348T/v0fGYywMnArc3jA5oykhxD53x8b1W9p6oOqqpDm9/nG1X12h0I8xrg\n8CSHJVnRHGN1kl2Ag6vqm3Sugu4NrNyB40qSJoj/RZQktdFuMz7n95Wq2r6cw75JbqRzNfC0puyt\nwKeSvBP4X+D0pvztwIVJ3kTniuFZwF2PM6azk7yEzs1i7gfesL0iyQ2zTDkNcHGSvZrt7zc/H+BV\nwFlJpoFNwKlVNedV0e1LcVTVBUkOAK6lc/fTx5K8Aziyqh5McjZwFTAFXFRVNydZDny2SZIDnFdV\nDzzO50GSNOYyzzlJkqTWaKZhHl1V9yzCz/o08KWeJTDGUpJVwDlzLMchSZowTjeVJGl2G4C/2n4F\nbxw1d439CJ0ro5IkAV5JlCRJkiT18EqiJEmSJKnLJFGSJEmS1GWSKEmSJEnqMkmUJEmSJHWZJEqS\nJEmSukwSJUmSJEld/wdcVvBca+LAqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aaf1de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = '../../models/cifar_densenet'\n",
    "fit_params = {\n",
    "    'model_name': model_name,\n",
    "    'loss':       'sparse_categorical_crossentropy',\n",
    "    'opt':        RMSprop(),\n",
    "    'batch_size': 64, \n",
    "    'nepochs':    20,\n",
    "    'patience':   5,\n",
    "    'ploss':      2.,\n",
    "    'reset':      True,\n",
    "}\n",
    "\n",
    "model_trained,_ = train_network(model, X_train, y_train, X_test, y_test, **fit_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "      optimizer=keras.optimizers.SGD(0.1, 0.9, nesterov=True), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parms = {'verbose': 2, 'callbacks': [TQDMNotebookCallback()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will likely need to run overnight + lr annealing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "561s - loss: 1.9801 - acc: 0.4810 - val_loss: 2.0473 - val_acc: 0.5045\n",
      "Epoch 2/20\n",
      "556s - loss: 1.4368 - acc: 0.6571 - val_loss: 1.8446 - val_acc: 0.5864\n",
      "Epoch 3/20\n",
      "547s - loss: 1.2204 - acc: 0.7122 - val_loss: 1.3181 - val_acc: 0.6696\n",
      "Epoch 4/20\n",
      "556s - loss: 1.0634 - acc: 0.7547 - val_loss: 1.3620 - val_acc: 0.6658\n",
      "Epoch 5/20\n",
      "560s - loss: 0.9536 - acc: 0.7829 - val_loss: 2.6235 - val_acc: 0.4702\n",
      "Epoch 6/20\n",
      "557s - loss: 0.8835 - acc: 0.8025 - val_loss: 2.4969 - val_acc: 0.4981\n",
      "Epoch 7/20\n",
      "551s - loss: 0.8293 - acc: 0.8155 - val_loss: 1.1944 - val_acc: 0.7281\n",
      "Epoch 8/20\n",
      "551s - loss: 0.7949 - acc: 0.8244 - val_loss: 1.1396 - val_acc: 0.7366\n",
      "Epoch 9/20\n",
      "551s - loss: 0.7620 - acc: 0.8340 - val_loss: 1.9196 - val_acc: 0.5916\n",
      "Epoch 10/20\n",
      "551s - loss: 0.7472 - acc: 0.8389 - val_loss: 2.6207 - val_acc: 0.4900\n",
      "Epoch 11/20\n",
      "550s - loss: 0.7251 - acc: 0.8449 - val_loss: 1.4957 - val_acc: 0.6859\n",
      "Epoch 12/20\n",
      "551s - loss: 0.7117 - acc: 0.8503 - val_loss: 1.0381 - val_acc: 0.7751\n",
      "Epoch 13/20\n",
      "552s - loss: 0.7006 - acc: 0.8547 - val_loss: 1.6471 - val_acc: 0.6685\n",
      "Epoch 14/20\n",
      "556s - loss: 0.6945 - acc: 0.8555 - val_loss: 0.9267 - val_acc: 0.8087\n",
      "Epoch 15/20\n",
      "551s - loss: 0.6859 - acc: 0.8592 - val_loss: 1.0987 - val_acc: 0.7642\n",
      "Epoch 16/20\n",
      "550s - loss: 0.6756 - acc: 0.8645 - val_loss: 0.9704 - val_acc: 0.7940\n",
      "Epoch 17/20\n",
      "551s - loss: 0.6730 - acc: 0.8642 - val_loss: 0.9401 - val_acc: 0.7800\n",
      "Epoch 18/20\n",
      "551s - loss: 0.6666 - acc: 0.8700 - val_loss: 0.9759 - val_acc: 0.7830\n",
      "Epoch 19/20\n",
      "550s - loss: 0.6654 - acc: 0.8709 - val_loss: 0.8896 - val_acc: 0.8044\n",
      "Epoch 20/20\n",
      "551s - loss: 0.6617 - acc: 0.8712 - val_loss: 1.1052 - val_acc: 0.7570\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f8b132b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "550s - loss: 0.5463 - acc: 0.9128 - val_loss: 0.5737 - val_acc: 0.9033\n",
      "Epoch 2/4\n",
      "551s - loss: 0.4833 - acc: 0.9311 - val_loss: 0.5695 - val_acc: 0.9033\n",
      "Epoch 3/4\n",
      "551s - loss: 0.4575 - acc: 0.9366 - val_loss: 0.5590 - val_acc: 0.9051\n",
      "Epoch 4/4\n",
      "550s - loss: 0.4361 - acc: 0.9429 - val_loss: 0.5656 - val_acc: 0.9048\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05ec7caf28>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 4, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "\n",
      "Epoch 1/20\n",
      "551s - loss: 0.6589 - acc: 0.8728 - val_loss: 1.3259 - val_acc: 0.6935\n",
      "Epoch 2/20\n",
      "551s - loss: 0.6510 - acc: 0.8766 - val_loss: 0.9672 - val_acc: 0.7880\n",
      "Epoch 3/20\n",
      "551s - loss: 0.6508 - acc: 0.8784 - val_loss: 1.1104 - val_acc: 0.7581\n",
      "Epoch 4/20\n",
      "551s - loss: 0.6462 - acc: 0.8793 - val_loss: 1.0601 - val_acc: 0.7877\n",
      "Epoch 5/20\n",
      "550s - loss: 0.6456 - acc: 0.8816 - val_loss: 0.9799 - val_acc: 0.7876\n",
      "Epoch 6/20\n",
      "551s - loss: 0.6427 - acc: 0.8830 - val_loss: 0.9377 - val_acc: 0.8028\n",
      "Epoch 7/20\n",
      "551s - loss: 0.6409 - acc: 0.8837 - val_loss: 1.8484 - val_acc: 0.5932\n",
      "Epoch 8/20\n",
      "551s - loss: 0.6378 - acc: 0.8831 - val_loss: 1.1806 - val_acc: 0.7420\n",
      "Epoch 9/20\n",
      "550s - loss: 0.6381 - acc: 0.8843 - val_loss: 1.0799 - val_acc: 0.7774\n",
      "Epoch 10/20\n",
      "551s - loss: 0.6344 - acc: 0.8870 - val_loss: 0.9114 - val_acc: 0.8163\n",
      "Epoch 11/20\n",
      "561s - loss: 0.6394 - acc: 0.8858 - val_loss: 0.9710 - val_acc: 0.7982\n",
      "Epoch 12/20\n",
      "560s - loss: 0.6367 - acc: 0.8863 - val_loss: 0.8751 - val_acc: 0.8249\n",
      "Epoch 13/20\n",
      "561s - loss: 0.6230 - acc: 0.8899 - val_loss: 1.2588 - val_acc: 0.7254\n",
      "Epoch 14/20\n",
      "561s - loss: 0.6298 - acc: 0.8895 - val_loss: 0.9942 - val_acc: 0.7801\n",
      "Epoch 15/20\n",
      "560s - loss: 0.6321 - acc: 0.8888 - val_loss: 0.8516 - val_acc: 0.8378\n",
      "Epoch 16/20\n",
      "559s - loss: 0.6268 - acc: 0.8893 - val_loss: 0.8288 - val_acc: 0.8301\n",
      "Epoch 17/20\n",
      "561s - loss: 0.6279 - acc: 0.8904 - val_loss: 1.2768 - val_acc: 0.7219\n",
      "Epoch 18/20\n",
      "561s - loss: 0.6248 - acc: 0.8920 - val_loss: 0.9362 - val_acc: 0.8015\n",
      "Epoch 19/20\n",
      "561s - loss: 0.6184 - acc: 0.8941 - val_loss: 0.9204 - val_acc: 0.8181\n",
      "Epoch 20/20\n",
      "561s - loss: 0.6254 - acc: 0.8915 - val_loss: 1.0211 - val_acc: 0.7706\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f55fcb00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "556s - loss: 0.5141 - acc: 0.9320 - val_loss: 0.5652 - val_acc: 0.9165\n",
      "Epoch 2/40\n",
      "560s - loss: 0.4530 - acc: 0.9477 - val_loss: 0.5451 - val_acc: 0.9199\n",
      "Epoch 3/40\n",
      "560s - loss: 0.4290 - acc: 0.9546 - val_loss: 0.5409 - val_acc: 0.9188\n",
      "Epoch 4/40\n",
      "559s - loss: 0.4101 - acc: 0.9584 - val_loss: 0.5259 - val_acc: 0.9224\n",
      "Epoch 5/40\n",
      "549s - loss: 0.3934 - acc: 0.9620 - val_loss: 0.5365 - val_acc: 0.9198\n",
      "Epoch 6/40\n",
      "551s - loss: 0.3813 - acc: 0.9631 - val_loss: 0.5150 - val_acc: 0.9209\n",
      "Epoch 7/40\n",
      "556s - loss: 0.3685 - acc: 0.9644 - val_loss: 0.5238 - val_acc: 0.9197\n",
      "Epoch 8/40\n",
      "556s - loss: 0.3565 - acc: 0.9668 - val_loss: 0.5188 - val_acc: 0.9204\n",
      "Epoch 9/40\n",
      "555s - loss: 0.3430 - acc: 0.9693 - val_loss: 0.5078 - val_acc: 0.9206\n",
      "Epoch 10/40\n",
      "553s - loss: 0.3325 - acc: 0.9707 - val_loss: 0.5107 - val_acc: 0.9191\n",
      "Epoch 11/40\n",
      "556s - loss: 0.3220 - acc: 0.9721 - val_loss: 0.5091 - val_acc: 0.9191\n",
      "Epoch 12/40\n",
      "556s - loss: 0.3121 - acc: 0.9738 - val_loss: 0.5033 - val_acc: 0.9212\n",
      "Epoch 13/40\n",
      "556s - loss: 0.3082 - acc: 0.9723 - val_loss: 0.4970 - val_acc: 0.9226\n",
      "Epoch 14/40\n",
      "556s - loss: 0.2986 - acc: 0.9749 - val_loss: 0.5553 - val_acc: 0.9058\n",
      "Epoch 15/40\n",
      "555s - loss: 0.2913 - acc: 0.9746 - val_loss: 0.5065 - val_acc: 0.9203\n",
      "Epoch 16/40\n",
      "552s - loss: 0.2824 - acc: 0.9762 - val_loss: 0.4912 - val_acc: 0.9218\n",
      "Epoch 17/40\n",
      "554s - loss: 0.2774 - acc: 0.9764 - val_loss: 0.5191 - val_acc: 0.9125\n",
      "Epoch 18/40\n",
      "554s - loss: 0.2722 - acc: 0.9769 - val_loss: 0.5023 - val_acc: 0.9184\n",
      "Epoch 19/40\n",
      "550s - loss: 0.2654 - acc: 0.9771 - val_loss: 0.4965 - val_acc: 0.9183\n",
      "Epoch 20/40\n",
      "547s - loss: 0.2603 - acc: 0.9778 - val_loss: 0.5552 - val_acc: 0.9061\n",
      "Epoch 21/40\n",
      "547s - loss: 0.2549 - acc: 0.9779 - val_loss: 0.4868 - val_acc: 0.9168\n",
      "Epoch 22/40\n",
      "547s - loss: 0.2494 - acc: 0.9793 - val_loss: 0.4754 - val_acc: 0.9242\n",
      "Epoch 23/40\n",
      "547s - loss: 0.2462 - acc: 0.9785 - val_loss: 0.5014 - val_acc: 0.9136\n",
      "Epoch 24/40\n",
      "548s - loss: 0.2427 - acc: 0.9792 - val_loss: 0.5226 - val_acc: 0.9075\n",
      "Epoch 25/40\n",
      "547s - loss: 0.2376 - acc: 0.9794 - val_loss: 0.4829 - val_acc: 0.9159\n",
      "Epoch 26/40\n",
      "547s - loss: 0.2325 - acc: 0.9800 - val_loss: 0.5066 - val_acc: 0.9125\n",
      "Epoch 27/40\n",
      "548s - loss: 0.2312 - acc: 0.9790 - val_loss: 0.4887 - val_acc: 0.9155\n",
      "Epoch 28/40\n",
      "548s - loss: 0.2277 - acc: 0.9792 - val_loss: 0.4959 - val_acc: 0.9107\n",
      "Epoch 29/40\n",
      "547s - loss: 0.2255 - acc: 0.9788 - val_loss: 0.6025 - val_acc: 0.8956\n",
      "Epoch 30/40\n",
      "548s - loss: 0.2216 - acc: 0.9798 - val_loss: 0.4708 - val_acc: 0.9180\n",
      "Epoch 31/40\n",
      "548s - loss: 0.2238 - acc: 0.9772 - val_loss: 0.5193 - val_acc: 0.9084\n",
      "Epoch 32/40\n",
      "548s - loss: 0.2174 - acc: 0.9790 - val_loss: 0.5216 - val_acc: 0.9100\n",
      "Epoch 33/40\n",
      "547s - loss: 0.2176 - acc: 0.9782 - val_loss: 0.4960 - val_acc: 0.9153\n",
      "Epoch 34/40\n",
      "548s - loss: 0.2128 - acc: 0.9790 - val_loss: 0.4644 - val_acc: 0.9188\n",
      "Epoch 35/40\n",
      "548s - loss: 0.2113 - acc: 0.9795 - val_loss: 0.4759 - val_acc: 0.9196\n",
      "Epoch 36/40\n",
      "547s - loss: 0.2090 - acc: 0.9789 - val_loss: 0.5176 - val_acc: 0.9066\n",
      "Epoch 37/40\n",
      "548s - loss: 0.2078 - acc: 0.9802 - val_loss: 0.4602 - val_acc: 0.9208\n",
      "Epoch 38/40\n",
      "547s - loss: 0.2112 - acc: 0.9772 - val_loss: 0.4998 - val_acc: 0.9096\n",
      "Epoch 39/40\n",
      "548s - loss: 0.2051 - acc: 0.9794 - val_loss: 0.5156 - val_acc: 0.9066\n",
      "Epoch 40/40\n",
      "547s - loss: 0.2046 - acc: 0.9781 - val_loss: 0.4961 - val_acc: 0.9108\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f5497d30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 40, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "547s - loss: 0.1885 - acc: 0.9845 - val_loss: 0.4287 - val_acc: 0.9256\n",
      "Epoch 2/20\n",
      "548s - loss: 0.1772 - acc: 0.9886 - val_loss: 0.4198 - val_acc: 0.9279\n",
      "Epoch 3/20\n",
      "547s - loss: 0.1734 - acc: 0.9901 - val_loss: 0.4181 - val_acc: 0.9283\n",
      "Epoch 4/20\n",
      "547s - loss: 0.1706 - acc: 0.9910 - val_loss: 0.4188 - val_acc: 0.9280\n",
      "Epoch 5/20\n",
      "548s - loss: 0.1679 - acc: 0.9918 - val_loss: 0.4127 - val_acc: 0.9298\n",
      "Epoch 6/20\n",
      "548s - loss: 0.1670 - acc: 0.9921 - val_loss: 0.4159 - val_acc: 0.9301\n",
      "Epoch 7/20\n",
      "548s - loss: 0.1650 - acc: 0.9926 - val_loss: 0.4139 - val_acc: 0.9300\n",
      "Epoch 8/20\n",
      "547s - loss: 0.1631 - acc: 0.9933 - val_loss: 0.4087 - val_acc: 0.9304\n",
      "Epoch 9/20\n",
      "548s - loss: 0.1619 - acc: 0.9934 - val_loss: 0.4150 - val_acc: 0.9302\n",
      "Epoch 10/20\n",
      "547s - loss: 0.1609 - acc: 0.9939 - val_loss: 0.4154 - val_acc: 0.9294\n",
      "Epoch 11/20\n",
      "547s - loss: 0.1611 - acc: 0.9933 - val_loss: 0.4102 - val_acc: 0.9310\n",
      "Epoch 12/20\n",
      "547s - loss: 0.1584 - acc: 0.9943 - val_loss: 0.4105 - val_acc: 0.9306\n",
      "Epoch 13/20\n",
      "547s - loss: 0.1594 - acc: 0.9934 - val_loss: 0.4093 - val_acc: 0.9309\n",
      "Epoch 14/20\n",
      "547s - loss: 0.1582 - acc: 0.9940 - val_loss: 0.4110 - val_acc: 0.9298\n",
      "Epoch 15/20\n",
      "547s - loss: 0.1567 - acc: 0.9942 - val_loss: 0.4080 - val_acc: 0.9315\n",
      "Epoch 16/20\n",
      "547s - loss: 0.1565 - acc: 0.9940 - val_loss: 0.4113 - val_acc: 0.9304\n",
      "Epoch 17/20\n",
      "548s - loss: 0.1558 - acc: 0.9942 - val_loss: 0.4093 - val_acc: 0.9292\n",
      "Epoch 18/20\n",
      "548s - loss: 0.1561 - acc: 0.9939 - val_loss: 0.4079 - val_acc: 0.9310\n",
      "Epoch 19/20\n",
      "548s - loss: 0.1552 - acc: 0.9942 - val_loss: 0.4153 - val_acc: 0.9297\n",
      "Epoch 20/20\n",
      "547s - loss: 0.1535 - acc: 0.9951 - val_loss: 0.4069 - val_acc: 0.9313\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05ec7ea6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "548s - loss: 0.1819 - acc: 0.9842 - val_loss: 0.4929 - val_acc: 0.9092\n",
      "Epoch 2/10\n",
      "547s - loss: 0.2018 - acc: 0.9751 - val_loss: 0.5761 - val_acc: 0.8880\n",
      "Epoch 3/10\n",
      "548s - loss: 0.2046 - acc: 0.9742 - val_loss: 0.5411 - val_acc: 0.8950\n",
      "Epoch 4/10\n",
      "548s - loss: 0.2008 - acc: 0.9765 - val_loss: 0.5607 - val_acc: 0.8957\n",
      "Epoch 5/10\n",
      "548s - loss: 0.1956 - acc: 0.9778 - val_loss: 0.4991 - val_acc: 0.9049\n",
      "Epoch 6/10\n",
      "548s - loss: 0.1996 - acc: 0.9760 - val_loss: 0.4714 - val_acc: 0.9112\n",
      "Epoch 7/10\n",
      "548s - loss: 0.1947 - acc: 0.9779 - val_loss: 0.5921 - val_acc: 0.8855\n",
      "Epoch 8/10\n",
      "547s - loss: 0.1958 - acc: 0.9770 - val_loss: 0.5096 - val_acc: 0.9058\n",
      "Epoch 9/10\n",
      "547s - loss: 0.1976 - acc: 0.9754 - val_loss: 0.5129 - val_acc: 0.9041\n",
      "Epoch 10/10\n",
      "548s - loss: 0.1940 - acc: 0.9767 - val_loss: 0.5693 - val_acc: 0.8869\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f52ac668>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 10, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "548s - loss: 0.1879 - acc: 0.9801 - val_loss: 0.4073 - val_acc: 0.9270\n",
      "Epoch 2/20\n",
      "548s - loss: 0.1631 - acc: 0.9893 - val_loss: 0.4040 - val_acc: 0.9265\n",
      "Epoch 3/20\n",
      "547s - loss: 0.1601 - acc: 0.9905 - val_loss: 0.4007 - val_acc: 0.9295\n",
      "Epoch 4/20\n",
      "547s - loss: 0.1560 - acc: 0.9919 - val_loss: 0.4016 - val_acc: 0.9294\n",
      "Epoch 5/20\n",
      "548s - loss: 0.1540 - acc: 0.9921 - val_loss: 0.3988 - val_acc: 0.9293\n",
      "Epoch 6/20\n",
      "547s - loss: 0.1529 - acc: 0.9926 - val_loss: 0.4013 - val_acc: 0.9283\n",
      "Epoch 7/20\n",
      "548s - loss: 0.1497 - acc: 0.9937 - val_loss: 0.3984 - val_acc: 0.9312\n",
      "Epoch 8/20\n",
      "548s - loss: 0.1508 - acc: 0.9929 - val_loss: 0.3993 - val_acc: 0.9304\n",
      "Epoch 9/20\n",
      "547s - loss: 0.1486 - acc: 0.9937 - val_loss: 0.3988 - val_acc: 0.9303\n",
      "Epoch 10/20\n",
      "547s - loss: 0.1471 - acc: 0.9938 - val_loss: 0.3978 - val_acc: 0.9302\n",
      "Epoch 11/20\n",
      "547s - loss: 0.1460 - acc: 0.9942 - val_loss: 0.3945 - val_acc: 0.9306\n",
      "Epoch 12/20\n",
      "547s - loss: 0.1453 - acc: 0.9943 - val_loss: 0.3988 - val_acc: 0.9292\n",
      "Epoch 13/20\n",
      "547s - loss: 0.1456 - acc: 0.9939 - val_loss: 0.4004 - val_acc: 0.9298\n",
      "Epoch 14/20\n",
      "547s - loss: 0.1434 - acc: 0.9946 - val_loss: 0.3978 - val_acc: 0.9314\n",
      "Epoch 15/20\n",
      "547s - loss: 0.1427 - acc: 0.9946 - val_loss: 0.3974 - val_acc: 0.9311\n",
      "Epoch 16/20\n",
      "547s - loss: 0.1417 - acc: 0.9949 - val_loss: 0.3978 - val_acc: 0.9320\n",
      "Epoch 17/20\n",
      "548s - loss: 0.1403 - acc: 0.9954 - val_loss: 0.4010 - val_acc: 0.9317\n",
      "Epoch 18/20\n",
      "548s - loss: 0.1395 - acc: 0.9955 - val_loss: 0.3989 - val_acc: 0.9324\n",
      "Epoch 19/20\n",
      "547s - loss: 0.1409 - acc: 0.9951 - val_loss: 0.3997 - val_acc: 0.9312\n",
      "Epoch 20/20\n",
      "548s - loss: 0.1402 - acc: 0.9948 - val_loss: 0.3973 - val_acc: 0.9323\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f5264588>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're able to replicate their state-of-the-art results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 452 ms, total: 31.6 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%time model.save_weights('models/93.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
