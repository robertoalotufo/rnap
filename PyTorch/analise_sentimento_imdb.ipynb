{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento usando word embeddings - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente vimos uma primeira solução de análise de sentimento utilizando *bag of words*.\n",
    "Agora iremos ilustrar o uso de *word embeddings* como vetor de atributos latentes de cada palavra.\n",
    "\n",
    "Duas soluções são propostas neste exercícios:\n",
    "\n",
    "1. Utilizando rede neural com camadas densas\n",
    "2. Utilizando camadas convolucionais 1D\n",
    "\n",
    "Diferentemente da solução apresentada com *bag of words*, nestas duas soluções, é necessário que o\n",
    "número de palavras seja o mesmo para cada amostra. Para isso, limita-se o número de palavras e caso\n",
    "o número de palavras for menor, completa-se com um código especial e palavras além do limite são\n",
    "descartadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:22.382532Z",
     "start_time": "2017-12-18T15:32:22.012453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo do disco\n",
    "\n",
    "O dataset é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. \n",
    "Cada amostra tem um rótulo igual a 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:23.173488Z",
     "start_time": "2017-12-18T15:32:22.383535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, 25000, 25000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = json.load(open('/data/datasets/IMDB/imdb_word_index.json'))\n",
    "data = np.load('/data/datasets/IMDB/imdb.npz')\n",
    "x_test, x_train, y_train, y_test = data['x_test'], data['x_train'], data['y_train'], data['y_test']\n",
    "\n",
    "n_words = len(word_index)\n",
    "n_train = x_train.shape[0]\n",
    "n_test  = x_test.shape[0]\n",
    "\n",
    "word_list = [None for i in range(n_words+1)]\n",
    "for k, v in word_index.items():\n",
    "    word_list[v] = k\n",
    "\n",
    "n_words, n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:23.462544Z",
     "start_time": "2017-12-18T15:32:23.174620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 88584\n",
      "Test word index limits: 1 88581\n",
      "\n",
      "Train sequence length limits: 10 2493\n",
      "Test sequence length limits: 6 2314\n",
      "\n",
      "Most frequent words: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']\n"
     ]
    }
   ],
   "source": [
    "def print_stats(x_train, x_test, word_list=None):\n",
    "    print('Train word index limits:', min([min(s) for s in x_train]), max([max(s) for s in x_train]))\n",
    "    print('Test word index limits:', min([min(s) for s in x_test]), max([max(s) for s in x_test]))\n",
    "    print('\\nTrain sequence length limits:', min([len(x) for x in x_train]), max([len(x) for x in x_train]))\n",
    "    print('Test sequence length limits:', min([len(x) for x in x_test]), max([len(x) for x in x_test]))\n",
    "    if word_list:\n",
    "        print('\\nMost frequent words:', word_list[1:11])\n",
    "    \n",
    "print_stats(x_train, x_test, word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitando o vocabulário\n",
    "\n",
    "Retiramos das sequências as palavras com índice maior que o valor especificado em `voc_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:24.392976Z",
     "start_time": "2017-12-18T15:32:23.463674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 9999\n",
      "Test word index limits: 1 9999\n",
      "\n",
      "Train sequence length limits: 9 2194\n",
      "Test sequence length limits: 6 2198\n"
     ]
    }
   ],
   "source": [
    "# voc_size = 5000\n",
    "voc_size = 10000\n",
    "\n",
    "xtra = [[w for w in x if (w < voc_size)] for x in x_train]\n",
    "xval = [[w for w in x if (w < voc_size)] for x in x_test]\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo sequências de mesmo comprimento\n",
    "\n",
    "Fazemos com que todas as sequências tenham o mesmo comprimento, especificado em `seq_len`. As sequências mais longas que `seq_len` são truncadas e as menores, completadas com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:24.403674Z",
     "start_time": "2017-12-18T15:32:24.394049Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, seq_len, post_pad=True, fill_value=0):\n",
    "    new_seq = []\n",
    "    for seq in sequences:\n",
    "        n = len(seq)\n",
    "        if n > seq_len:\n",
    "            if post_pad:\n",
    "                new_seq.append(seq[-seq_len:])\n",
    "            else:\n",
    "                new_seq.append(seq[:seq_len])\n",
    "        else:\n",
    "            zseq = [fill_value for i in range(seq_len)]\n",
    "            if post_pad:\n",
    "                zseq[-n:] = seq\n",
    "            else:\n",
    "                zseq[:n] = seq\n",
    "            new_seq.append(zseq)\n",
    "    return new_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:25.658655Z",
     "start_time": "2017-12-18T15:32:24.404439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 0 9999\n",
      "Test word index limits: 0 9999\n",
      "\n",
      "Train sequence length limits: 500 500\n",
      "Test sequence length limits: 500 500\n"
     ]
    }
   ],
   "source": [
    "seq_len = 500\n",
    "xtra = pad_sequences(xtra, seq_len, post_pad=True)\n",
    "xval = pad_sequences(xval, seq_len, post_pad=True)\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:26.451033Z",
     "start_time": "2017-12-18T15:32:25.659555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 500]), 9999, torch.Size([25000, 500]), 9999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = torch.from_numpy(np.array(xtra, np.int))\n",
    "Xvalid = torch.from_numpy(np.array(xval, np.int))\n",
    "ytrain = torch.from_numpy(np.array(y_train, np.int))\n",
    "yvalid = torch.from_numpy(np.array(y_test, np.int))\n",
    "\n",
    "Xtrain.size(), Xtrain.max(), Xvalid.size(), Xvalid.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Densa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:26.462418Z",
     "start_time": "2017-12-18T15:32:26.451961Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySimpleNet(nn.Module):\n",
    "    def __init__(self, seq_len=seq_len, voc_size=voc_size, embed_dim=None):\n",
    "        super().__init__()\n",
    "        self.flat_size = seq_len * embedding_dim        \n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.fc1 = nn.Linear(self.flat_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:27.626961Z",
     "start_time": "2017-12-18T15:32:26.463303Z"
    }
   },
   "outputs": [],
   "source": [
    "trainIt = False\n",
    "resetIt = False\n",
    "\n",
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_1'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=resetIt, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "model = MySimpleNet(seq_len, voc_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.e-4, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, chkpt_cb, print_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:27.630504Z",
     "start_time": "2017-12-18T15:32:27.627916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training disabled.\n",
      "This model was trained for 0 epochs.\n"
     ]
    }
   ],
   "source": [
    "if trainIt:\n",
    "    trainer.fit(n_epochs, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    print('\\nTraining disabled.\\nThis model was trained for {} epochs.'.format(trainer.last_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento em CPU (AWS c4x.2large, _compute optimized 8 cores_ ):\n",
    "    Start training for 10 epochs\n",
    "      1:  16.4s   T: 0.67021 0.58424   V: 0.54485 0.77676 best\n",
    "      2:  14.3s   T: 0.37235 0.85344   V: 0.31478 0.87156 best\n",
    "      3:  17.2s   T: 0.25475 0.90344   V: 0.28751 0.87880 best\n",
    "      4:  14.2s   T: 0.21120 0.92364   V: 0.27403 0.88536 best\n",
    "      5:  16.4s   T: 0.17996 0.93792   V: 0.27677 0.88396 \n",
    "      6:  29.8s   T: 0.16073 0.94736   V: 0.27144 0.88504 best\n",
    "      7: 125.4s   T: 0.14337 0.95564   V: 0.27229 0.88516 \n",
    "      8: 167.4s   T: 0.12665 0.96464   V: 0.28843 0.87948 \n",
    "      9: 181.3s   T: 0.11348 0.97008   V: 0.28020 0.88552 \n",
    "     10: 185.8s   T: 0.09831 0.97824   V: 0.28082 0.88344 \n",
    "    Best model was saved at epoch 6 with loss 0.27144: ../../models/sentimento_1\n",
    "    Stop training at epoch: 10/10\n",
    "\n",
    "#### Treinamento em GPU (GTX1080 Ti, 8GB):\n",
    "    Start training for 10 epochs\n",
    "      1:   5.8s   T: 0.66767 0.58464   V: 0.54069 0.77204 best\n",
    "      2:   5.8s   T: 0.36899 0.85356   V: 0.31553 0.86972 best\n",
    "      3:   5.7s   T: 0.24919 0.90684   V: 0.28281 0.88276 best\n",
    "      4:   5.6s   T: 0.20338 0.92668   V: 0.27315 0.88716 best\n",
    "      5:   5.7s   T: 0.17143 0.94212   V: 0.27240 0.88580 best\n",
    "      6:   5.7s   T: 0.15174 0.95248   V: 0.27210 0.88704 best\n",
    "      7:   5.7s   T: 0.13394 0.96144   V: 0.27415 0.88724 \n",
    "      8:   5.6s   T: 0.11788 0.96788   V: 0.27716 0.88436 \n",
    "      9:   5.6s   T: 0.10352 0.97472   V: 0.28018 0.88488 \n",
    "     10:   5.6s   T: 0.08865 0.98192   V: 0.28278 0.88464 \n",
    "    Best model was saved at epoch 6 with loss 0.27210: ../../models/sentimento_1\n",
    "    Stop training at epoch: 10/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:30.375962Z",
     "start_time": "2017-12-18T15:32:27.631276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model training set accuracy after training: 0.96156\n",
      "\n",
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.88704\n"
     ]
    }
   ],
   "source": [
    "if 'ModelCheckpoint' in [cb.__class__.__name__ for cb in trainer.callbacks]:\n",
    "    trainer.load_state(state_fn)\n",
    "\n",
    "rmetrics = trainer.evaluate(Xtrain, ytrain, metrics=[accuracy_cb])\n",
    "print('Model training set accuracy after training: {:.5f}'.format(rmetrics['acc']))\n",
    "print()\n",
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[accuracy_cb])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rede convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:30.416285Z",
     "start_time": "2017-12-18T15:32:30.377537Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len=seq_len, voc_size=voc_size, embed_dim=embedding_dim, \n",
    "                 n_conv_filters=128, conv_kernel_size=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        k = conv_kernel_size - 1\n",
    "        n = (((seq_len - k) // 2 - k) // 2 - k) // 2\n",
    "        self.flat_size = n * n_conv_filters\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.embedding.weight)\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv1d(n_conv_filters, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv1d(n_conv_filters, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = self.fc_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:30.486663Z",
     "start_time": "2017-12-18T15:32:30.417499Z"
    }
   },
   "outputs": [],
   "source": [
    "trainIt = False\n",
    "resetIt = False\n",
    "\n",
    "# embedding_dim = 50\n",
    "# batch_size = 100\n",
    "# n_epochs = 15\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_3'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=resetIt, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "model = MyNet(seq_len, voc_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.e-3, weight_decay=0)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, chkpt_cb, print_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:30.491537Z",
     "start_time": "2017-12-18T15:32:30.487919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training disabled.\n",
      "This model was trained for 0 epochs.\n"
     ]
    }
   ],
   "source": [
    "if trainIt:\n",
    "    trainer.fit(n_epochs, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    print('\\nTraining disabled.\\nThis model was trained for {} epochs.'.format(trainer.last_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento em CPU (AWS c4x.2large, _compute optimized 8 cores_ ):\n",
    "    Start training for 10 epochs\n",
    "      1: 177.9s   T: 0.54422 0.66508   V: 0.31881 0.86320 best\n",
    "      2: 176.4s   T: 0.24304 0.90532   V: 0.30452 0.86872 best\n",
    "      3: 180.2s   T: 0.17245 0.93924   V: 0.33442 0.87108 \n",
    "      4: 176.7s   T: 0.11281 0.95988   V: 0.37404 0.86836 \n",
    "      5: 188.7s   T: 0.06431 0.97768   V: 0.45143 0.86640 \n",
    "      6: 179.7s   T: 0.04120 0.98564   V: 0.58594 0.85720 \n",
    "      7: 175.4s   T: 0.03657 0.98700   V: 0.73639 0.85680 \n",
    "      8: 189.7s   T: 0.02824 0.99052   V: 0.73352 0.85636 \n",
    "      9: 192.3s   T: 0.01951 0.99304   V: 0.77173 0.85816 \n",
    "     10: 188.2s   T: 0.01272 0.99564   V: 1.04543 0.86184 \n",
    "    Best model was saved at epoch 2 with loss 0.30452: ../../models/sentimento_3\n",
    "    Stop training at epoch: 10/10\n",
    "\n",
    "#### Treinamento em GPU (GTX1080 Ti, 8GB):\n",
    "      1:  10.0s   T: 0.56728 0.65152   V: 0.31363 0.86676 best\n",
    "      2:   9.7s   T: 0.25570 0.90016   V: 0.26888 0.88768 best\n",
    "      3:   9.8s   T: 0.17377 0.93516   V: 0.31691 0.87440 \n",
    "      4:   9.7s   T: 0.12177 0.95736   V: 0.32377 0.87524 \n",
    "      5:   9.6s   T: 0.07530 0.97344   V: 0.41996 0.86808 \n",
    "      6:   9.6s   T: 0.05182 0.98280   V: 0.50035 0.86080 \n",
    "      7:   9.6s   T: 0.03777 0.98748   V: 0.59080 0.86900 \n",
    "      8:   9.6s   T: 0.03523 0.98824   V: 0.59438 0.87164 \n",
    "      9:   9.6s   T: 0.02474 0.99096   V: 0.75827 0.86708 \n",
    "     10:   9.6s   T: 0.01636 0.99436   V: 0.85570 0.86812 \n",
    "    Best model was saved at epoch 2 with loss 0.26888: ../../models/sentimento_3\n",
    "    Stop training at epoch: 10/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T15:32:36.162451Z",
     "start_time": "2017-12-18T15:32:30.492684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model training set accuracy after training: 0.95284\n",
      "\n",
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.88768\n"
     ]
    }
   ],
   "source": [
    "if 'ModelCheckpoint' in [cb.__class__.__name__ for cb in trainer.callbacks]:\n",
    "    trainer.load_state(state_fn)\n",
    "\n",
    "rmetrics = trainer.evaluate(Xtrain, ytrain, metrics=[ptt.AccuracyMetric()])\n",
    "print('Model training set accuracy after training: {:.5f}'.format(rmetrics['acc']))\n",
    "print()\n",
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[ptt.AccuracyMetric()])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T20:15:36.885766Z",
     "start_time": "2017-10-26T20:15:36.880693Z"
    }
   },
   "source": [
    "## Resumo dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experimento *bag of words*: 87% de acurácia\n",
    "2. Experimento *word embeddings*, rede densa: 88.7%\n",
    "3. Experimento *word embeddings*, rede convolucional: 88.8%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
