{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento usando word embeddings - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente vimos uma primeira solução de análise de sentimento utilizando *bag of words*.\n",
    "Agora iremos ilustrar o uso de *word embeddings* como vetor de atributos latentes de cada palavra.\n",
    "\n",
    "Duas soluções são propostas neste exercícios:\n",
    "\n",
    "1. Utilizando rede neural com camadas densas\n",
    "2. Utilizando camadas convolucionais 1D\n",
    "\n",
    "Diferentemente da solução apresentada com *bag of words*, nestas duas soluções, é necessário que o\n",
    "número de palavras seja o mesmo para cada amostra. Para isso, limita-se o número de palavras e caso\n",
    "o número de palavras for menor, completa-se com um código especial e palavras além do limite são\n",
    "descartadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:10.320109Z",
     "start_time": "2017-12-15T16:57:09.863409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo do disco\n",
    "\n",
    "O dataset é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. \n",
    "Cada amostra tem um rótulo igual a 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:11.574750Z",
     "start_time": "2017-12-15T16:57:10.321708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, 25000, 25000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = json.load(open('/data/datasets/IMDB/imdb_word_index.json'))\n",
    "data = np.load('/data/datasets/IMDB/imdb.npz')\n",
    "x_test, x_train, y_train, y_test = data['x_test'], data['x_train'], data['y_train'], data['y_test']\n",
    "\n",
    "n_words = len(word_index)\n",
    "n_train = x_train.shape[0]\n",
    "n_test  = x_test.shape[0]\n",
    "\n",
    "word_list = [None for i in range(n_words+1)]\n",
    "for k, v in word_index.items():\n",
    "    word_list[v] = k\n",
    "\n",
    "n_words, n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:12.004353Z",
     "start_time": "2017-12-15T16:57:11.576348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 88584\n",
      "Test word index limits: 1 88581\n",
      "\n",
      "Train sequence length limits: 10 2493\n",
      "Test sequence length limits: 6 2314\n",
      "\n",
      "Most frequent words: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']\n"
     ]
    }
   ],
   "source": [
    "def print_stats(x_train, x_test, word_list=None):\n",
    "    print('Train word index limits:', min([min(s) for s in x_train]), max([max(s) for s in x_train]))\n",
    "    print('Test word index limits:', min([min(s) for s in x_test]), max([max(s) for s in x_test]))\n",
    "    print('\\nTrain sequence length limits:', min([len(x) for x in x_train]), max([len(x) for x in x_train]))\n",
    "    print('Test sequence length limits:', min([len(x) for x in x_test]), max([len(x) for x in x_test]))\n",
    "    if word_list:\n",
    "        print('\\nMost frequent words:', word_list[1:11])\n",
    "    \n",
    "print_stats(x_train, x_test, word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitando o vocabulário\n",
    "\n",
    "Retiramos das sequências as palavras com índice maior que o valor especificado em `voc_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:13.386460Z",
     "start_time": "2017-12-15T16:57:12.005951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 4999\n",
      "Test word index limits: 1 4999\n",
      "\n",
      "Train sequence length limits: 9 1973\n",
      "Test sequence length limits: 6 2113\n"
     ]
    }
   ],
   "source": [
    "voc_size = 5000\n",
    "\n",
    "xtra = [[w for w in x if (w < voc_size)] for x in x_train]\n",
    "xval = [[w for w in x if (w < voc_size)] for x in x_test]\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo sequências de mesmo comprimento\n",
    "\n",
    "Fazemos com que todas as sequências tenham o mesmo comprimento, especificado em `seq_len`. As sequências mais longas que `seq_len` são truncadas e as menores, completadas com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:13.402695Z",
     "start_time": "2017-12-15T16:57:13.387963Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, seq_len, post_pad=True, fill_value=0):\n",
    "    new_seq = []\n",
    "    for seq in sequences:\n",
    "        n = len(seq)\n",
    "        if n > seq_len:\n",
    "            if post_pad:\n",
    "                new_seq.append(seq[-seq_len:])\n",
    "            else:\n",
    "                new_seq.append(seq[:seq_len])\n",
    "        else:\n",
    "            zseq = [fill_value for i in range(seq_len)]\n",
    "            if post_pad:\n",
    "                zseq[-n:] = seq\n",
    "            else:\n",
    "                zseq[:n] = seq\n",
    "            new_seq.append(zseq)\n",
    "    return new_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:15.321106Z",
     "start_time": "2017-12-15T16:57:13.404001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 0 4999\n",
      "Test word index limits: 0 4999\n",
      "\n",
      "Train sequence length limits: 500 500\n",
      "Test sequence length limits: 500 500\n"
     ]
    }
   ],
   "source": [
    "seq_len = 500\n",
    "xtra = pad_sequences(xtra, seq_len, post_pad=True)\n",
    "xval = pad_sequences(xval, seq_len, post_pad=True)\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:16.505595Z",
     "start_time": "2017-12-15T16:57:15.322689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 500]), 4999, torch.Size([25000, 500]), 4999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = torch.from_numpy(np.array(xtra, np.int))\n",
    "Xvalid = torch.from_numpy(np.array(xval, np.int))\n",
    "ytrain = torch.from_numpy(np.array(y_train, np.int))\n",
    "yvalid = torch.from_numpy(np.array(y_test, np.int))\n",
    "\n",
    "Xtrain.size(), Xtrain.max(), Xvalid.size(), Xvalid.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Densa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:16.523367Z",
     "start_time": "2017-12-15T16:57:16.507044Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len=seq_len, voc_size=voc_size, embed_dim=None):\n",
    "        super().__init__()\n",
    "        self.flat_size = seq_len * embedding_dim        \n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.fc1 = nn.Linear(self.flat_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:16.584124Z",
     "start_time": "2017-12-15T16:57:16.524737Z"
    }
   },
   "outputs": [],
   "source": [
    "trainIt = False\n",
    "resetIt = False\n",
    "\n",
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_1'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=resetIt, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "model = MySimpleNet(seq_len, voc_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, chkpt_cb, print_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:57:16.589497Z",
     "start_time": "2017-12-15T16:57:16.585559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training disabled.\n",
      "This model was trained for 0 epochs.\n"
     ]
    }
   ],
   "source": [
    "if trainIt:\n",
    "    trainer.fit(n_epochs, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    print('\\nTraining disabled.\\nThis model was trained for {} epochs.'.format(trainer.last_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento em CPU (AWS c4x.2large, _compute optimized 8 cores_ ):\n",
    "`\n",
    "Start training for 10 epochs\n",
    "  1:  12.8s   T: 0.67543 0.57752   V: 0.59997 0.73932 best\n",
    "  2:  13.1s   T: 0.41619 0.83124   V: 0.33354 0.86456 best\n",
    "  3:  16.8s   T: 0.27492 0.89132   V: 0.29643 0.87532 best\n",
    "  4:  13.1s   T: 0.23069 0.91264   V: 0.28460 0.87956 best\n",
    "  5:  13.1s   T: 0.19852 0.92964   V: 0.27674 0.88468 best\n",
    "  6:  22.7s   T: 0.17766 0.93948   V: 0.27637 0.88312 best\n",
    "  7:  96.0s   T: 0.16022 0.94736   V: 0.27513 0.88440 best\n",
    "  8: 116.6s   T: 0.14364 0.95680   V: 0.27759 0.88304 \n",
    "  9: 106.3s   T: 0.12834 0.96488   V: 0.28848 0.87756 \n",
    " 10: 100.8s   T: 0.11256 0.97276   V: 0.28174 0.88176 \n",
    "Best model was saved at epoch 7 with loss 0.27513: ../../models/sentimento_1\n",
    "Stop training at epoch: 10/10\n",
    "`\n",
    "#### Treinamento em GPU (GTX1080 Ti, 8GB):\n",
    "`\n",
    "  1:   5.8s   T: 0.68932 0.53556   V: 0.65994 0.63064 best\n",
    "...\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:58:37.843111Z",
     "start_time": "2017-12-15T16:57:16.590880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model training set accuracy after training: 0.95920\n",
      "\n",
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.88344\n"
     ]
    }
   ],
   "source": [
    "if 'ModelCheckpoint' in [cb.__class__.__name__ for cb in trainer.callbacks]:\n",
    "    trainer.load_state(state_fn)\n",
    "\n",
    "rmetrics = trainer.evaluate(Xtrain, ytrain, metrics=[accuracy_cb])\n",
    "print('Model training set accuracy after training: {:.5f}'.format(rmetrics['acc']))\n",
    "print()\n",
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[accuracy_cb])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rede convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:58:37.882645Z",
     "start_time": "2017-12-15T16:58:37.844790Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len=seq_len, voc_size=voc_size, embed_dim=embedding_dim, \n",
    "                 n_conv_filters=128, conv_kernel_size=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        k = conv_kernel_size - 1\n",
    "        n = (((seq_len - k) // 2 - k) // 2 - k) // 2\n",
    "        self.flat_size = n * n_conv_filters\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.embedding.weight)\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv1d(n_conv_filters, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv1d(n_conv_filters, n_conv_filters, conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = self.fc_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:58:37.927287Z",
     "start_time": "2017-12-15T16:58:37.884258Z"
    }
   },
   "outputs": [],
   "source": [
    "trainIt = False\n",
    "resetIt = False\n",
    "\n",
    "# embedding_dim = 50\n",
    "# batch_size = 100\n",
    "# n_epochs = 15\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_3'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=resetIt, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "model = MyNet(seq_len, voc_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, chkpt_cb, print_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T16:58:37.932799Z",
     "start_time": "2017-12-15T16:58:37.928868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training disabled.\n",
      "This model was trained for 0 epochs.\n"
     ]
    }
   ],
   "source": [
    "if trainIt:\n",
    "    trainer.fit(n_epochs, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    print('\\nTraining disabled.\\nThis model was trained for {} epochs.'.format(trainer.last_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento em GPU (GTX1080 Ti, 8GB):\n",
    "`\n",
    "Start training for 10 epochs\n",
    "  1:  10.0s   T: 0.51950 0.69276   V: 0.30188 0.87500 best\n",
    "  2:   9.7s   T: 0.26000 0.89792   V: 0.26565 0.88956 best\n",
    "  3:   9.7s   T: 0.20300 0.92276   V: 0.27181 0.88824 \n",
    "  4:   9.8s   T: 0.16678 0.93864   V: 0.29407 0.88728 \n",
    "  5:   9.8s   T: 0.12640 0.95380   V: 0.31985 0.87868 \n",
    "  6:   9.7s   T: 0.09621 0.96600   V: 0.35878 0.87376 \n",
    "  7:   9.7s   T: 0.08262 0.97012   V: 0.41535 0.87944 \n",
    "  8:   9.7s   T: 0.05677 0.98076   V: 0.48370 0.87468 \n",
    "  9:   9.7s   T: 0.04682 0.98400   V: 0.53459 0.87464 \n",
    " 10:   9.8s   T: 0.02949 0.98916   V: 0.62111 0.87328 \n",
    "Best model was saved at epoch 2 with loss 0.26565: ../../models/sentimento_3\n",
    "Stop training at epoch: 10/10\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:00:06.380392Z",
     "start_time": "2017-12-15T16:58:37.934274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model training set accuracy after training: 0.93916\n",
      "\n",
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.88956\n"
     ]
    }
   ],
   "source": [
    "if 'ModelCheckpoint' in [cb.__class__.__name__ for cb in trainer.callbacks]:\n",
    "    trainer.load_state(state_fn)\n",
    "\n",
    "rmetrics = trainer.evaluate(Xtrain, ytrain, metrics=[ptt.AccuracyMetric()])\n",
    "print('Model training set accuracy after training: {:.5f}'.format(rmetrics['acc']))\n",
    "print()\n",
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[ptt.AccuracyMetric()])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T20:15:36.885766Z",
     "start_time": "2017-10-26T20:15:36.880693Z"
    }
   },
   "source": [
    "## Resumo dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experimento *bag of words*: 87% de acurácia\n",
    "2. Experimento *word embeddings*, rede densa: 88%\n",
    "3. Experimento *word embeddings*, rede convolucional: 89%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
