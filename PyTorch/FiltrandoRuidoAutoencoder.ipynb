{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando ruído com autoencoder convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aplicação o autocodificador é utilizado como filtro de ruído. \n",
    "Utiliza-se o dataset do MNIST, adicionando ruído gaussiano.\n",
    "A rede é treinada com a entrada com a imagem ruidosa e a saída como a imagem original.\n",
    "\n",
    "Esta é uma rede totalmente convolucional e para tanto, a rede aceita imagens de entrada de quaisquer dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "#import codecs\n",
    "#import errno\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# from torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "# from torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.mnist import read_image_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: False\n"
     ]
    }
   ],
   "source": [
    "# verifica se a GPU esta disponivel\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"Usando GPU:\", use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe para carregar os dados e adicionar ruído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Noise(data.Dataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``processed/training.pt``\n",
    "            and  ``processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "    ]\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "        target = Image.fromarray(target.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        from six.moves import urllib\n",
    "        import gzip\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for url in self.urls:\n",
    "            print('Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
    "                    gzip.GzipFile(file_path) as zip_f:\n",
    "                out_f.write(zip_f.read())\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        train_file = read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte'))\n",
    "        test_file = read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte'))\n",
    "        \n",
    "        # add noise\n",
    "        train_noise = train_file + torch.from_numpy(np.random.normal(loc=0, scale=1, size=train_file.shape)).type(torch.ByteTensor)\n",
    "        train_noise = torch.clamp(train_noise, 0, 255)\n",
    "        \n",
    "        test_noise = test_file + torch.from_numpy(np.random.normal(loc=0, scale=1, size=test_file.shape)).type(torch.ByteTensor)\n",
    "        test_noise = torch.clamp(test_noise, 0, 255)\n",
    "        \n",
    "        # create sets\n",
    "        training_set = (\n",
    "            train_noise,\n",
    "            train_file\n",
    "        )\n",
    "        test_set = (\n",
    "            test_noise,\n",
    "            test_file\n",
    "        )\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e mostrando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "amostras para treinamento: 60000\n",
      "amostras para validação: 10000\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = './data/datasets/MNIST_Noise/'\n",
    "\n",
    "# Transformara os dados em tensores no intervalo [0.0, 1.0] (Os dados serão normalizados)\n",
    "data_transform = transforms.ToTensor()\n",
    "\n",
    "# carrega o conjunto de treinamento e de teste\n",
    "mnist_datasets = {\n",
    "    'train' : MNIST_Noise(dataset_dir, train=True, transform=data_transform, target_transform=data_transform, download=True),\n",
    "    'val'   : MNIST_Noise(dataset_dir, train=False, transform=data_transform, target_transform=data_transform, download=True),\n",
    "}\n",
    "\n",
    "print('amostras para treinamento:', len(mnist_datasets['train']))\n",
    "print('amostras para validação:',   len(mnist_datasets['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando algumas imagens do conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAACbCAYAAAATDWgqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XvUVkXd8PHfJIIgkAKGPiIJQR5K8QCaLhFJX3TlETRJUykfD6WigtpBeQ1FErUeweBJMhPJNE0rFdTMElHTMEVR1CWZj4FiIHI+KNR+/7hv5p0Z7z17z+y9r3vT8/2s5XI2s6/Zc1/7vK7fb0YlSSIAAAAAgPr4RGt3AAAAAABg40UNAAAAAGqGFzUAAAAAqBle1AAAAACgZnhRAwAAAICa4UUNAAAAAGqGFzUAQKtQSv2PUuqI1u5Ha1BKdVJKvaGUOqCFuq8ppZ5qjX4BAOqDFzUAQKbml6r1SqnVSqkVSqk/KaW+oZTiPhLnv0RkYpIkc1q7IwCAeuIGCwDI69gkSTqJyKdFZIKIfFtEbm3dLtWPUmqrjPrOIjI3SZL/blCXAABbIF7UAABBkiRZmSTJAyIyXERGKKU+LyKilDpaKTVXKbVKKbVQKTXW/JxS6nSl1NtKqWVKqSucunZKqYlKqXeb/5uolGrXXNdNKTWj+Ze8D5RST6b9kqeU+pxS6vfN6/1DKXV5jvYPU0otUkp9Sym1RCm1WCl1glLqS83hiR9sbidlm9OUUj9WSj2klForIoOVUrOUUmcZ6+hwxiRJVonIFKVUn+a6rkqpB5q/tzki8hmn/YOVUs8ppVY2///gPPsJALBl40UNABClOWxvkYgMbP6ntSJyhohsJyJHi8g3lVIniIgopfYUkR+LyOki8h8i0lVEehjNXSEiXxCRfUSkn4gcICJjmusuad7ODiLSXUQuF5HE7Y9SqpOIPCYijzRvo4+I/CFH+yIiO4rINiKys4hcKSK3iMhpIrJ/8993pVKqt+frOFVExotIJxEJzS+bIiIbRGQnETmz+b/Nf1MXEZkpIjdJ03f2XyIyUynVNXAbAIAtDC9qAIAi3hWRLiIiSZLMSpLk5SRJ/pUkyTwRuUtEBjWvd5KIzEiSZHaSJB+KyP8VkX8Z7XxVRK5OkmRJkiRLReQqaXqpExHZKE0vMZ9OkmRjkiRPJknysRc1ETlGRN5LkuSHSZJsSJJkdZIkf87R/uZtjE+SZKOI/FJEuonIpOY25ovIfBHZ2/M93J8kydPNf/uGrC9ts+YwyRNF5MokSdYmSfKKiNxurHK0iCxIkuTnSZJsSpLkLhF5XUSOzbsNAMCWiRc1AEARO4vIByIiSqkDlVKPK6WWKqVWisg3pOmFR6TpF66Fmz+UJMlaEVlmtPMfIvK2sfx287+JiNwgIn8VkUeVUn9TSn0npS+7iMibKXW+9kVEliVJ8s/m8vrm///DqF8vIh1T2hYx/rZAO4hIG+fzZj/dfm+u3zlyewCALQQvagCAKEqpAdL0wrA51O9OEXlARHZJkuSTInKziKjmusXS9CK1+bMdpCmUb7N3pWmQks16Nv+bNP+qdUmSJL2l6Zek0Uqpw1vo0kJx8rvytF8S9xe+tSLSwVjeMeVzS0VkkxjfjTT1bTO335vr34noIwBgC8KLGgAgiFKqs1LqGGkKEbwjSZKXm6s6icgHSZJsaJ4f7FTjY/eKyDFKqUOUUm1F5Gqx70F3icgYpdQOSqlu0pQndkfz9o5RSvVRSikRWSUi/2z+zzVDRHZUSl3cPHhIJ6XUgVntV+RFERmmlOrQPGjIf7a0UvOveL8WkbHN6+4pIiOMVR4Skc8qpU5VSrVRSg0XkT2l6W8FAPwb40UNAJDXg0qp1dL0y9UV0jSwxdeN+vNE5Ormda4UkXs2VzTneZ0vTb+6LRaR5dI0QMhm14jIX0Rknoi8LCIvNP+biEhfaRokZI2IPCMi/50kySy3c0mSrBaR/yNNv7q9JyILRGRwjvarcKOIfCRN4ZO3i8gvPOteIE1hle+JyDQRuW1zRZIky6Qp9+4SaQoV/ZaIHJMkyfuV9BoAUBuq5XxsAAAAAEBr4Rc1AAAAAKgZXtQAAAAAoGZ4UQMAAACAmuFFDQAAAABqhhc1AAAAAKiZNg3eHkNMAgAAAPjfTOVZiV/UAAAAAKBmeFEDAAAAgJrhRQ0AAAAAaqbROWqWrbbaSpf/9a9/pa6XJHZqm1LpYZ2+uixmH4q0k5e7Dffv9K2bt69Zf4e5TXf7n/jEJ1LrQvpeN2Zfi+xn87Ox+y6rD2ZdyOd8GrHvimwjZP/46sv6u0L6U9axldamSP7rZohG7S/fNnznU8g1ztf3vOdpVpu+vprXzbL2T4iQ7y6E754Tsg3fd9eI8ye2XXO/ttRu7PWmiv3lHndu32PF9jXkuunj/h1lPTeWxXds+/ZJWc+4sc8WIiL//Oc/W+xbVUL2paus60RZ7Zh99313sdcIflEDAAAAgJrhRQ0AAAAAakY1OGTN2ljesKWQ8Key/p4tObQvhC+0Lm29rPqQkJCQn/Eb8XO8T5FQVd/nfD+bx4YblXX8huyDkPPQrPeFFGW104hwFt/2YkO3WuP6UlUoUN5wH3c/+0KCs8LMfHz7IDbUJTb0PIv5d5qhR7713O27ffD11f1cmzZtUtet6twqa//4QscaEbrr437PZrhyVl9j+9CIcLDYvha5l8f+XUW2WcUzpasRIeO+c8SnqutdSHh7yHFoCgk9b8R+NrWwDYbnBwAAAIAtES9qAAAAAFAzvKgBAAAAQM3UJketSJx2WXGmsUN9t3aOXGzMsltf1j7wCcnrKmvo5tghfcuKxS5rWP2sYytvPHqRYdHNXAs3p6aKv9nlO0Z8fS8yFHBIf/LmebbG0NGxQq4hrSHkmPB9rsg207YfojVycmPvM43IgQrJyY2dksFtt8jw5r5txg6TXmQahLz9cY+zTZs2pdY1YqqDLSmXqrWvfa68uVtZ6/pU9ZxWVR5anZCjBgAAAAD/JnhRAwAAAICaaZO9SmOEDNXs8v3U6vspvKxwKN9Q4yFD3sf+/J93yPSY+rzy9reqENeQYf7zbqNIf8xlM1wwVFlTAOQNo8pazzeEeGzISlXhwiZfmGZWO0XCqtLa9X3PVYVp+rRv395a7tmzpy4//fTT3v68+uqrujx27Fir7u6779Zl9zwoa78XCaeuYpuxQkId3XV94XI+VYV1xYYMhqyXdzqHrJAvs76sKVCK7APfUP6mIs8avs81IuS2rGtakTD+kHbT6oqEcOY9v4qELMY+3/mOibKmzAmZ+sE37YvLdx6GXDd9IdGNDv/nFzUAAAAAqBle1AAAAACgZnhRAwAAAICaadUcNV8sa2ycdEj8eRW5HyFtjhw50loeOnRoi2URkRUrVqS2E5JD04j486qY+88XT9ymjX1Yjxo1KnV5xx13tOp23XVXXX777bdTt+Hy5Qu4x0TIPonNGYkdOtrN5fL1rcjQ543IaTz77LN1ecqUKVbd888/by1369ZNl/fff3+rbsmSJboccv6E9NWXb+OL5fcdd1k5LB07dtTlRx991Kq76667dHncuHFW3ZAhQ6zlM888U5dvv/12q27gwIEt9rslZv98+Wwhx0TscNW+XAZ3Xd+1KCSfw7euu+8GDx5sLT/xxBO6bO5XEZE1a9Z4+2CqIh+zEflIvnWzzsO8w9GHDEt+6KGHWnVXX321Lh977LFWnbt/8n7Pvu+1NfJcQ8Tmlfbq1cuqO+WUU6zl0047TZd9991HHnnEqps9e3ZqH2666SarztxfRb5X3/NMWVNhhMi7zazzMPaZJXZKiyLP/LE5jSF/Vxnn3pb71A4AAAAA/6Z4UQMAAACAmuFFDQAAAABqRlU9/r/D2lgVsbdlzZURso2Qdrbeemtdfumll6w6Mw/tqKOOsupWrVoV0sXcfLkEIfNI+Op8+SWxsdjuum3bttXln/zkJ1adGbcuIvLUU0/p8vjx4606M9fjww8/9PahrDyV2NhwX0x1bMx7WXOIhczfFzKfYJcuXaxlMw/ti1/8olXXr18/Xe7evbtVt27dOmv5tttu0+Vvf/vbVt3atWs//gek8M2fFzvPUWxurfvd7b777tby5ZdfrsuvvPKKVTdp0iRd/uijj7zbOeKII3R55syZVt3LL7+sy0ceeaRVt2zZMm+7abJyCfLmFTUq9yOvkDlAr7/+emv5kksu0WU3B+qhhx7K3Ye81zTf54poxP4JucaZ57BvDknX3LlzreW99tpLl817jIjI4YcfnrtdU1XzhMXu5yL3Fd9ne/furcszZsyw6nbbbbfc/fOt53tmMZ8XRESuu+46XQ45t2IVuSe3xjUub857hw4drLquXbtay4MGDdLlfffd16rr27evLi9evNiqe+CBB3TZPV4a8X0Ezo+cq0P8ogYAAAAANcOLGgAAAADUTKsOz19FiEpIiF4Vw/1mtXnyySfrsvmTvojIHnvsoctuqKMvLMa3Td9P+i0tpwkZlrwRQwG7/R49erQuu6GOY8aMsZYffPBBXW7fvr1VlxXuGKOq4WxD9kne86BI+GmIvOERbhjiRRddZC2b0yu4/Xnrrbd02RxCXuTjw9G/8847ebr9MSFDqse2EzJlgsn9PoYPH24tH3300bo8YsSI3O24HnvsMV3euHGjVWeGrLhTYcSGPmb1J3YKkiqG1Q8REno+ceLE1Hbc658ZnlXkWlRW2FAjpuZoRGhfbArEPvvsYy3vtNNO1rIbypUm5NnGDcPO+11mXcPKasc3NYYZNvrZz3429XMhQo6lQw45xFru37+/Lp9//vlW3bRp01L7FnK8mN9BFWHFLfUnb11IKocb3jhs2DBdvvTSS626Aw44ILUPvr67daeffrouH3TQQd6++sLLG/UslAe/qAEAAABAzfCiBgAAAAA1w4saAAAAANRMq+aoxcbIunw5Nmasb1b+Wt6YWF+ssS++WkTkxhtv1OXp06dbdWZOTZHhbENUMT2Dm1PjxsenCYnhHjhwoLU8btw4XZ48ebJVN2HCBGv5e9/7ni6fccYZVt3ee++ty2vWrPH2L2+eSlmxzkX2Vd7+FZl6IrY/AwYMsOrMnKfBgwd7+/PMM8/o8o9//GOr7o477tDlsWPHWnWdOnUK63CKKs5L9xriO39CjpehQ4day/fff3+u/hSZFsJ0wgknWMvz58/PtX1X1jEamyMbcg7nza/OEjulxg9/+MPUdny5S0XyvX2yhjtPqytruhbfNkL+jiqeSdzPbrfddlbdt771LWt51KhRudrx5TRm5Wman/U9B2XluKd9LktIPpB5P5g3b55V5z5fPffcc7r87rvvRvfHzN9t08Z+TN5mm210+Uc/+pFV99prr6W26Xs2dVVx3w3J7fWdM+4w+ub4ACIiL774oi67OeZufqbP+vXrddkccl9EpGfPnrr8hS98wapr166dLvfo0cOqq+r4rXqaM35RAwAAAICa4UUNAAAAAGqmtqGPRYbJNZlheGWFqPg+5w5HaoYxiIh07NhRl3/wgx+ktlNkaNeQz8WGiPja8YVq+bYRsj/c4cTfe+89XXZDHd1299tvP13etGmTVWcuZ313ZYXp+Lbha9cXlhI79URZx53LbcccQve6666z6rp3767L7nDvI0eOtJZnz56ty4sWLbLqzNBHN+zODTc655xzWvxcFt8+CPnufCFNrtjjbtKkSdby3/72t1z9CdnnvnXN/RqqrCkkYo/fkOGhQ/oaG5LsHiN5p2sJGVo7ZJqDsu7leT/n1vvOw6zwPV+on+979QlJjzjwwANzt5PWZkidiP8e7bsf+BQJK/MNk7527Vpddqee2Hbbba3lBQsW6PLy5cu92/QxnxHc8EYz1M593nP7YwrZXyH7Pfaa5jvWffr27Wstu+GNppDz4Oabb7aWZ86cqctz58616saPH6/LbuijOR3JjBkzvP3xyXuNd5W1f0z8ogYAAAAANcOLGgAAAADUDC9qAAAAAFAzrZqjVla+gE/eIfeLbNOsmzJlilX3ta99zVq+9dZbdfmNN97IvX3fULw+IXH+IULyBGPzS3x5cCeeeKJVZ051sHjxYm+722+/fYufExHZsGFD7v5VkZdW1fDZvs/5zpGQob59x9Lw4cOt5RtuuEGXu3XrZtWZw/u6w+q7w/TuueeeuuzmnQ0bNkyXzSGVRUQ++ugja/nwww/XZTeufcWKFVIG3/fsy9EImRairOtdLN82p06d6v2sb9j2EGW1k1fI+RR7/evcubNV179/f2s5Ni/YN2S4Ly8uZPqYkHuFb72yjue8Oe3uukW27/s+2rdvn7sdU5FpGPLm0jZqGHLf3+I77qri5jubynqeyXuOZN2TzXXd49fXV3fdtDZdbt64u2zeaxcuXGjV3XLLLbpsTqnUEjMf0W3H5D7vmc8M7vNcyPFcxTQrsfhFDQAAAABqhhc1AAAAAKgZXtQAAAAAoGZaNUfNxxdbGxsD6ovtzWrH1x8zT+aJJ56w6tz5H3xzToT01ceM6Q6ZC6eIMuYDytrP5rqzZs2y6gYOHKjLXbp0sep69+5tLR9yyCG6PHr06NR+N0oVcfcheQe+7fvyb3x1O+ywg1V36qmnWsuf+tSndPnNN9+06o488khdXrJkiVW32267WcuXXnqpLptzs4nYeTtjxoyx6txct/nz5+tyyHxAjZhnKHZOySLHVcg2e/Xqpctt2ti3lH/84x+6vHLlyujtx+aCxM5ll7Vf8163isz5Zi736NHDqjv00EOt5djrryt2Lsay8r19yjoGYtct8hwSkt9XxpyBIbnpZe3XIorMwRbDvP+IiHTt2tVanjZtmi67+aCmdevWWcvr169PXTfke/Y9w4WMV1BWHqe5f55//nmr7uijj7aWzZzze+65J/c2zz77bGv5kksu0WX3bzbv10OGDLHqli5dmrqNqubCLeu+koZf1AAAAACgZnhRAwAAAICaqc3w/FWFTpjrlvUTsrt9c0h+d6jx+++/31pevnx5aju+sDJ32QzrChmi2+ULjfR9d751Q9YL6au5rjvtgbkPHnnkEavOHYrd3AfuFAll/RSe1mZWu2UN7VpVeJEvNGrrrbfW5WuuucaqGzRokLV822236fKkSZOsOjPc8aSTTrLqrr32Wmt56NChuuyGoZx33nm6fO+991p17rqmIseoKXZY9NhpF0TChhsu6/g2p0Vo27atVWeGEPmGvBaxh4suEjIYe+yHDAMeO8S8u2/zXmMvvPDC1M+FaMR0BSLlhPsU2a+x4WAh50hsSLIv/Eokf7pCWdMFhGwjRN4weXc7vjr3u3H7Z15/+vTpY9WZaSf77befVfe5z30utV3f/powYYJV98ILL0gZfM93riqmqQjxhz/8Iardnj17Wsvjx4+3ls1wVPf+fdZZZ+ny6tWrc22vJXmP0bLSr2LxixoAAAAA1AwvagAAAABQM7yoAQAAAEDNtGqOWiOG+zXbCRny3ldnDscvIjJ79mxddoegnjhxYmpfQ+LYY4frLxKHXMWQ2CF1Pmbui4jIwoULdfmmm26y6tzhdV977TVdNvNispQ1XHVZuUJlTS9RVu7Htttuq8vnnnuuVecOvWvurxdffNGqGzBggC4/+eSTVl27du2sZTP/0I1xnz59ui5nfedlDWufdxtZuRa+dvJ+LqQ/Ie106tTJWh48eHDqunPmzMndri9fIG8urbtc1vDmsdc0X26xSP7rjzvty6hRo7z9K4PvO6hqmH9T1jGQ95itKt+wrOtmVs5aGULybsu6FoZMSVDWs6B5Xnz/+99PbSfkO/ddN6+++mqrrnv37rp8+eWXW3Vr1qxJ3YbL993FngdVca9pPuZzgHu/3n777a3lq666SpdvuOEGq86cBsF3b8jK9459Hm/EdCQmflEDAAAAgJrhRQ0AAAAAaqZVQx/Lkvdn85Bhr30/jQ8fPtyqM4fkP+OMM6w6M8zO7U+RkIeyQqXKCrsrK3Qhdvt//OMfdfnXv/61VTdmzBhreY899tDlefPmWXXuML2+bfp+Ng8Zmj2tzSxVhGIW+Um/Y8eOupx1rplD+R9//PFW3cMPP6zLbqijGwr5ne98R5efeeaZjB7nE3tuhbQbMhRwyDFhhoFkXV9iQ8f69etnLfft21eXly5datXNmjUrtV3f8Zt3iPKWxA49HhKamjcsJitkPW9f3etUVVOH+LZRVihQ3ntyyHQ6vnOmrNCwItfm1g5Pa8SUMEW26RNyjJqpJ77QWTdczzdtRsixfv755+vyKaecYtUdccQR1rJ5ToeEb8deQ4pMPREyzZW57k9/+lOr7swzz9RlM3xRROSEE06wlmfOnJmrr75rSNbfnDekPuuciA2hzItf1AAAAACgZnhRAwAAAICa4UUNAAAAAGrm3yJHrayhx3222247XX700UetuuXLl+uyO9S4O/xyyDDTppC4ZF87scP8u3z5Am7896ZNm1K3Fxs7726/TZv/fygffPDBVt1f//pXa/n000/XZTd/7fHHH9flHj16WHXuvvTlSMTGJRc5fqPjnz15Tb794x5Lbdu21eU333zTquvTp4+1bMaqf/e737XqzD48++yzVt2wYcOs5WXLlqX2zxQynHhIvktZOZ5p2wvdZkgebl7ufh45cmTquu4wyh988IEuh+QHhHDPS/P6E/t9xOYFurJyV/MeBz179sz9Od81PuvvMj8bO/2Hq6yc6ZD7gZkDG3IMFMl1Mz87depUq27y5Mm67N4f3Txcc9mcfiSrr2UpK4falDXVgu9e+o1vfEOXV61alfo5ETs//bzzzrPq9tprr9T+uPcZM7fW/ZvNqWe6dOkiacyxC0Q+ntv70ksvpX7Wd0/2resKyZet4hx2v2dz2Z3awMxJc9uJnU6hyN8ckrtf1bm4Gb+oAQAAAEDN8KIGAAAAADXDixoAAAAA1Ixq8Pwe1sZi4zp98zi4f48v5j4kdt2cO+2Xv/ylVXfLLbfo8jnnnJO7zbKUlTfjtlNGrofbn9j5iFzuuqeddpouT58+3apz5zO5++67dblDhw5W3YIFC3T5wgsvtOruu+++1P74YvBD8qNC1mvE/HTuNs2/0xcb/5nPfMZaduc/22mnnVK38eGHH+qym5vjztPl62tsPqhP1vdT1px0vm2WlUNi5mD26tXLqluyZIkuu/PbjB8/PnVdtx1zX2bJmycYkmfgu1cUOX98eRhVzHHmngdvvfVW6jbdvNtrr722xTZFyjtHymo3Ni+kyD6Ibcd3PXZz1c35ObPm6DvqqKN0+bHHHkvdRlXznYXkIpZ1rO+99966/NBDD1l13bt312Uzh1xEZMiQIan9MZ8JREQGDBigy+44A7///e+tZfO65Zsv1Dy3ROx51FzuOevmbZtC9nMjrkWxfDlqZv6yyMefq2fPnq3L7hyp77zzTlldjBLyHGue7y08R+faQfyiBgAAAAA1w4saAAAAANTMFjk8v2+YdF+oS8hw0J07d7aWR40apcvz5s2z6kaPHp3aTtawtHnFhvAUCVUICXc0t5MV2pG2jZDPuQ499FBdXrlypVXnhjmY1q1bZy1v3LhRl4877jirzhf6GBKeFhKeUNbw77HDz4f0Z5dddtFld2jkHXfc0Vo2z2F3v7/33nu67O6fEHmnc2ip3lRFuJFvG1l8+9Iclty9Lpnh2yJ2SJEZiipiT3tgTk3S0jbNUKUNGzZ41zX5Qq1dsedIFdMniPivd75QJFfeqTE6depk1T3xxBPW8qBBg3R5++23927TFBu6WyRMvYrQ0CJiw2HLmibDncpk7ty5qduo4m8uEv4aG+rtMu/f7rXI9MILL+Tuzx133GHV/fznP0/9XOw15LLLLkvdvhsGaV6b3XWLhJ/mfaZ01/M9R4fw9d2cCknEDsvu27evVec+M5jfnzsNj3mfcb+Pa665RpfNtKQiikwjUgZ+UQMAAACAmuFFDQAAAABqhhc1AAAAAKiZLTJHLST/xpe/5osDdoegPuCAA3T51ltvterWrFmT2o4vfjgkV6is+OHY4YezcrB8fc879HhWzorZdzdn48wzz9Tlm2++2apbvnx5ru27fQjJ1yqSX5e3P0XaKaNNt133b54xY4Yuf/7zn/e289JLL+nyokWLrLpjjz1WlydMmGDVXXDBBan9CYkbD8khic2jLGuIeV9/tt12W6vu1Vdf1WU31+Ppp5+2ls1pLNx8hYkTJ+buz4gRI3TZHcq/rOk3ymonr5DcHJfvmPRdG31/l5sneNhhh6W2M2vWrFx9a2k5lq+d2LzskHOkrKkWijD75w7P78tbdPfX+++/X2nf3HZ9dVn7oKznB1+d2c69996b+jl33ZDcu9jv2Z1+xMwvzFLW9FR5t1HW0P0hz0V33nmnVfeLX/xCl90ctWOOOSZ1O27dIYccostuvvnUqVN12X0WNKdfEhEZN26cLj/wwANW3erVq3W5tac54Bc1AAAAAKgZXtQAAAAAoGa2iNDHrLClvD8FZw1H2qtXL102w3lERF577TVdvuiii1LbiQ1hKiLkZ/yQYf5DQopCQjzT2skKszC3ceSRR6a2+dRTT6XWue20bdvWqjN/Rn/77be97YQMw532OZd7jIaEOZQ1VLLJbccMuTJDHUVE2rdvr8srVqyw6o4//nhr+dlnn9XlnXfe2aozw4y//vWvW3X9+vXL3dfY0F1fu7HD6IfUZa1rhv26Q0737NlTl93w7SuvvNJaNo/9s846K3X7b731lrXs7tt9991Xl91Ql8GDB+vynDlzUrfhig0pdeurCk0tKxQ9dphn3z3QvI9ltVlFaLVbX1bIVYiQe3Le60RWO+Y+8A0jn3VNz/v9+Palb8qIIu2E8H3WPSZGjhyZq01zeHcRO+VBJH9IZ5G/y9S/f39r2QwZb40wY1fs9S8kHDZ2+24Yohtub372xhtvtOoOP/xwXe7atatVd/HFF+uymxrgpmTcfvvtuuymBpjTA8yfPz+1b43AL2oAAAAAUDO8qAEAAABAzfCiBgAAAAA1U9sctdh4a187Lred0aNH63LHjh2tuuuvv16X161bl9qmLwfM7U9s3LiInUvl22ZWLLaMp63DAAAMDElEQVRZH5vrUYQvbtwX492lSxerbvHixbr8m9/8xrtN8/s5++yzrboePXro8uTJk739MdupKocldihen5D9PGjQIGv5scceS+2bmdP3zW9+06p78sknU/uzatWq1HVPPPFEq+7kk0+2lufNm5fad5/Y/LGq8mtC8kJ23XVXXR46dKhV99xzz+myG9fvMnMCzj333NT1Lr30Umv50UcftZZvu+02XT7ppJOsOjOX9M9//rNVF5JLG3utjJ1+I2QI6iLXRl+OsI/7/ZjLAwcOtOqmTJmiyyF5VkXy2crIl82a5iVkGPm0z4mEDePuq9tmm210eeutt07dhrv92bNnp/bP9x34/g7fc0dLnzW5w51Xwd3+l7/8ZV12c2LNnEt3mHb3WnTTTTfp8oEHHmjV7b///rr8l7/8xdsfU7du3azl3r176/IVV1xh1X3yk5/UZfc7/9nPfpa6jRCx+fBZ51Pe62bWM27ea3XI9STkGvKrX/1Kl91pnNz7pXltNIf8d9txr6lLly4tpa958YsaAAAAANQML2oAAAAAUDO8qAEAAABAzdQmRy0khjp23jD3c+acQyL2XEKvv/66VXfPPfekbsPXN99yVqyvjy8/qqw8A3NOr7LmHXGF5OaYf7M7f9SDDz6oyxs2bLDq3O+gT58+unzBBRdYdea8Gu+//35qX93l2PmQQtrxfS7ksyF9dfMFzPo333zTqhsyZEjq53x9db9nM88q63zyiZ1HzT0Oy8pZC+mPye2PGVfvfm7SpEm67ObJTJ061Vo28zPdc8acQ+a3v/1tat9ERE455RRdduc33LhxY2pffUKu/yGf9a2Xdx6hPH3IK29fjzvuOO/2zWu1r2/uPI1bbbVVarsh15eQPEFTyD3P7Wva9rLE5uZknbPmsb/77rtbdebf6ebk/ulPf0pdN+S+6/sufX0vMs9dyNyDvjrzs+YcmyL2/FbunJv77LOPtWzmy7o2bdqky24eu7ts9m+XXXax6vbbb7/UbZjfpXtNnTt3buo2QvINQ4ScF75jzTz3qnpWL2tOR7Od1atXW3XTp0+3ls0cx3feeceq69u3ry6bz4wtbTNt+yLlPDvzixoAAAAA1AwvagAAAABQM60a+pg37C0kTMhltuuGfVx22WXWcrt27XT5uuuus+rWr1+fa3tZP3vm/Xm3rOHei4QJhewTX7hG3tCxrP1s/vw+bdo0q84Ma3A/5w6ve9999+myOaSyiMjFF1+cq98tbSdNyDDTIWLDjVxmiJw73PGnP/1pa/mNN97QZTfM7e9//7sut23b1qozhzQWEdltt910+ZVXXrHqBgwYoMsrVqyw6ubMmfPxPyBFSHhCbLhRVUMMm9zQlzFjxqRu/6qrrtLlCRMmWHXm1BMidujqtddea9X5Qoh8/XOHP3788cdzt2OqajqQvNvICuMqI7xSJH+IkzklQ57tpHHDB8saSjp2GoSQcDn3/l1F38sKA/QdL25InDmtiYg/zCxte1l9DUmPCOELR4393MKFC61lMz1h7NixVl2/fv1yb9O8z7mhxG5/8oYbut+rGeo9btw4q84NIS8rJDj2Whly/MSGX4Y8oxx22GHWsjm0vpna4ipyrzCfNdz+fPTRR7psPl9m9aHI+0oaflEDAAAAgJrhRQ0AAAAAaoYXNQAAAAComVbNUYvN8fG14xu22N1ehw4drOUFCxbo8p133pmrb0X4YoTLym0I7UPe9XxxuGUN7+tbd/ny5VadOUT4oEGDrDp3SN9169bpsjmkvIjIypUrM3rccn9i8ybddlxlDWnuY+bpde7c2buuGbc9efLk1PW22247a/nggw+2ln3HyzPPPKPL55xzjlU3f/781HZCVDGErit2SPesc22nnXbS5YMOOsiq+9KXvqTLS5Ysseouuugia/muu+7S5WXLluXqW0vM/rk5abHnSJHh8H33g9jPxV7TQqYc8U3X4uYXuhYtWqTL5hQjvr61tBx7HsTeg3z7x+1LWcOA+xTJvTavne7UJe7w3mnbCBGyr8rK53PF9t3NN8x7nXCnVDr++OOt5VNPPVWX99xzT6vOzHkq61rkjl1g5qW54xz4npmK5FmVcb1zVTElTdZn3akWzO/y1ltvteruvvvuXNs375UiIsOHD7eWzful29cZM2bosjllkEjY828Z+db8ogYAAAAANcOLGgAAAADUjGrEMMgGa2MhISJpnxOJ/9lxxIgR1rI5JPVXvvIVq+6ee+7J1bes7fv+Zt/P/1XM2u4qK9ynqjALkzlrvIjIvffeq8vu0PDuvjPDWl9//XWrLmS6grxDULtiQ3iyho4uYx+cf/751rIbKjBw4MDUvvqObTOsWETk/vvv1+VbbrnFqjOH+XeHsq67vMP0+vZzWWEwZQ01nvW5vH3P6k9s6KHbrm9487JCenxivzs3HMw8h7761a9adcOGDbOWTzzxxNRtmt9PkbAl83sNCXmLvQdlDXOd9zwpck/2nU8+o0aNspbNaTPca+rDDz+cu928x1ZZ4WluO7FTJITsgyLnodmOG8Zv3tvclIfBgwdby+bfOXfuXKvOnN7HDTN+9913dTnr+PUdT+ZxGHI/CHl+8D1zVzU9lO/Y6t69u7U8a9YsXXaf9/KGTGedszNnztTl3/3ud1adGV7pptq47fquIb70KxHJ9cXyixoAAAAA1AwvagAAAABQM7yoAQAAAEDNtGqOmtWRAkPv5v1sVjt5Y1urGH6zqm348jfcdkPyZhqRh1ZWLuKWrKyco7zbKDIVRojYXIsiuTF51y2SPxGbZxqbF1LVediIXDff91PV9SY2Fyb2WtSIv8PV2tfmkM9WkWfr215Wu1Xdc6rIwSoyJYx7TTGVNdx7WVNzbNq0SZd9zy9Z2ykrj7wsvtxR8+/05a6K+K83vmfakP2V9/7Y0mdjmVP8uHmd5nQX/fv3t+rMv/P555+36tzxCl588UVd9k0ZEZLf59PCMUiOGgAAAABsiXhRAwAAAICa4UUNAAAAAGqmVXPUYnOg3PjQkLyVvMqa+6Ws2NbYPIiy4vPLEhIXXdbccWXldsXO19QauRZZn83LFw/v20ZV+UiN+JuLaMR8giHzxPjE5qLkncNGxO5f1rUv9tgKUbd2yth+Vh/yzt0p4s+NieXLmyly3Swrr9Q8RrNyoHzM78ttx9dm7D2xyL3cd16GzB3X2rmjjXgWjNWo/NSQPDRT7LxqRfINQ+bx9bVTVh6lr86XQxj7bEGOGgAAAAD8m+BFDQAAAABqpjahj5VtMCDsw1fnC9eIVWQo2bz9CRlK1fc3h7Rb1rCvrrKG3s37uZDjJXYo9qx1feuF7J8ytl9EWSHAriqG+na/j9jQqNjrW2tMhVHVMdHaIYIusz/uvjRD17K+j0b/LSH7x3euhUwv0RrKuh63hryhmFXdV/KGM7rLIdN/hITWxYbCh7Tjyht+6iorRM93roWEZVYxjUiWvM9iWd9V3nD3rPPX3H/mFA1Z7frEfj9FrpsZocSEPgIAAADAlogXNQAAAACoGV7UAAAAAKBmWjVHzeQbZrVRMbp5FcnnKGt43bLyvkK2WbXWyJ0KEZsjV2RfltWOL046JN67rCkT8sZ0h/zNIe248uaQZOUJ5r1uVZHb5rbbqOHN6ybv3+X7nCt2OPwi310jcnJj7x1Z52Fs7mhILlfea1pInrYr9l7qayckz6sR98SQa5iv70X6mvd7Lms4/pB2Yu+XIXlNIcd2yDm7JeVJu2LPGVNVf4d7DPjym811W5jyhBw1AAAAANgS8aIGAAAAADVTm9BHAAAAAPhfgNBHAAAAANgS8aIGAAAAADXDixoAAAAA1EybBm9vyxrnGQAAAABaAb+oAQAAAEDN8KIGAAAAADXDixoAAAAA1AwvagAAAABQM7yoAQAAAEDN8KIGAAAAADXDixoAAAAA1AwvagAAAABQM7yoAQAAAEDN8KIGAAAAADXDixoAAAAA1AwvagAAAABQM7yoAQAAAEDN8KIGAAAAADXDixoAAAAA1AwvagAAAABQM7yoAQAAAEDN8KIGAAAAADXDixoAAAAA1AwvagAAAABQM7yoAQAAAEDN8KIGAAAAADXDixoAAAAA1Mz/AyqnijXnu/n9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48228d9588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAACbCAYAAAATDWgqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHLBJREFUeJzt3XmUlMXZ9/FfKYsgKAcXRMEFQQ1BiQuagwgRExZBBfSIPojEBXwQRUCjiRgVWYToGzVoXpUQERUf4xaBMXEXEMXkcUPAFxAJAeMuy8i+1PvHDEVVMT0MQ09PTc/3c47H6+Lq6btmip7u4q7FWGsFAAAAAEjHXpXdAAAAAABAiIEaAAAAACSGgRoAAAAAJIaBGgAAAAAkhoEaAAAAACSGgRoAAAAAJIaBGgAgGcaYfxljfl7JbfibMaZfth+7i+c5wxizcE+fBwCQPwznqAEAysMY8y9JjSRtkbRV0gJJkyU9bK3dtgfPeaW19tUsNRMAgCqJO2oAgD1xjrW2vqQjJI2VdJOkiZXbpPIxRXhfBAAkgTckAMAes9auttZOldRbUj9jTCtJMsZ0M8Z8YIxZY4xZboy53f86Y0xfY8wyY8x3xpjhUa22MeZeY8x/iv+71xhTu7h2oDFmujFmlTHme2PMrEyDLGNMW2PMP40xq4v/39arvWmMGW2MmS1pnaRmxX92ZXF9b2PM/zHGfGuMWWqMucYYY40xNbyv3/7YXxpj3jLG3G2MWVn8+K7etS4zxnxijCk0xnxmjLnKq/3MGLPCy28yxnxe/NiFxpizytUxAIAqi4EaACBrrLX/kLRC0hnFf7RW0qWSGkjqJmmgMaaHJBljWkr6v5L6SjpU0gGSmnhPN1zSTyX9RFJrSadKuqW4dn3xdQ5S0fTLmyXtNJffGNNQUoGkPxQ//+8lFRhjDvAe1lfSAEn1JS2LnqK/pK7FbThJUo9d/AhOk7RQ0oGSfidpojHGFNe+ltRd0n6SLpN0jzHmpBLafKykayS1Kb5b2VnSv3ZxXQBAnmGgBgDItv9IaihJ1to3rbUfW2u3WWvnSnpSUofix10gabq1dqa1dqOk30ry17b1kXSHtfZra+03kkaoaFAlSZslNZZ0hLV2s7V2li150XU3SYuttY9Za7dYa5+U9P8kneM9ZpK1dn5xfXP09RdKus9au8Jau1JF0ztLs8xaO8Fau1XSo8VtbFT8syiw1i6xRWZIelk7BrS+rZJqS2ppjKlprf2XtXbJLq4LAMgzDNQAANl2mKTvJckYc5ox5g1jzDfGmNWS/ltFd5ukortoy7d/kbV2raTvvOc5VOEdrmXFfyZJd0n6VNLLxdMIf52hLfFzbH+ew7x8uTI7NKqX9lhJ+nJ7YK1dVxzWkyRjTFdjzJziqZqrJJ2tHT8LeV/3qaQhkm6X9LUx5n+MMYfGjwMA5DcGagCArDHGtFHRIOit4j+aImmqpKbW2v0lPShp+1TALyQ19b62roqmJ273HxVtUrLd4cV/JmttobX2emttMxXdHRuWYR1X/Bzbn+dzLy9t++MvFE7HbJrpgaUpXlv3rKS7JTWy1jaQ9KJ2/CwC1top1tp2Kmq7lTSuPNcFAFRdDNQAAHvMGLOfMaa7pP+R9Li19uPiUn1J31trNxhjTpX0X96XPSOpuzGmnTGmlqQ7FL4vPSnpFmPMQcaYAyXdKunx4ut1N8Y0L17/tUZF0wW3ltC0FyUdY4z5L2NMDWNMb0ktJU0v47f2F0nXGWMOM8Y0UNGuluVRS0XTGb+RtKV4k5FOJT3QGHOsMaZj8eBug6T1Kvl7AwDkMQZqAIA9Mc0YU6iiKYHDVbRZx2Ve/WpJdxQ/5lYVDXwkSdba+ZIGqeiu2xeSVqpog5DtRkn6X0lzJX0s6f3iP5OkFpJelfSDpHck/dFa+2bcOGvtdyrawON6FU2rvFFSd2vtt2X8/iaoaC3ZXEkfqGjgt/3cuDKz1hZKGqyi73+ligasUzM8vLaK1sJ9q6KplAeraLMUAEA1woHXAACUUfGdsAettfF0SgAAsoo7agAAZGCMqWOMObt42uRhkm6T9HxltwsAkP+4owYAQAbFG5zMkHScitaKFUi6zlq7plIbBgDIewzUAAAAACAxTH0EAAAAgMQwUAMAAACAxNTI8fWYZwkAAACgOjNleRB31AAAAAAgMQzUAAAAACAxDNQAAAAAIDG5XqMWMKZM0zORI/FRDfRPWuif9Pl9RP+kh/5JG/2TNt6D0kb/pK28x6FxRw0AAAAAEsNADQAAAAASw0ANAAAAABLDQA0AAAAAEsNADQAAAAASw0ANAAAAABLDQA0AAAAAEsNADQAAAAASw0ANAAAAABJTo7IbAACoXHXq1Anypk2bunjixIlBrV27dkG+YMECF99+++1B7emnn85SCwEAqH64owYAAAAAiWGgBgAAAACJYaAGAAAAAIkx1tpcXi+4mDEml9dOzuDBg4O8V69eLu7Ro0dQW7VqVYW3J/67UFX7p2bNmkE+dOjQIB82bJiLDz744KB21FFHuXjZsmUV0Lryy5f+yZX+/fu7+IQTTghq8Tqrhg0burhNmzZB7euvvy7zNf0+Sr1/6tWr5+JXXnklqE2ZMsXFCxcuDGqdO3cO8ssvv9zFtWvXDmq/+MUvXDx79uzyNzZLqlL/lKZDhw5BPmLECBd37949qP3www85aVM25Ev/tG/fPshL65+1a9fmpE3ZUNnvQUceeWSQX3zxxUF+ySWXuPhHP/pRxud56aWXgnzWrFlB7n+f999/f1ArLCwsU1srQ2X3D0pXwnirTB3EHTUAAAAASAwDNQAAAABIDAM1AAAAAEgMa9RyrEaNHUfXzZ07N6j569C6dOkS1NasWVOxDVPVnt9cq1YtF0+YMCGo9e3bN8hnzpzp4jFjxgS1GTNmuHjjxo3ZbOIeq8r9ky3+WjIpXIfWsWPHoNa6dWsXx2sR169fH+SPPPKIi2+66aagtjtrSFJeY3PssccG+c033+ziefPmBbU//OEPLt7V6+DnP/+5iwsKCoLaxx9/7OJ4bdt33323ixZnX8r9szvuvvvuIPfX4Z5zzjlB7cUXX8xJm7IhX/rngw8+CPLjjz/exf57jCSdddZZOWlTNlTGe9DRRx/t4vjvcvPmzbNyjfj78L/Pt956K6iNGzcuY3sqW1X6jFC3bt0gP+CAA4LcX4d74oknBrUWLVq4+Isvvghq06ZNc/H06dP3uJ3ZxBo1AAAAAMgTDNQAAAAAIDE1dv0QZFPv3r1d3KxZs6DWsmVLF+diqmM+uf76610cT3UcPnx4kPu3xvfZZ5+gltp0x+por712/PvRDTfcENSGDBkS5IccckjG51m6dKmLr7jiiqAWb0e/YsWK3W5nVfOTn/wkyP1twvv161fu53311VddvHnz5qDmT1mJ+6oypj5WB/Hvv9SmZ1V38euwcePGQR5P5aru/KNVsjXVcXfER7mccsopLr7mmmuCmj+FHjsf19KtWzcX+5/ZJOm0004Lcn/a5u4s0erTp4+L/Wn5kjRnzpwyP09KuKMGAAAAAIlhoAYAAAAAiWGgBgAAAACJYY1aBfO35ZWke+65x8WPPfZYUPvss89y0qZ8EM8bHzlypIvHjx8f1MaOHRvkt912m4svvfTSoOb31+5sy47y8+f8S+H2x2eeeWapX/v222+7+MEHHwxqjz/+uItvv/32oFavXr3dbWaV16NHjyB//vnnc3r9nj17Bvn8+fNzen0gBQ0aNAjyG2+8Mcj9oxYQroGNjxFp1apVkL/77rsu3p21fvE29v5aKv9IJSlc1+4fYyJJn3zyiYur6nqoXYm30Y/XmvlHU8RH3cTrM0vjH6ETb7PfpEkTF//0pz8NanXq1CnxcVUZd9QAAAAAIDEM1AAAAAAgMUx9zDL/tqu08zSGfffd18V33XVXTtqUj+LtxL/88ksXx1Md461d/S3Dt2zZEtS2bt2arSaiFP6UU3+qoyQ1atTIxZs2bQpqgwcPDvKZM2e6ON5i35/6GE/7i6cbDRgwoMSvyyfxlOAlS5bk9PoHH3xwTq8HVAVt2rSp7CYkrbCw0MUXX3xxUPM/T0nSp59+6uKVK1eW+5onnXSSi+Pfm/5Uu7p16wa1OM9HxxxzTJDH76W+eEppadvsT5gwIcj96Y7+dEpJGj16tIvjqY8FBQUlxlUZd9QAAAAAIDEM1AAAAAAgMQzUAAAAACAxrFHLsnjd2S9/+csgnzhxoosXLVqUiyblpQsuuCDI/aMOdrUtr7+97OTJk4Pahg0bstA6xC666KIg/93vfufieO3Shx9+6OJ4W/2pU6cGecuWLV0cbwXcq1cvF/tbKks7r30766yzXBxvBbxq1SrlA/8og8rw0EMPVer1q7L69esH+cknnxzke+21499c43UhyL24D/zc7ytp53VWyGzBggU5uc7nn3/uYl5Pofi9c/PmzUFeq1YtF/s/Rylch+YfqVSSSy65xMXLly/P+Lj4896IESNc7G/xX5VxRw0AAAAAEsNADQAAAAASw0ANAAAAABLDGrUs8NfJXHjhhUEtPv+htDMnUHbxz/X00093ccOGDYNas2bNgrxt27YuHjZsWAW0DgcddFCQ9+nTJ8j9dWlLly4Nap07d3bxN998E9TiM1xuuOEGF/ft2zeo7b333i6+9dZbg9pf//rXIJ83b55QPv7ry/+ZS9LXX3/t4tWrV+esTfmmQYMGQX7GGWcE+bZt21xc2llFyI24D/zc76uSHouKF6+L9tetS9KkSZNcfMopp2R8nnXr1pWa56P33nsvyLt16xbk/uevZ555pszP279//yC//vrrXRy/Rvz3606dOgU1/z0nX3BHDQAAAAASw0ANAAAAABLD1McsuPrqq1184IEHBrUXXnghyFeuXJmTNuW7eFv9Bx54wMV///vfg1q8nay/3frChQsroHXVU40aO36djB49Oqi1b98+yP2pJffdd19Q86c7+lvsSztPoezZs6eL4614Bw4c6OJ4CkZ1mKKSK/70U39rZins5xUrVuSqSXnn2muvrewmAMmrWbOmi1u0aBHU/ONbTjzxxKDWqlWrIC/rdNSxY8cGebwkozp4/fXXy/V1TZs2DfL4M4M/HfXZZ58NaldccYWLCwsLy3X9qoQ7agAAAACQGAZqAAAAAJAYBmoAAAAAkBjWqJXDcccdF+S9e/d2cbwF9b333puTNlU3/toXSVq+fLmLx48fH9Ti/vrkk09cvHXr1uw3rpqqXbu2i6+88spSH+v310cffRTU2rRp4+Innngi4zWkcP3hqFGjglq8jhHZUb9+/SDv2LFjxsf+4x//qOjmVAszZswIco4VAXbmvy7GjBlT4de74447grxRo0Yuvvnmm4PaDz/8UOHtSZ2/BX+8Ji0+VmnkyJEuHjduXFCL16PnO+6oAQAAAEBiGKgBAAAAQGKY+lgOrVu3DnJ/G9HLLrssqM2fPz8nbaruXnvtNRfHW7kOHz48yFu2bOniuXPnBrXu3bu7eMGCBdlsIjz+Vv7nnntuUJs4caKL46mOs2bNCvJf//rXLn7nnXey2URkEP/+87fB9o9WkKQ333wzF03Ke/HvKQA7i5ee5NqgQYNcfNFFFwW1zp07B3l12Mr/4YcfDnJ/W/14+uJ5550X5AUFBRXXsCqGO2oAAAAAkBgGagAAAACQGAZqAAAAAJAY1qiV0X777efieGvkVatWufj999/PWZuww9577+3itm3bBrXFixcH+aWXXuri3/72t0HtjTfecHGTJk2C2ubNm/e4nflsy5YtLl6yZElQO/roo4Pcn6v+m9/8JuNzzpkzJ8h79eoV5N99991utxO7Z6+9wn/Pu/baazM+9u677w7ylStXVkibqjtjTJD7fRTXkHsPPfRQkD/wwAMujl9PNWvWDHJ/Xe7GjRsroHVVy1VXXeXiNWvWBLUnn3wyyJ977jkXX3311UGtVatWGa8Rv8+UtrZ2wIABLo63lPcddNBBQX788ccHeXVYo+a/z0uStdbF8dEGrEnLjDtqAAAAAJAYBmoAAAAAkBgGagAAAACQGOPPGc2B4GJVaS597969XRzPi/bPferfv3/O2pRt8d+FqtQ/l1xyiYsnT54c1OLzTP7yl7+4uG7dukHt008/dXG8Fic+ny3XqlL/xGvSZs6cGeSNGzfO+LX+uozDDz88qMXndKXG76OU+0cKf7al/Zx79uwZ1EaPHh3kX331lYubNWsW1DZs2LDH7cymqtQ/vqZNmwb50qVLg9z/Xm655Zagduedd1Zcw7KsqvZPLF5/5K9Pir+v+Pd6165dXfzKK69UQOvKr6Leg0444QQXv/jii0GtUaNGLvbXkEtSp06dMj5n3759g7xNmzYufvnll4Na/HMubW3gvvvu6+L4teWfoxaLX7PNmzfP+NjySu0zwtatW4Pcb9/3338f1J566qkg9z8zxGekrlixIltNzKkSxltl6iDuqAEAAABAYhioAQAAAEBi2J4/A387fkkaOnSoiz/++OOgFm/Xj9xr166di1evXh3USps+sm7duiDftGmTi88999ygVtlTH1PnT8+Kp42WNtUx9uWXX7o47h+Uzt/qO/695E/flsIpRYccckhQ87fV33///Uu9pj9VKbWpjvnCPwJGkmbMmBHkP/vZz1y8q/5CWj777LMgr45H/LRv397Fpb1X7M7P5rHHHis1L6+1a9e6+Fe/+lVQ27Ztm4vj98BatWpl5fpViX8UkhROy27RokVQGzhwYJD7xyvEx/CsX78+4zVHjRrl4gkTJpS9sQnjjhoAAAAAJIaBGgAAAAAkhoEaAAAAACSGNWoZxFtQn3rqqS72t+OXpMLCwpy0CTvUr18/yK+88koXP/jgg0HNX2+DijNt2jQX+9stl+TDDz908fLly4PaOeec4+IxY8YEteuuu25Pmph36tSpE+Tz58938aGHHhrUZs+eHeT+MRbxNsr33ntvmdvgr0Pw/w5I0gsvvFDm50FmDRo0CPIOHTpkfGx8FAbSFm85H6/HwQ6prROPt/H/6KOPKqklaXriiScy5sccc0xQ69atW5D7RwucffbZQe300093cbz27+GHH3Zx/Flw8eLFQT5y5EgXT506Nail9LmeO2oAAAAAkBgGagAAAACQGKY+eo466igX9+vXL6gtWLDAxUOGDMlZm1CyLl26BPlee+34N4e33nqrzM+zzz77BHmNGjteEvG2ydWRP/1ACrcBnz59elDzf5bxduLnnXdekM+ZM8fFhx12WFA77bTTXHzFFVcEtXja8dy5czM1PW/Vq1fPxY888khQO+KII1wcT9++9dZbg9zfyr9///4Zr7d06dIgj/v2xBNPdPGUKVOCWseOHV387rvvZrwGsufII4+s7CYAFWL48OFBfvnllwf5999/n8vmBEtiJOmee+7J6fWrskWLFpWa+37/+98Huf++0rBhw6DmH6W17777BrVWrVoF+aOPPurit99+O6j5xwPMmzcvY9tygTtqAAAAAJAYBmoAAAAAkBgGagAAAACQGNaoeYYNG+Zifx2IJN11110uXrt2bc7ahJIdcMABQf7555+7+Pnnny/z8/jb+kvhlub3339/OVuXP84444wgf+211zI+9t///reLBw4cGNRmzZqV8evWrFmT8bEXXHBBULvwwguDvDquUfPXIPXq1Suo/fOf/3TxrtZL3HfffS6+6qqrMj7uhhtuCPKXX345yCdNmuTi888/P6h16tTJxaxRy5547ai/Rrd9+/ZB7Y9//GNO2lTd1a5d28X++k8p7C+/r6TSfzdWF1999ZWL47XhzZo1c7F/dIu08+8i/z3b31dACo8gee+998rctgMPPDBje+I1c/GxQb4///nPZb4mSvf6669nrD3zzDMujvujZ8+eQf7AAw+42N/yX5KefvppF7dr1y6o5foIDe6oAQAAAEBiGKgBAAAAQGIYqAEAAABAYqr1GrWmTZsGuX9m08KFC4PaU089lZM2oWzatm0b5NOmTXPxxo0bS/3a5s2bu3jw4MFBbfLkyS7O9ZksKfLPGYnFawn89Ui7cwZd/HM+/PDDXWytDWrbtm0r8/PmK/98l5i/7sw/E1CSHnrooSD312fGr5lBgwa5+IUXXghqcZ9cdNFFLo77ctOmTRnbirI799xzg7y010VcQ2506NDBxccdd1xQ8/skPodw9uzZFduwKsBfDxSvZfXPRY3P3PTPcJR2PmfTt2XLFhcXFBQEtXjNp99f8efEk046KeM1fPHvvvfff79MX4fsKSwsDHL/850kvfLKKy5esWJFUGvRokWJscQaNQAAAACo9hioAQAAAEBiqvXUxxtvvDHI99lnHxePHTs2qG3YsCEnbUL5+NMaYvFW/s8995yL/S2VpfCIhuoyhcjfSjre7viII44I8kWLFrm4S5cuQW3ZsmUujn+u/rEHcd2f9iJJP/7xj10cTxPyt5+vruIpGr4RI0a4OP4d1qRJkyD3p6feeeedQe2RRx4pc3v8aXc9evQIam+++WaZnweZxa9DVF3r168P8qVLl1ZSS9LkH/MiSddee62Lb7vttqDWunXrMj+vPxX8vPPOC2rxkQnlnWLvT3f0fxdL0tSpU8v1nNWFP3VYCrfWnz59eoVc8+STT3Zx/HfA/8xf2VP4uaMGAAAAAIlhoAYAAAAAiWGgBgAAAACJqdZr1OrUqRPkS5YscfETTzyR6+ZgN6xcuTLIL774Yhe3b98+qMVrc9atW+dif0t5qXpuye+vF9tvv/1Kfaw/V3v8+PEZH7f//vsHebt27crcnjlz5rh4wIABQW3evHllfp585a8fi4+p6Nq1q4u//fbboDZkyJAgnzJliouztd0wa9Iqxq7WqPnrFidNmlTBrUFJ3nnnHRcvXrw4qMXbe6Ps/ONB4i3u42Mr+vTp42J/rbMk1atXL+tt8z9LSNKoUaNcPG7cuKxfL5/Fxx6MHDnSxX/605+CWlmPy2rcuHGQ9+7dO8jPPvtsF8frEv11cZV9tAJ31AAAAAAgMQzUAAAAACAxJsdbkAcXi0+Dz7V+/foFuT+lKL5FGm8hno/ivwuV3T+lad68eZA/++yzLva3m5d27jt/Wqu/3XzqctE/gwYNCvL4dbA7Uxh98VQgf6viCRMmBDV/e+aqdiyG30cpv36qq6raP/6ULknq1atXkJ9//vm5bE6Fqar9Exs6dGiQ+1u1x79T//a3v+WkTdlQlT4j+Nu7S+F7W7zk4cwzzwxyfxpcPO3N/6zx6KOPBrUvvviifI3NkqrUP7F4muIbb7zh4vjzXmn873lX45uCggIXv/TSS0HNn16ZraUBJbSnTB3EHTUAAAAASAwDNQAAAABIDAM1AAAAAEhMtV6jhlBVnt9cHdA/6cuXNTb5iv5JG/2TNt6D0pZP/dOgQQMXx+s6/TVrJ598csbneO+994I83tb/gw8+cPHWrVvL1c7dwRo1AAAAAMgTDNQAAAAAIDEM1AAAAAAgMaxRg5NP85vzEf2TPtbYpI3+SRv9kzbeg9JG/6SNNWoAAAAAkCcYqAEAAABAYhioAQAAAEBiGKgBAAAAQGIYqAEAAABAYhioAQAAAEBiGKgBAAAAQGIYqAEAAABAYhioAQAAAEBiTAknZVeknF4MAAAAABJjyvIg7qgBAAAAQGIYqAEAAABAYhioAQAAAEBiauT4emWajwkAAAAA1Rl31AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAxDNQAAAAAIDEM1AAAAAAgMQzUAAAAACAx/x+JaEVLv1NGqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48227b9e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 8\n",
    "\n",
    "# cria um DataLoader temporario para pegar um batch de 'n_samples' imagens de treinamento\n",
    "temp_dataloader = torch.utils.data.DataLoader(mnist_datasets['train'], \n",
    "                                              batch_size = n_samples,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "\n",
    "# pega um batch de imagens\n",
    "image_batch, labels = next(iter(temp_dataloader))\n",
    "\n",
    "# cria um grid com as imagens\n",
    "grid_data = torchvision.utils.make_grid(image_batch, normalize=True, pad_value=1.0, padding=1)\n",
    "grid_labels = torchvision.utils.make_grid(labels, normalize=True, pad_value=1.0, padding=1)\n",
    "\n",
    "# dados com ruído\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Dados com ruído\")\n",
    "plt.imshow(grid_data.numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# dados originais\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Dados originais\")\n",
    "plt.imshow(grid_labels.numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "del temp_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria DataLoader para os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_datasets['train'],\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(mnist_datasets['val'],\n",
    "                                         batch_size=10000,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição das classes que representam as redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# classe do AutoEncoder (Encoder + Decoder)\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# classe do Encoder\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encode_conv1 = torch.nn.Conv2d(1, 16, 3, stride=3, padding=1)\n",
    "        self.encode_pool1 = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.encode_conv2 = torch.nn.Conv2d(16, 8, 3, stride=2, padding=1)\n",
    "        self.encode_pool2 = torch.nn.MaxPool2d(2, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encode_conv1(x))\n",
    "        x = self.encode_pool1(x)\n",
    "        x = F.relu(self.encode_conv2(x))\n",
    "        x = self.encode_pool2(x)\n",
    "        return x\n",
    "\n",
    "# classe do Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decode_conv1 = torch.nn.ConvTranspose2d(8, 16, 3, stride=2)\n",
    "        self.decode_conv2 = torch.nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1)\n",
    "        self.decode_conv3 = torch.nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.decode_conv1(x))\n",
    "        x = F.relu(self.decode_conv2(x))\n",
    "        x = F.tanh(self.decode_conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(Encoder(), Decoder())\n",
    "if use_gpu:\n",
    "    autoencoder = autoencoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:28:08.661324",
     "start_time": "2017-08-12T17:28:08.339147"
    }
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(None, None, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumário da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que nesta rede, as dimensões das imagens em todas as camadas podem ser quaisquer, com exceção dos canais (última dimensão) que são 1 na entrada (imagem de cinza) e 1 na saída (imagem de cinza).\n",
    "Assim, esta rede é dita totalmente convolucional, aceitando portanto imagens de cinza de qualquer tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:28:08.674836",
     "start_time": "2017-08-12T17:28:08.663117"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (encode_conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n",
      "    (encode_pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    (encode_conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (encode_pool2): MaxPool2d(kernel_size=(2, 2), stride=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decode_conv1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (decode_conv2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1))\n",
      "    (decode_conv3): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poucas amostras para teste inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:28:08.690418",
     "start_time": "2017-08-12T17:28:08.677869"
    }
   },
   "outputs": [],
   "source": [
    "testing = True\n",
    "if testing:\n",
    "    n_samples = 1000\n",
    "    x_train = x_train[:n_samples]\n",
    "    x_train_noisy = x_train_noisy[:n_samples]\n",
    "    \n",
    "    n_samples_test = 200\n",
    "    x_test = x_test[:n_samples_test]\n",
    "    x_test_noisy = x_test_noisy[:n_samples_test]\n",
    "print(x_train.shape, x_train_noisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:28:08.700356",
     "start_time": "2017-08-12T17:28:08.693017"
    }
   },
   "outputs": [],
   "source": [
    "print(x_train[0].mean(),x_train_noisy[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(loader, model, loss_fn):\n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(Variable(images))\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(outputs, Variable(images))\n",
    "    return (loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - 13.0 s \t train loss: 0.14572 \t val loss: 0.14335\n",
      "Loss improved, saving model from epoch 0\n",
      "Epoch: 1 - 14.0 s \t train loss: 0.14085 \t val loss: 0.13651\n",
      "Loss improved, saving model from epoch 1\n",
      "Epoch: 2 - 13.0 s \t train loss: 0.13863 \t val loss: 0.13488\n",
      "Loss improved, saving model from epoch 2\n",
      "Epoch: 3 - 14.0 s \t train loss: 0.13584 \t val loss: 0.13352\n",
      "Loss improved, saving model from epoch 3\n",
      "Epoch: 4 - 13.0 s \t train loss: 0.13604 \t val loss: 0.1324\n",
      "Loss improved, saving model from epoch 4\n",
      "Epoch: 5 - 13.0 s \t train loss: 0.1345 \t val loss: 0.13166\n",
      "Loss improved, saving model from epoch 5\n",
      "Epoch: 6 - 15.0 s \t train loss: 0.13608 \t val loss: 0.13124\n",
      "Loss improved, saving model from epoch 6\n",
      "Epoch: 7 - 16.0 s \t train loss: 0.13394 \t val loss: 0.13095\n",
      "Loss improved, saving model from epoch 7\n",
      "Epoch: 8 - 15.0 s \t train loss: 0.13262 \t val loss: 0.13096\n",
      "Epoch: 9 - 17.0 s \t train loss: 0.13374 \t val loss: 0.13066\n",
      "Loss improved, saving model from epoch 9\n",
      "Epoch: 10 - 14.0 s \t train loss: 0.1345 \t val loss: 0.12998\n",
      "Loss improved, saving model from epoch 10\n",
      "Epoch: 11 - 15.0 s \t train loss: 0.136 \t val loss: 0.12882\n",
      "Loss improved, saving model from epoch 11\n",
      "Epoch: 12 - 15.0 s \t train loss: 0.12966 \t val loss: 0.12831\n",
      "Loss improved, saving model from epoch 12\n",
      "Epoch: 13 - 15.0 s \t train loss: 0.13413 \t val loss: 0.12814\n",
      "Loss improved, saving model from epoch 13\n",
      "Epoch: 14 - 15.0 s \t train loss: 0.1339 \t val loss: 0.12775\n",
      "Loss improved, saving model from epoch 14\n",
      "Epoch: 15 - 15.0 s \t train loss: 0.135 \t val loss: 0.12758\n",
      "Loss improved, saving model from epoch 15\n",
      "Epoch: 16 - 16.0 s \t train loss: 0.13171 \t val loss: 0.12743\n",
      "Loss improved, saving model from epoch 16\n",
      "Epoch: 17 - 15.0 s \t train loss: 0.13292 \t val loss: 0.12734\n",
      "Loss improved, saving model from epoch 17\n",
      "Epoch: 18 - 17.0 s \t train loss: 0.13298 \t val loss: 0.12742\n",
      "Epoch: 19 - 16.0 s \t train loss: 0.13435 \t val loss: 0.12746\n",
      "Epoch: 20 - 16.0 s \t train loss: 0.13227 \t val loss: 0.12711\n",
      "Loss improved, saving model from epoch 20\n",
      "Epoch: 21 - 16.0 s \t train loss: 0.13279 \t val loss: 0.12689\n",
      "Loss improved, saving model from epoch 21\n",
      "Epoch: 22 - 15.0 s \t train loss: 0.13426 \t val loss: 0.12687\n",
      "Loss improved, saving model from epoch 22\n",
      "Epoch: 23 - 15.0 s \t train loss: 0.13336 \t val loss: 0.12702\n",
      "Epoch: 24 - 15.0 s \t train loss: 0.13354 \t val loss: 0.12678\n",
      "Loss improved, saving model from epoch 24\n",
      "Epoch: 25 - 15.0 s \t train loss: 0.13287 \t val loss: 0.12668\n",
      "Loss improved, saving model from epoch 25\n",
      "Epoch: 26 - 15.0 s \t train loss: 0.13569 \t val loss: 0.12671\n",
      "Epoch: 27 - 16.0 s \t train loss: 0.13222 \t val loss: 0.12687\n",
      "Epoch: 28 - 15.0 s \t train loss: 0.13367 \t val loss: 0.12677\n",
      "Epoch: 29 - 16.0 s \t train loss: 0.13118 \t val loss: 0.12664\n",
      "Loss improved, saving model from epoch 29\n",
      "Epoch: 30 - 16.0 s \t train loss: 0.13203 \t val loss: 0.12644\n",
      "Loss improved, saving model from epoch 30\n",
      "Epoch: 31 - 15.0 s \t train loss: 0.1322 \t val loss: 0.12631\n",
      "Loss improved, saving model from epoch 31\n",
      "Epoch: 32 - 16.0 s \t train loss: 0.13025 \t val loss: 0.12633\n",
      "Epoch: 33 - 16.0 s \t train loss: 0.13145 \t val loss: 0.12644\n",
      "Epoch: 34 - 16.0 s \t train loss: 0.13166 \t val loss: 0.12634\n",
      "Epoch: 35 - 16.0 s \t train loss: 0.13433 \t val loss: 0.12644\n",
      "Epoch: 36 - 16.0 s \t train loss: 0.13334 \t val loss: 0.12631\n",
      "Loss improved, saving model from epoch 36\n",
      "Epoch: 37 - 16.0 s \t train loss: 0.13355 \t val loss: 0.12646\n",
      "Epoch: 38 - 16.0 s \t train loss: 0.13408 \t val loss: 0.12597\n",
      "Loss improved, saving model from epoch 38\n",
      "Epoch: 39 - 16.0 s \t train loss: 0.13114 \t val loss: 0.12611\n",
      "Epoch: 40 - 15.0 s \t train loss: 0.13245 \t val loss: 0.12608\n",
      "Epoch: 41 - 16.0 s \t train loss: 0.13416 \t val loss: 0.12609\n",
      "Epoch: 42 - 15.0 s \t train loss: 0.13122 \t val loss: 0.1261\n",
      "Epoch: 43 - 16.0 s \t train loss: 0.13226 \t val loss: 0.12604\n",
      "Epoch: 44 - 15.0 s \t train loss: 0.13406 \t val loss: 0.12611\n",
      "Epoch: 45 - 15.0 s \t train loss: 0.13439 \t val loss: 0.12594\n",
      "Loss improved, saving model from epoch 45\n",
      "Epoch: 46 - 16.0 s \t train loss: 0.13048 \t val loss: 0.126\n",
      "Epoch: 47 - 15.0 s \t train loss: 0.13406 \t val loss: 0.12578\n",
      "Loss improved, saving model from epoch 47\n",
      "Epoch: 48 - 16.0 s \t train loss: 0.13001 \t val loss: 0.12591\n",
      "Epoch: 49 - 15.0 s \t train loss: 0.13561 \t val loss: 0.1259\n",
      "Epoch: 50 - 16.0 s \t train loss: 0.13037 \t val loss: 0.12596\n",
      "Epoch: 51 - 16.0 s \t train loss: 0.13086 \t val loss: 0.12585\n",
      "Epoch: 52 - 15.0 s \t train loss: 0.1347 \t val loss: 0.12589\n",
      "Epoch: 53 - 16.0 s \t train loss: 0.13211 \t val loss: 0.12567\n",
      "Loss improved, saving model from epoch 53\n",
      "Epoch: 54 - 16.0 s \t train loss: 0.13397 \t val loss: 0.12575\n",
      "Epoch: 55 - 16.0 s \t train loss: 0.13537 \t val loss: 0.12581\n",
      "Epoch: 56 - 15.0 s \t train loss: 0.13414 \t val loss: 0.12573\n",
      "Epoch: 57 - 16.0 s \t train loss: 0.13621 \t val loss: 0.12589\n",
      "Epoch: 58 - 15.0 s \t train loss: 0.13459 \t val loss: 0.12594\n",
      "Epoch: 59 - 16.0 s \t train loss: 0.13148 \t val loss: 0.12559\n",
      "Loss improved, saving model from epoch 59\n",
      "Epoch: 60 - 16.0 s \t train loss: 0.13198 \t val loss: 0.12577\n",
      "Epoch: 61 - 16.0 s \t train loss: 0.13252 \t val loss: 0.12584\n",
      "Epoch: 62 - 16.0 s \t train loss: 0.12851 \t val loss: 0.12575\n",
      "Epoch: 63 - 15.0 s \t train loss: 0.13282 \t val loss: 0.12566\n",
      "Epoch: 64 - 15.0 s \t train loss: 0.13417 \t val loss: 0.12543\n",
      "Loss improved, saving model from epoch 64\n",
      "Epoch: 65 - 16.0 s \t train loss: 0.13184 \t val loss: 0.12553\n",
      "Epoch: 66 - 16.0 s \t train loss: 0.13676 \t val loss: 0.12571\n",
      "Epoch: 67 - 15.0 s \t train loss: 0.13304 \t val loss: 0.12563\n",
      "Epoch: 68 - 15.0 s \t train loss: 0.13362 \t val loss: 0.12578\n",
      "Epoch: 69 - 16.0 s \t train loss: 0.1361 \t val loss: 0.12561\n",
      "Epoch: 70 - 16.0 s \t train loss: 0.13278 \t val loss: 0.12558\n",
      "Epoch: 71 - 16.0 s \t train loss: 0.13227 \t val loss: 0.12547\n",
      "Epoch: 72 - 15.0 s \t train loss: 0.13278 \t val loss: 0.12557\n",
      "Epoch: 73 - 15.0 s \t train loss: 0.12927 \t val loss: 0.12552\n",
      "Epoch: 74 - 19.0 s \t train loss: 0.13038 \t val loss: 0.12549\n",
      "Epoch: 75 - 19.0 s \t train loss: 0.13261 \t val loss: 0.12568\n",
      "\n",
      "Early stopping as accuracy did not improve on last 10 epochs!\n",
      "Trainning completed!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from time import time\n",
    "\n",
    "epochs = 100\n",
    "last_epoch = epochs - 1\n",
    "patience = 10\n",
    "patience_count = 0\n",
    "best_model = []\n",
    "best_val_loss = 999\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# histórico de acurácia e perda\n",
    "train_loss = np.empty(epochs)\n",
    "val_loss = np.empty(epochs)\n",
    "    \n",
    "for t in range(epochs):\n",
    "    start_time = time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # pega entradas\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels.type(torch.LongTensor))\n",
    "\n",
    "        # forward pass\n",
    "        predict = autoencoder(inputs)\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(predict, inputs)\n",
    "\n",
    "        # zera gradiente\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # calcula novos gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # atualiza pesos\n",
    "        optimizer.step()\n",
    "\n",
    "    elapsed_time = time() - start_time\n",
    "    # perda no conjunto de treinamento\n",
    "    train_loss[t] = prediction(train_loader, autoencoder, loss_fn)\n",
    "    # perda no conjunto de teste\n",
    "    val_loss[t] = prediction(val_loader, autoencoder, loss_fn)\n",
    "    # imprime dados do treinamento\n",
    "    print('Epoch:', t, '-',round(elapsed_time,0),'s',\n",
    "          '\\t train loss:', round(train_loss[t],5), \n",
    "          '\\t val loss:', round(val_loss[t],5))\n",
    "\n",
    "    # verifica melhora do modelo\n",
    "    if(best_val_loss > val_loss[t]):\n",
    "        patience_count = 0\n",
    "        best_val_loss = val_loss[t]\n",
    "        best_model = copy.deepcopy(autoencoder)\n",
    "        print('Loss improved, saving model from epoch', t)\n",
    "\n",
    "    # early stop\n",
    "    if(patience_count > patience):\n",
    "        print('\\nEarly stopping as accuracy did not improve on last', patience, 'epochs!')\n",
    "        last_epoch = t\n",
    "        break;\n",
    "\n",
    "    patience_count += 1\n",
    "\n",
    "# recupera melhor modelo\n",
    "autoencoder = best_model\n",
    "print('Trainning completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:28:55.201628",
     "start_time": "2017-08-12T17:28:37.193992"
    }
   },
   "outputs": [],
   "source": [
    "model_name = '../../models/FiltrandoRuidoAutoencoder'\n",
    "fit_params = {\n",
    "    'model_name': model_name,\n",
    "    'loss':       'mse',\n",
    "    'opt':        Adam(),\n",
    "    'batch_size': 32, \n",
    "    'nepochs':    50,\n",
    "    'patience':   5,\n",
    "    'ploss':      0.07,\n",
    "    'reset':      False,\n",
    "}\n",
    "\n",
    "autoencoder_trained,_ = train_network(autoencoder, x_train_noisy, x_train, x_test_noisy, x_test, **fit_params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando 10 primeiras amostras de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:29:07.838324",
     "start_time": "2017-08-12T17:29:07.745550"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = autoencoder_trained.predict(x_test_noisy[:10],batch_size=10)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:29:13.557833",
     "start_time": "2017-08-12T17:29:11.617414"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plt.gray()\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(2, 10, i+1)\n",
    "    ax.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    ax.axis('off')\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(2, 10, i+11)\n",
    "    ax.imshow(y_hat[i].reshape(28, 28))\n",
    "    ax.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com imagem de tamanho maior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a rede é formada de camadas convolucionais, é possível que a imagem de entrada tenha qualquer tamanho.\n",
    "Veja o exemplo a seguir, onde 4 imagens são juntadas para formar uma de shape=(56,56)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:29:16.408601",
     "start_time": "2017-08-12T17:29:16.180386"
    }
   },
   "outputs": [],
   "source": [
    "img1 = np.hstack([x_test_noisy[0],x_test_noisy[1]])\n",
    "img2 = np.hstack([x_test_noisy[2],x_test_noisy[3]])\n",
    "img = np.vstack([img1,img2]).reshape(1,56,56,1)\n",
    "print(img.shape)\n",
    "plt.imshow(img.reshape(56,56),cmap='gray')\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T17:29:17.442307",
     "start_time": "2017-08-12T17:29:17.231722"
    }
   },
   "outputs": [],
   "source": [
    "yy_hat = autoencoder_trained.predict(img,batch_size=1)\n",
    "plt.imshow(yy_hat.reshape(56,56),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
