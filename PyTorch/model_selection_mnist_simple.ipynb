{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção dos hyperparâmetros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:42:06.358993",
     "start_time": "2017-10-25T14:42:06.342896"
    }
   },
   "source": [
    "Hyperparâmetros são os parâmetros que não são aprendidos no processo de treinamento da rede neural. São parâmetros tanto do modelo, como número e tipo de camadas, \n",
    "número de neurônios ou canais em cada camada, como parâmetros do processo de otimização, tais como tipo do otimizador, taxa de aprendizado (learning rate), tamanho\n",
    "do mini-batch, entre outros.\n",
    "\n",
    "Uma opção é disparar um processo automático de busca no espaço de hyperparâmetros para buscar o melhor índice de validação cruzada. \n",
    "\n",
    "Normalmente a busca neste espaço de hyperparâmetros consiste de:\n",
    "- o modelo da rede;\n",
    "- o espaço de hyperparâmetros;\n",
    "- o método de busca e amostragem neste espaço;\n",
    "- o esquema de validação cruzada; e\n",
    "- uma função alvo (*score function*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:47:19.785475",
     "start_time": "2017-10-25T14:47:19.781571"
    }
   },
   "source": [
    "<img src='../figures/model_selection.png', width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:43:06.568757",
     "start_time": "2017-10-25T14:43:06.564456"
    }
   },
   "source": [
    "<img src='../figures/cross_validation.png', width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos desse experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este experimento utiliza como:\n",
    "- **modelo da rede:** fixa - Fully connected, já usada anteriormente\n",
    "- **espaço de hyperparâmetros:** variando learning rate e decay\n",
    "- **método de busca:** RandomizedSearch, onde número de iterações é especificado\n",
    "- **validação cruzada:** n. de folds especificado, opção se usar ou não dados de teste\n",
    "- **função alvo:** loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos pacotes tradicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.115686Z",
     "start_time": "2018-02-20T10:52:37.897280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as st\n",
    "import numpy.random as nr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision as tv\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.167700Z",
     "start_time": "2018-02-20T10:52:39.118661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset (treino): 60000\n",
      "Número de amostras no dataset (teste ): 10000\n"
     ]
    }
   ],
   "source": [
    "train_ds = tv.datasets.MNIST('/data/datasets/MNIST/', train=True, \n",
    "                             transform=tv.transforms.ToTensor())\n",
    "valid_ds = tv.datasets.MNIST('/data/datasets/MNIST/', train=False, \n",
    "                             transform=tv.transforms.ToTensor())\n",
    "n_train = len(train_ds)\n",
    "n_valid = len(valid_ds)\n",
    "print('Número de amostras no dataset (treino):', n_train)\n",
    "print('Número de amostras no dataset (teste ):', n_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduzindo o tamanho do dataset apenas para acelerar e testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.180364Z",
     "start_time": "2018-02-20T10:52:39.170206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    fator_reduc = 0.02\n",
    "    n_train = int(fator_reduc * n_train)\n",
    "    n_valid = int(fator_reduc * n_valid)\n",
    "    train_ds.train_data   = train_ds.train_data[:n_train]\n",
    "    train_ds.train_labels = train_ds.train_labels[:n_train]\n",
    "    valid_ds.test_data   = valid_ds.test_data[:n_valid]\n",
    "    valid_ds.test_labels = valid_ds.test_labels[:n_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.228347Z",
     "start_time": "2018-02-20T10:52:39.182639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset (treino): 1200\n",
      "Número de amostras no dataset (teste ): 200\n"
     ]
    }
   ],
   "source": [
    "print('Número de amostras no dataset (treino):', len(train_ds))\n",
    "print('Número de amostras no dataset (teste ):', len(valid_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.264194Z",
     "start_time": "2018-02-20T10:52:39.230748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class MySampler(Sampler):\n",
    "    def __init__(self, indexes):\n",
    "        self.indexes = indexes\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indexes[torch.randperm(len(self.indexes)).long()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição de rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:39.315772Z",
     "start_time": "2018-02-20T10:52:39.266891Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n=50):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, n)\n",
    "        self.at1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.at1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:52:40.019360Z",
     "start_time": "2018-02-20T10:52:39.318409Z"
    }
   },
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model_class':        None,\n",
    "    'loss_class':         [nn.CrossEntropyLoss],\n",
    "    'optim_class':        [optim.Adam], \n",
    "    'optim_lr':           st.uniform(0.0001, 0.005),\n",
    "    'optim_weight_decay': st.uniform(0.0, 0.01),\n",
    "    'optim_momentum':     [0.9],\n",
    "    'batch_size':         [20],\n",
    "    'num_epochs':         [10],\n",
    "}\n",
    "\n",
    "class MyRandomizedSearch(object):\n",
    "    def __init__(self, params=param_distributions, num_splits=3, num_iter=10):\n",
    "        if params['model_class'] is None:\n",
    "            raise Exception('')\n",
    "        self.parameters = params\n",
    "        self.n_splits = num_splits\n",
    "        self.n_iteractions = num_iter\n",
    "        \n",
    "    def fit(self, dataset, verbose=False):\n",
    "        self.parameter_sets = []\n",
    "        self.scores = []\n",
    "        \n",
    "        for it in range(self.n_iteractions):\n",
    "            print('Iteraction {:2d}:'.format(it), end=' ')\n",
    "            t0 = time.time()\n",
    "            p = self.sample_parameter_space()\n",
    "            self.parameter_sets.append(p)\n",
    "            split_scores = []\n",
    "            trainer = self.make_trainer(p, verbose=verbose)\n",
    "            for train_dloader, valid_dloader in self.gen_data_loaders(dataset, \n",
    "                                                                      self.n_splits, \n",
    "                                                                      p['batch_size']):\n",
    "                trainer.fit_loader(p['num_epochs'], train_dloader)\n",
    "                metrics = trainer.evaluate_loader(valid_dloader, verbose=0)\n",
    "                score = metrics['losses']\n",
    "                split_scores.append(score)\n",
    "                print('{:.5f}'.format(score), end=' ')\n",
    "            self.scores.append(split_scores)\n",
    "            print('[{:.1f} s]'.format(time.time() - t0))\n",
    "            \n",
    "        self.show_results()\n",
    "            \n",
    "    def sample_parameter_space(self):\n",
    "        pars = dict()\n",
    "        for p, v in self.parameters.items():\n",
    "            if type(v) == list:\n",
    "                pars[p] = v[0] if len(v) == 1 else nr.randint(0, len(v))\n",
    "            elif getattr(v, 'rvs', None) is not None:\n",
    "                pars[p] = v.rvs()\n",
    "            else:\n",
    "                raise Exception('Unknown par dist type')\n",
    "        return pars\n",
    "    \n",
    "    def gen_data_loaders(self, dataset, n_splits, batch_size):\n",
    "        n_total = len(dataset)\n",
    "        split_size = n_total // n_splits\n",
    "        indices = torch.arange(n_total)\n",
    "        split = torch.LongTensor([n_splits - 1 for _ in indices])\n",
    "        split[:n_splits * split_size] = torch.arange(n_splits * split_size).long() / split_size\n",
    "        for i in range(n_splits):\n",
    "            vii = indices[split == i].long()\n",
    "            tii = indices[split != i].long()\n",
    "            train_dloader = DataLoader(dataset, batch_size=batch_size, sampler=MySampler(tii))\n",
    "            valid_dloader = DataLoader(dataset, batch_size=batch_size, sampler=MySampler(vii))\n",
    "            yield train_dloader, valid_dloader\n",
    "    \n",
    "    def make_trainer(self, p, verbose):\n",
    "        if verbose > 0:\n",
    "            callbacks = [ptt.PrintCallback()]\n",
    "        else:\n",
    "            callbacks = None            \n",
    "        self.model = p['model_class']()\n",
    "        loss_fn = self.make_criterion(p)\n",
    "        optimizer = self.make_optimizer(p)\n",
    "        trainer = ptt.DeepNetTrainer(model=self.model, criterion=loss_fn, \n",
    "                                     optimizer=optimizer, callbacks=callbacks)\n",
    "        return trainer\n",
    "        \n",
    "    def make_optimizer(self, p):\n",
    "        if p['optim_class'] == 'Adam':\n",
    "            optimz = optim.Adam(self.model.parameters(), lr=p['optim_lr'], \n",
    "                                weight_decay=p['optim_weight_decay'])\n",
    "        elif p['optim_class'] == 'SGD':\n",
    "            optimz = optim.SGD(self.model.parameters(), lr=p['optim_lr'], \n",
    "                               momentum=p['optim_momentum'], \n",
    "                               weight_decay=p['optim_weight_decay'], nesterov=True)\n",
    "        elif p['optim_class'] == 'RMSprop':\n",
    "            optimz = optim.RMSprop(self.model.parameters(), lr=p['optim_lr'], \n",
    "                                   weight_decay=p['optim_weight_decay'])\n",
    "        else:\n",
    "            raise Exception(\"A ser implementado...\")\n",
    "        return optimz\n",
    "    \n",
    "    def make_criterion(self, p):\n",
    "        if p['loss_class'] == 'CrossEntropyLoss':\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "        elif p['loss_class'] == 'MSELoss':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            raise Exception(\"A ser implementado...\")\n",
    "        return loss_fn\n",
    "    \n",
    "    def show_results(self):\n",
    "        self.mean_scores = torch.FloatTensor(rs.scores).mean(1)\n",
    "        self.best_loss, self.best_index = [x[0] for x in self.mean_scores.min(0)]\n",
    "        self.best_trainer = self.make_trainer(rs.parameter_sets[self.best_index], verbose=0)\n",
    "        print('\\nBest parameter set from iteraction {} with loss = {:.5f}:'.format(self.best_index, self.best_loss))\n",
    "        for p, v in self.parameter_sets[self.best_index].items():\n",
    "            print('    {:20s}: {}'.format(p, v))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efetuando a busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T10:58:17.672013Z",
     "start_time": "2018-02-20T10:52:40.021891Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction  0: 0.48420 0.47077 0.46774 [37.3 s]\n",
      "Iteraction  1: 0.41834 0.47786 0.53363 [33.3 s]\n",
      "Iteraction  2: 0.50510 0.36839 0.31426 [33.3 s]\n",
      "Iteraction  3: 0.48735 0.49378 0.51934 [33.5 s]\n",
      "Iteraction  4: 0.46025 0.42248 0.37672 [33.6 s]\n",
      "Iteraction  5: 0.43813 0.43806 0.41564 [33.4 s]\n",
      "Iteraction  6: 0.41738 0.49789 0.50616 [33.2 s]\n",
      "Iteraction  7: 0.43290 0.50205 0.49366 [33.2 s]\n",
      "Iteraction  8: 0.44634 0.39853 0.39854 [33.3 s]\n",
      "Iteraction  9: 0.43594 0.49803 0.50154 [33.3 s]\n",
      "\n",
      "Best parameter set from iteraction 2 with loss = 0.39592:\n",
      "    model_class         : <class '__main__.Model'>\n",
      "    loss_class          : CrossEntropyLoss\n",
      "    optim_class         : Adam\n",
      "    optim_lr            : 0.0019526153512656936\n",
      "    optim_weight_decay  : 0.0004022120167432541\n",
      "    optim_momentum      : 0.9\n",
      "    weight_decay        : 0.0\n",
      "    batch_size          : 20\n",
      "    num_epochs          : 50\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'model_class':        [Model],\n",
    "    'loss_class':         ['CrossEntropyLoss'],\n",
    "    'optim_class':        ['Adam'], \n",
    "    'optim_lr':           st.uniform(0.0001, 0.005),\n",
    "    'optim_weight_decay': st.uniform(0.0, 0.01),\n",
    "    'optim_momentum':     [0.9],\n",
    "    'weight_decay':       [0.0],\n",
    "    'batch_size':         [20],\n",
    "    'num_epochs':         [50],\n",
    "}\n",
    "\n",
    "rs = MyRandomizedSearch(params, num_splits=3, num_iter=10)\n",
    "rs.fit(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T16:26:48.669640Z",
     "start_time": "2018-02-19T16:26:48.644075Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adicione um parâmetro adicional na escolha do otimizador: 'SGV' e o 'Adam'.\n",
    "2. Adicione agora um parâmetro na rede, por exemplo trocar a função de ativação de 'ReLU' para 'Sigmoid'.\n",
    "3. Adicione um parâmetro a sua escolha."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
