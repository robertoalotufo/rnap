{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento usando word embeddings - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente vimos uma primeira solução de análise de sentimento utilizando *bag of words*.\n",
    "Agora iremos ilustrar o uso de *word embeddings* como vetor de atributos latentes de cada palavra.\n",
    "\n",
    "Duas soluções são propostas neste exercícios:\n",
    "\n",
    "1. Utilizando rede neural com camadas densas\n",
    "2. Utilizando camadas convolucionais 1D\n",
    "\n",
    "Diferentemente da solução apresentada com *bag of words*, nestas duas soluções, é necessário que o\n",
    "número de palavras seja o mesmo para cada amostra. Para isso, limita-se o número de palavras e caso\n",
    "o número de palavras for menor, completa-se com um código especial e palavras além do limite são\n",
    "descartadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:17.781387",
     "start_time": "2017-11-01T13:54:16.641348"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo do disco\n",
    "\n",
    "O dataset é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. \n",
    "Cada amostra tem um rótulo igual a 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:19.671784",
     "start_time": "2017-11-01T13:54:17.783109"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, 25000, 25000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = json.load(open('/data/datasets/IMDB/imdb_word_index.json'))\n",
    "data = np.load('/data/datasets/IMDB/imdb.npz')\n",
    "x_test, x_train, y_train, y_test = data['x_test'], data['x_train'], data['y_train'], data['y_test']\n",
    "\n",
    "n_words = len(word_index)\n",
    "n_train = x_train.shape[0]\n",
    "n_test  = x_test.shape[0]\n",
    "\n",
    "word_list = [None for i in range(n_words+1)]\n",
    "for k, v in word_index.items():\n",
    "    word_list[v] = k\n",
    "\n",
    "n_words, n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:20.286489",
     "start_time": "2017-11-01T13:54:19.673559"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 88584\n",
      "Test word index limits: 1 88581\n",
      "\n",
      "Train sequence length limits: 10 2493\n",
      "Test sequence length limits: 6 2314\n",
      "\n",
      "Most frequent words: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']\n"
     ]
    }
   ],
   "source": [
    "def print_stats(x_train, x_test, word_list=None):\n",
    "    print('Train word index limits:', min([min(s) for s in x_train]), max([max(s) for s in x_train]))\n",
    "    print('Test word index limits:', min([min(s) for s in x_test]), max([max(s) for s in x_test]))\n",
    "    print('\\nTrain sequence length limits:', min([len(x) for x in x_train]), max([len(x) for x in x_train]))\n",
    "    print('Test sequence length limits:', min([len(x) for x in x_test]), max([len(x) for x in x_test]))\n",
    "    if word_list:\n",
    "        print('\\nMost frequent words:', word_list[1:11])\n",
    "    \n",
    "print_stats(x_train, x_test, word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitando o vocabulário\n",
    "\n",
    "Retiramos das sequências as palavras com índice maior que o valor especificado em `voc_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:21.893854",
     "start_time": "2017-11-01T13:54:20.288155"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 4999\n",
      "Test word index limits: 1 4999\n",
      "\n",
      "Train sequence length limits: 9 1973\n",
      "Test sequence length limits: 6 2113\n"
     ]
    }
   ],
   "source": [
    "voc_size = 5000\n",
    "\n",
    "xtra = [[w for w in x if (w < voc_size)] for x in x_train]\n",
    "xval = [[w for w in x if (w < voc_size)] for x in x_test]\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo sequências de mesmo comprimento\n",
    "\n",
    "Fazemos com que todas as sequências tenham o mesmo comprimento, especificado em `seq_len`. As sequências mais longas que `seq_len` são truncadas e as menores, completadas com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:21.907180",
     "start_time": "2017-11-01T13:54:21.895679"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, seq_len, post_pad=True, fill_value=0):\n",
    "    new_seq = []\n",
    "    for seq in sequences:\n",
    "        n = len(seq)\n",
    "        if n > seq_len:\n",
    "            if post_pad:\n",
    "                new_seq.append(seq[-seq_len:])\n",
    "            else:\n",
    "                new_seq.append(seq[:seq_len])\n",
    "        else:\n",
    "            zseq = [fill_value for i in range(seq_len)]\n",
    "            if post_pad:\n",
    "                zseq[-n:] = seq\n",
    "            else:\n",
    "                zseq[:n] = seq\n",
    "            new_seq.append(zseq)\n",
    "    return new_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:24.852932",
     "start_time": "2017-11-01T13:54:21.908926"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 0 4999\n",
      "Test word index limits: 0 4999\n",
      "\n",
      "Train sequence length limits: 500 500\n",
      "Test sequence length limits: 500 500\n"
     ]
    }
   ],
   "source": [
    "seq_len = 500\n",
    "xtra = pad_sequences(xtra, seq_len, post_pad=True)\n",
    "xval = pad_sequences(xval, seq_len, post_pad=True)\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T13:54:26.589949",
     "start_time": "2017-11-01T13:54:24.854720"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 500]), 4999, torch.Size([25000, 500]), 4999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = torch.from_numpy(np.array(xtra, np.int))\n",
    "Xvalid = torch.from_numpy(np.array(xval, np.int))\n",
    "ytrain = torch.from_numpy(np.array(y_train, np.int))\n",
    "yvalid = torch.from_numpy(np.array(y_test, np.int))\n",
    "\n",
    "Xtrain.size(), Xtrain.max(), Xvalid.size(), Xvalid.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Densa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T14:05:26.909593",
     "start_time": "2017-11-01T14:05:26.897679"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len=seq_len, voc_size=voc_size, embed_dim=None):\n",
    "        super().__init__()\n",
    "        self.flat_size = seq_len * embedding_dim        \n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.fc1 = nn.Linear(self.flat_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T14:12:16.747771",
     "start_time": "2017-11-01T14:12:16.716322"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelRNN (\n",
       "  (emb): Embedding(5000, 50)\n",
       "  (rnn): RNN(50, 100, batch_first=True, dropout=0.05)\n",
       "  (fc1): Linear (100 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, voc_size=voc_size, embed_dim = None):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.rnn = nn.RNN(input_size=embed_dim,hidden_size=hidden_size, dropout=0.05,batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        _,x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.squeeze(x,0)\n",
    "        return x\n",
    "    \n",
    "model_rnn = ModelRNN(100,5000,50)\n",
    "model_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando predict com uma amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T14:12:17.861085",
     "start_time": "2017-11-01T14:12:17.784669"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       " -1.2748  3.7050\n",
       " -0.6047  3.2608\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xtrain[0:2].size())\n",
    "y = model_rnn(Variable(Xtrain[0:2]))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-01T14:13:54.233892",
     "start_time": "2017-11-01T14:13:54.221615"
    }
   },
   "outputs": [],
   "source": [
    "trainIt = True\n",
    "resetIt = True\n",
    "\n",
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_rnn'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=resetIt, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model_rnn, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, chkpt_cb, print_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-01T16:13:54.983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 10 epochs\n",
      "  1:  16.2s   T: 0.69187 0.53324   V: 0.68895 0.56520 best\n",
      "  2:  16.0s   T: 0.56055 0.71936   V: 0.42588 0.82616 best\n",
      "  3:  16.2s   T: 0.34488 0.85644   V: 0.35621 0.85184 best\n",
      "  4:  16.2s   T: 0.30676 0.87376   V: 0.33113 0.86048 best\n",
      "  5:  16.1s   T: 0.27349 0.88940   V: 0.33376 0.85828 \n",
      "Best model was saved at epoch 4 with loss 0.33113: ../../models/sentimento_rnn\n",
      "Stop training at epoch: 5/10\n"
     ]
    }
   ],
   "source": [
    "if trainIt:\n",
    "    trainer.fit(n_epochs, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    print('\\nTraining disabled.\\nThis model was trained for {} epochs.'.format(trainer.last_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T19:38:03.695638Z",
     "start_time": "2017-10-26T19:37:57.858331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model training set accuracy after training: 0.89896\n",
      "\n",
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.86048\n"
     ]
    }
   ],
   "source": [
    "if 'ModelCheckpoint' in [cb.__class__.__name__ for cb in trainer.callbacks]:\n",
    "    trainer.load_state(state_fn)\n",
    "\n",
    "rmetrics = trainer.evaluate(Xtrain, ytrain, metrics=[accuracy_cb])\n",
    "print('Model training set accuracy after training: {:.5f}'.format(rmetrics['acc']))\n",
    "print()\n",
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[accuracy_cb])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T20:15:36.885766Z",
     "start_time": "2017-10-26T20:15:36.880693Z"
    }
   },
   "source": [
    "## Resumo dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experimento *bag of words*: 87% de acurácia\n",
    "2. Experimento *word embeddings*, rede densa: 88%\n",
    "3. Experimento *word embeddings*, rede convolucional: 89%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
