{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento usando word embeddings - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente vimos uma primeira solução de análise de sentimento utilizando *bag of words*.\n",
    "Agora iremos ilustrar o uso de *word embeddings* como vetor de atributos latentes de cada palavra.\n",
    "\n",
    "Duas soluções são propostas neste exercícios:\n",
    "\n",
    "1. Utilizando rede neural com camadas densas\n",
    "2. Utilizando camadas convolucionais 1D\n",
    "\n",
    "Diferentemente da solução apresentada com *bag of words*, nestas duas soluções, é necessário que o\n",
    "número de palavras seja o mesmo para cada amostra. Para isso, limita-se o número de palavras e caso\n",
    "o número de palavras for menor, completa-se com um código especial e palavras além do limite são\n",
    "descartadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:45:39.186385Z",
     "start_time": "2018-02-04T20:45:36.691956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo do disco\n",
    "\n",
    "O dataset é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. \n",
    "Cada amostra tem um rótulo igual a 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:45:45.896489Z",
     "start_time": "2018-02-04T20:45:42.338932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, 25000, 25000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = json.load(open('/data/datasets/IMDB/imdb_word_index.json'))\n",
    "data = np.load('/data/datasets/IMDB/imdb.npz')\n",
    "x_test, x_train, y_train, y_test = data['x_test'], data['x_train'], data['y_train'], data['y_test']\n",
    "\n",
    "n_words = len(word_index)\n",
    "n_train = x_train.shape[0]\n",
    "n_test  = x_test.shape[0]\n",
    "\n",
    "word_list = [None for i in range(n_words+1)]\n",
    "for k, v in word_index.items():\n",
    "    word_list[v] = k\n",
    "\n",
    "n_words, n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:45:48.336082Z",
     "start_time": "2018-02-04T20:45:47.104306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 88584\n",
      "Test word index limits: 1 88581\n",
      "\n",
      "Train sequence length limits: 10 2493\n",
      "Test sequence length limits: 6 2314\n",
      "\n",
      "Most frequent words: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']\n"
     ]
    }
   ],
   "source": [
    "def print_stats(x_train, x_test, word_list=None):\n",
    "    print('Train word index limits:', min([min(s) for s in x_train]), max([max(s) for s in x_train]))\n",
    "    print('Test word index limits:', min([min(s) for s in x_test]), max([max(s) for s in x_test]))\n",
    "    print('\\nTrain sequence length limits:', min([len(x) for x in x_train]), max([len(x) for x in x_train]))\n",
    "    print('Test sequence length limits:', min([len(x) for x in x_test]), max([len(x) for x in x_test]))\n",
    "    if word_list:\n",
    "        print('\\nMost frequent words:', word_list[1:11])\n",
    "    \n",
    "print_stats(x_train, x_test, word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitando o vocabulário\n",
    "\n",
    "Retiramos das sequências as palavras com índice maior que o valor especificado em `voc_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:45:55.503676Z",
     "start_time": "2018-02-04T20:45:50.775448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 1 4999\n",
      "Test word index limits: 1 4999\n",
      "\n",
      "Train sequence length limits: 9 1973\n",
      "Test sequence length limits: 6 2113\n"
     ]
    }
   ],
   "source": [
    "voc_size = 5000\n",
    "\n",
    "xtra = [[w for w in x if (w < voc_size)] for x in x_train]\n",
    "xval = [[w for w in x if (w < voc_size)] for x in x_test]\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo sequências de mesmo comprimento\n",
    "\n",
    "Fazemos com que todas as sequências tenham o mesmo comprimento, especificado em `seq_len`. As sequências mais longas que `seq_len` são truncadas e as menores, completadas com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:45:57.654471Z",
     "start_time": "2018-02-04T20:45:57.605637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, seq_len, post_pad=True, fill_value=0):\n",
    "    new_seq = []\n",
    "    for seq in sequences:\n",
    "        n = len(seq)\n",
    "        if n > seq_len:\n",
    "            if post_pad:\n",
    "                new_seq.append(seq[-seq_len:])\n",
    "            else:\n",
    "                new_seq.append(seq[:seq_len])\n",
    "        else:\n",
    "            zseq = [fill_value for i in range(seq_len)]\n",
    "            if post_pad:\n",
    "                zseq[-n:] = seq\n",
    "            else:\n",
    "                zseq[:n] = seq\n",
    "            new_seq.append(zseq)\n",
    "    return new_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:46:04.592929Z",
     "start_time": "2018-02-04T20:45:58.602549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word index limits: 0 4999\n",
      "Test word index limits: 0 4999\n",
      "\n",
      "Train sequence length limits: 500 500\n",
      "Test sequence length limits: 500 500\n"
     ]
    }
   ],
   "source": [
    "seq_len = 500\n",
    "xtra = pad_sequences(xtra, seq_len, post_pad=True)\n",
    "xval = pad_sequences(xval, seq_len, post_pad=True)\n",
    "print_stats(xtra, xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:46:09.458933Z",
     "start_time": "2018-02-04T20:46:05.885681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 500]), 4999, torch.Size([25000, 500]), 4999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = torch.from_numpy(np.array(xtra, np.int))\n",
    "Xvalid = torch.from_numpy(np.array(xval, np.int))\n",
    "ytrain = torch.from_numpy(np.array(y_train, np.int))\n",
    "yvalid = torch.from_numpy(np.array(y_test, np.int))\n",
    "\n",
    "Xtrain.size(), Xtrain.max(), Xvalid.size(), Xvalid.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Recorrente RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:46:09.592723Z",
     "start_time": "2018-02-04T20:46:09.464383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelRNN(\n",
       "  (emb): Embedding(5000, 50)\n",
       "  (rnn): RNN(50, 100, batch_first=True, dropout=0.05)\n",
       "  (fc1): Linear(in_features=100, out_features=2)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, voc_size=voc_size, embed_dim = None):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.rnn = nn.RNN(input_size=embed_dim,hidden_size=hidden_size, dropout=0.05,batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        _,x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.squeeze(x,0)\n",
    "        return x\n",
    "    \n",
    "model_rnn = ModelRNN(100,5000,50)\n",
    "if use_gpu:\n",
    "    model_rnn = model_rnn.cuda()\n",
    "model_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando predict com uma amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:46:12.259155Z",
     "start_time": "2018-02-04T20:46:11.067201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3286  4.8072\n",
      "  6.3952  4.9287\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0:2].size())\n",
    "xin = Variable(Xtrain[0:2])\n",
    "if use_gpu:\n",
    "    xin = xin.cuda()\n",
    "y = model_rnn(xin)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:46:28.870436Z",
     "start_time": "2018-02-04T20:46:28.844332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_rnn'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=True, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model_rnn, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, print_cb, chkpt_cb],\n",
    "}\n",
    "trainer = ptt.DeepNetTrainer(**training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:47:17.779310Z",
     "start_time": "2018-02-04T20:47:17.765911Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainer.fit(10, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    trainer.load_state('/data/models/sentimento_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando na GPU:\n",
    "\n",
    "        Start training for 10 epochs\n",
    "          1:  16.2s   T: 0.69239 0.52684   V: 0.69060 0.58852 best\n",
    "          2:  16.2s   T: 0.64847 0.64060   V: 0.46676 0.78716 best\n",
    "          3:  16.1s   T: 0.38345 0.83408   V: 0.36423 0.84764 best\n",
    "          4:  16.2s   T: 0.30459 0.87692   V: 0.32521 0.86508 best\n",
    "          5:  16.1s   T: 0.26332 0.89768   V: 0.31838 0.86872 best\n",
    "          6:  16.1s   T: 0.25598 0.90048   V: 0.31837 0.86880 best\n",
    "          7:  16.2s   T: 0.23778 0.90864   V: 0.36216 0.85484 \n",
    "          8:  16.1s   T: 0.22960 0.91344   V: 0.33566 0.86748 \n",
    "          9:  16.1s   T: 0.21615 0.92124   V: 0.33796 0.86820 \n",
    "         10:  16.2s   T: 0.22252 0.91800   V: 0.34421 0.85672 \n",
    "        Stop training at epoch: 10/10\n",
    "        Best model was saved at epoch 6 with loss 0.31837: ../../models/sentimento_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T20:48:46.941450Z",
     "start_time": "2018-02-04T20:48:13.858301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 228/2499 interrupted!\n",
      "Model validation set accuracy after training: 0.91703\n"
     ]
    }
   ],
   "source": [
    "rmetrics = trainer.evaluate(Xvalid, yvalid, metrics=[accuracy_cb])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Recorrente LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:42:49.066361Z",
     "start_time": "2017-12-15T17:42:49.026158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLSTM (\n",
       "  (emb): Embedding(5000, 50)\n",
       "  (lstm): LSTM(50, 100, batch_first=True, dropout=0.05)\n",
       "  (fc1): Linear (100 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, voc_size=voc_size, embed_dim = None):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(voc_size, embed_dim)\n",
    "        nn.init.xavier_uniform(self.emb.weight)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim,hidden_size=hidden_size, dropout=0.05,batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        _,(x,_) = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.squeeze(x,0)\n",
    "        return x\n",
    "    \n",
    "model_lstm = ModelLSTM(100,5000,50)\n",
    "if use_gpu:\n",
    "    model_lstm = model_lstm.cuda()\n",
    "model_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando predict com uma amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:42:49.523763Z",
     "start_time": "2017-12-15T17:42:49.068020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1146 -0.0398\n",
       "-0.1185 -0.0402\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xtrain[0:2].size())\n",
    "xin = Variable(Xtrain[0:2])\n",
    "if use_gpu:\n",
    "    xin = xin.cuda()\n",
    "y = model_lstm(xin)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:42:49.541273Z",
     "start_time": "2017-12-15T17:42:49.525742Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "state_fn = '../../models/sentimento_lstm'\n",
    "accuracy_cb = ptt.AccuracyMetric()\n",
    "chkpt_cb = ptt.ModelCheckpoint(state_fn, reset=True, verbose=1)\n",
    "print_cb = ptt.PrintCallback()\n",
    "plot_cb = ptt.PlotCallback()\n",
    "\n",
    "# Model, optimizer and learning rate scheduler\n",
    "# --------------------------------------------\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.75)\n",
    "\n",
    "# Network trainer\n",
    "# ---------------\n",
    "training_parameters = {\n",
    "    'model':         model_lstm, \n",
    "    'criterion':     nn.CrossEntropyLoss(),\n",
    "    'optimizer':     optimizer, \n",
    "    'lr_scheduler':  scheduler, \n",
    "    'callbacks':     [accuracy_cb, print_cb, chkpt_cb],\n",
    "}\n",
    "trainer_lstm = ptt.DeepNetTrainer(**training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:42:49.548496Z",
     "start_time": "2017-12-15T17:42:49.542993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainer_lstm.fit(10, Xtrain, ytrain, valid_data=(Xvalid, yvalid), batch_size=batch_size)\n",
    "else:\n",
    "    trainer_lstm.load_state('/data/models/sentimento_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado do treinamento na GPU:\n",
    "\n",
    "        Start training for 10 epochs\n",
    "          1:  19.8s   T: 0.69289 0.51508   V: 0.69216 0.54064 best\n",
    "          2:  19.9s   T: 0.68615 0.61652   V: 0.64578 0.67128 best\n",
    "          3:  19.8s   T: 0.44313 0.80380   V: 0.37075 0.84224 best\n",
    "          4:  19.9s   T: 0.32085 0.86912   V: 0.32077 0.86364 best\n",
    "          5:  19.9s   T: 0.27029 0.89392   V: 0.29334 0.87972 best\n",
    "          6:  19.9s   T: 0.23622 0.91056   V: 0.28798 0.87756 best\n",
    "          7:  19.9s   T: 0.21872 0.91748   V: 0.27990 0.88404 best\n",
    "          8:  19.9s   T: 0.20478 0.92356   V: 0.28781 0.88408 \n",
    "          9:  20.0s   T: 0.19705 0.92720   V: 0.28215 0.88464 \n",
    "         10:  19.9s   T: 0.18264 0.93348   V: 0.29655 0.88156 \n",
    "        Stop training at epoch: 10/10\n",
    "        Best model was saved at epoch 7 with loss 0.27990: ../../models/sentimento_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T17:48:26.133318Z",
     "start_time": "2017-12-15T17:42:49.550122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: 2499/2499 ok\n",
      "Model validation set accuracy after training: 0.88404\n"
     ]
    }
   ],
   "source": [
    "rmetrics = trainer_lstm.evaluate(Xvalid, yvalid, metrics=[accuracy_cb])\n",
    "print('Model validation set accuracy after training: {:.5f}'.format(rmetrics['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T20:15:36.885766Z",
     "start_time": "2017-10-26T20:15:36.880693Z"
    }
   },
   "source": [
    "## Resumo dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experimento *bag of words*: 87% de acurácia\n",
    "2. Experimento *word embeddings*, rede densa: 88%\n",
    "3. Experimento *word embeddings*, rede convolucional: 89%\n",
    "4. Experimento *word embeddings*, RNN: 87%\n",
    "5. Experimento *word embeddings*, LSTM: 88.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
