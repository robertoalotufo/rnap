{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence em PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial é uma versão modificada de https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste tutorial iremos ensinar como treinar uma rede neural para traduzir sentenças do portugues para o inglês.\n",
    "\n",
    "Isto é possível graças a simples mas poderosa idéia da [rede sequence to sequence](http://arxiv.org/abs/1409.3215), na qual duas redes neurais recorrentes transformam uma sequência de símbolos em outra sequência. A rede codificadora condensa uma sequência de entrada em um vetor e a rede decodificadora converte esse vetor em uma nova sequência.\n",
    "\n",
    "Para melhorar este modelo iremos usar o [mecanismo de atenção](https://arxiv.org/abs/1409.0473), o qual faz com que o decodificador aprenda a se atentar a partes específicas da sequência de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O modelo de Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [rede Sequence to Sequence](http://arxiv.org/abs/1409.3215), ou rede seq2seq, ou [rede Codificadora-Decodificadora](https://arxiv.org/pdf/1406.1078v3.pdf), é um modelo que possui duas redes RNNs chamadas de  **codificadora** e **decodificadora**. O codificador lê a sequência de entrada um item por vez e retorna um vetor a cada passo. A saída final do codificador é um vetor de **contexto**, representado na figura abaixo por $h_5$. O decodificador usa esse vetor de contexto para produzir uma sequência de simbolos, um de cada vez.\n",
    "\n",
    "<img src=\"../figures/NMT-Simples.png\",width=700>\n",
    "\n",
    "Quando trabalhamos com tradução automática, é comum não termos uma relação de um para um entre os textos de entrada e os de saída. Por exemplo, dada a sentença \"Eu não gosto daquele vestido preto\" e sua tradução \"I don't like that black dress\", muitas das palavras tem uma tradução direta, como \"gosto\" e \"like\". Entretanto, diferenças gramaticais podem acaretar em diferentes ordenações das palavras, como por exemplo \"vestido preto\" e \"black dress\". Há também diferenças na construção da negação 'do not' e 'não', que faz com que as sentenças tenham diferentes tamanhos.\n",
    "\n",
    "Com o modelo seq2seq, o codificador converte a entrada para um vetor de tamanho fixo, não importando seu tamanho original. E o decodificador produz sequências de tamanho não fixo, mesmo usando um vetor de tamanho fixo como entrada.\n",
    "\n",
    "Esta idéia pode ser extendida para além de sequências. Geração automática de legenda de imagens é uma delas (https://arxiv.org/abs/1411.4555), onde a entrada é uma imagem e a saída é um texto descrevendo a imagem. O inverso também é possivel, onde a entrada é a descrição textual de um objeto e a saída é a imagem deste objeto (https://arxiv.org/abs/1511.02793). Estes modelos podem ser mais genericamente chamados de redes \"codificadoras decodificadoras\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Mecanismo de Atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O vetor de tamanho fixo carrega o ônus de representar todo a \"semântica\" da sequência de entrada, não importa o quão longa ela seja. Com todas as variações da linguagem, este é um problema bem difícil. Imagine duas sentenças quase idênticas, com 20 palavras cada, diferindo em apenas uma palavra. O codificador e o decodificador precisam ser poderosos o suficiente para capturar essa pequena mudança na representação.\n",
    "\n",
    "O **mecanismo de atenção** introduzido por [Bahdanau et al.](https://arxiv.org/abs/1409.0473) tenta resolver esse problema através de um mecanismo que permite que o decodificador \"preste atenção\" em partes da sequência de entrada ao invés de depender de uma vetor de tamanho fixo. A cada passo, o decodificador pode considerar partes diferentes da sequência de entrada.\n",
    "\n",
    "A figura abaixo mostra uma rede seq2seq com o mecanismo de atenção. O módulo de atenção (Att) tem como entrada todos os vetores da sequência de entrada ($h_1, ..., h_5$), e a saída é um vetor de tamanho fixo ($h'_2$) que é a soma ponderada dos vetores da sequência de entrada. Os pesos dessa soma ponderada são calculados por uma rede neural simples que tem como entrada o vetor de estado obtido no passo anterior ($g_1$) e os vetores da sequência de entrada ($h_1, ..., h_5$). Para que a soma dos pesos fique entre 0 e 1, a função de softmax é usada.\n",
    "O novo vetor de atenção ($g_2$) é então usado para gerar a próxima saída.\n",
    "\n",
    "<img src=\"../figures/NMT-Attention.png\",width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você precisará do [PyTorch](http://pytorch.org/) para treinar os modelos e do [matplotlib](https://matplotlib.org/) para visualizar o treinamento e os pesos do mecanismo de atenção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.681275",
     "start_time": "2017-10-18T13:16:51.260033"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo iremos definir uma constante que diz se iremos usar uma GPU ou CPU. **Se você não possuir uma GPU, deixe esta variável como `False`**. Mais tarde quando criarmos tensores, esta variável será usada para decidir se os executaremos na CPU ou na GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.685882",
     "start_time": "2017-10-18T13:16:51.682939"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados para esse tutorial vem do site http://www.manythings.org/anki/, o qual contém milhares de pares de sentenças em diversos idiomas que foram originalmente extraídos de http://tatoeba.org/. Neste tutorial iremos usar apenas os pares português-inglês, então basta fazer download do arquivo http://www.manythings.org/anki/por-eng.zip, extraí-lo na mesma pasta deste notebook e renomeá-lo para `eng-por.txt`. Este arquivo contém mais de 100.000 pares de sentenças separadas por tab:\n",
    "\n",
    "```\n",
    "I'm cold.\tEstou com frio.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós iremos reprensentar cada palavra como um vetor one-hot, que é um vetor do tamanho do vocabulário usado (ex: 300.000 palavras únicas) onde todos os elementos são zero com exceção de um único elemento, que corresponde ao índice da palavra. Como este vetor pode ser muito grande, iremos limitar nosso vocabulário para apenas algumas milhares de palavras por língua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexando palavras\n",
    "\n",
    "Nós precisaremos de um índice único por palavra para usá-lo como entrada e sáida da rede. Para isso, utilizaremos uma classe que chamaremos de 'Lang', que possui dois dicionários: palavra &rarr; índice (`word2index`) e índice &rarr; palavra (`index2word`). Esta classe também armazena o número de occorências de cada palavra (`word2count`), que será usada mais tarde para lidarmos com palavras raras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.701772",
     "start_time": "2017-10-18T13:16:51.687581"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Conta Start-of-Sequence (SOS) e End-of-Sequence (EOS)\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        '''\n",
    "        Adiciona o número de ocorrências das palavras de uma sentença no contador geral. \n",
    "        '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # Se a palavra não estiver no dicionário, crie uma entrada para ela, com contagem = 1.\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Caso já exista no dicionário, apenas incremente seu número de ocorrências.\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo e decodificando arquivos\n",
    "\n",
    "Os arquivos estão em formato unicode e para simplificar iremos convertê-los para ASCII, caixa baixa, e remover a maioria da pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.710785",
     "start_time": "2017-10-18T13:16:51.703563"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converte uma string Unicode para ASCII. Extraído de http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Caixa baixa, remove espaços antes e depois, e remove caracteres que não são letras\n",
    "def normalize_string(s):\n",
    "    #s = s.decode('ascii', 'ignore').lower().strip()\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo é lido linha por linha, e cada linha é dividida em um par de sentenças. Os arquivos são sempre do tipo  Inglês &rarr; Outra Língua, então se quisermos traduzir da outra língua para o inglês basta passarmos o argumento reverse=True para a função abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.724545",
     "start_time": "2017-10-18T13:16:51.712697"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Lendo Linhas...\")\n",
    "\n",
    "    # Lê o arquivo e o divide em linhas\n",
    "    lines = open('/data/datasets/seq2seq-translation/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Divide cada linha em um par de sentenças e normaliza-as.\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Inverte os pares\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo sequências longas\n",
    "\n",
    "Como temos muitos exemplos (mais de 100.000) e queremos que o treinamento seja rápido, iremos remover todas as sentenças com mais de 10 palavras do dataset. Também removeremos todas as sentenças que não começam com termos simples, como \"I am\" and \"He is\", pois isso acelera o treino ainda mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:51.732784",
     "start_time": "2017-10-18T13:16:51.726692"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(good_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em suma, o processo de preparação dos dados é:\n",
    "* Ler o arquivo texto e separá-lo em linhas e depois dividir as linhas em pares.\n",
    "* Normalizar o texto, filtrar por comprimento e conteúdo.\n",
    "* Converter os pares de sentenças em listas de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.554508",
     "start_time": "2017-10-18T13:16:51.734802"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo Linhas...\n",
      "Lendo par de sentenças 110158\n",
      "Removendo espaços do par de sentenças 1773\n",
      "Indexando palavras...\n",
      "['ele e diplomata na embaixada americana .', 'he is a diplomat at the american embassy .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Lendo par de sentenças %s\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Removendo espaços do par de sentenças %s\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexando palavras...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'por', True)\n",
    "\n",
    "# Imprime um par exemplo\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.566093",
     "start_time": "2017-10-18T13:16:56.556318"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retorna uma lista de índices, um para cada palavra na sentença.\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo o dataset de treino em Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar uma rede neural precisamos converter as sentenças (strings) em algo que a rede entenda (tensores). Cada sentença será dividida em palavras. Cada palavra será convertida para um índice (que é nada mais que um número inteiro) e isso que é feito pela classe \"Lang\", como explicado anteriormente. Após converter todas as palavras em índices, um símbolo chamado de EOS (End-of-Sequence, fim da sequência) é adicionado ao fim da sequência. Este símbolo é a forma de indicarmos para a rede neural que a sequência terminou.\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)\n",
    "\n",
    "Um Tensor é um array númerico multi-dimensional e que possui um tipo, por exemplo, Float ou Long. Iremos usar tensores do tipo Long para representar os índices do array.\n",
    "\n",
    "As redes neurais em pytorch tem como entrada objetos do tipo Variable, que são que tensores que armazenam o  estado do grafo. Isso torna possível o cálculo automático dos gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Codificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O codificador de uma rede seq2seq é uma RNN que produz um vetor saída e um vetor de estado para cada palavra da sequência de entrada.\n",
    "\n",
    "<img src=\"../figures/encoder-network.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.581322",
     "start_time": "2017-10-18T13:16:56.567999"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Nota: está função é executado de uma só vez em toda a sequência de entrada.\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificador com Mecanismo de Atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretando o modelo de Bahdanau et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mecanismo de atenção (https://arxiv.org/abs/1409.0473) é descrito a seguir pelas equações abaixo.\n",
    "\n",
    "Cada saída do decodificador é condicionada pelas saídas anteriores e alguns $\\mathbf x$, onde $\\mathbf x$ consiste nos vetores de estado atuais (os quais codificam as saídas anteriores) e o \"contexto\" de atenção, o qual é calculado abaixo. A função $g$ é uma camada fully-connected com ativação não-linear que tem como entrada os vetores $y_{i-1}$, $s_i$, e $c_i$ concatenados.\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "O vetor de estado atual $s_i$ é calculado por uma RNN $f$ usando o vetor de estado anterior $s_{i-1}$, a última saída do decodificador $y_{i-1}$, e o vetor de contexto $c_i$.\n",
    "\n",
    "No código, a RNN será uma camada do tipo `nn.GRU`, o vetor de estado $s_i$ será chamado de `hidden`, a sáida $y_i$ será chamada `output`, e o contexto $c_i$ será chamado `context`.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "O vetor de contexto $c_i$ é a soma ponderada de todas as saídas do codificador, onde cada peso $a_{ij}$ é a quantidade de atenção \"attention\" que é prestada ao vetor $h_j$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "... onde cada peso $a_{ij}$ é a \"pontuação\" $e_{ij}$ normalizada ...\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "... onde cada pontuação é calculada como usando uma função $a$ (como uma outra camada linear) que tem como entrada o último vetor de estado $s_{i-1}$ e a saída do codificador $h_j$:\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando o modelo de Bahdanau et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em suma, nosso decodificador consiste em quatro módulos principais: - uma camada de embedding que converte uma palavra (string) em um vetor; uma camada que calcula a pontuação para cada saída do codificador; uma camada RNN; e uma camada de saída.\n",
    "\n",
    "As entradas do decodificador são o último vetor de estado da RNN $s_{i-1}$, última saida do decodificador $y_{i-1}$, e todas as entradas do codificador $h_*$.\n",
    "\n",
    "* Camada embedding com entradas $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* Camada de atenção $a$ com entradas $(s_{i-1}, h_j)$ e saídas $e_{ij}$, normalizadas de maneira a criar $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* Vetor de contexto $c_i$, que é a média ponderada das saídas do codificador de acordo com os pesos atribuídos pelo mecanismo de atenção.\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* Camada(s) RNN $f$ com entradas $(s_{i-1}, y_{i-1}, c_i)$ e vetores de estados, resultantes em $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* Camada de saída $g$ com entradas $(y_{i-1}, s_i, c_i)$, e saída $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.637332",
     "start_time": "2017-10-18T13:16:56.583111"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define os parâmetros\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define as camadas\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note que esta função roda o decodificador por apenas um passo mas usa todas as saídas do codificador.\n",
    "        \n",
    "        # Converte a última palavra escolhida pelo decodificador para um vetor (embedding)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calcula os pesos de atenção e aplica-os as saídas do codificador.\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Concatena o vetor da palavra de entrada com o vetor de contexto de atenção, e envia-os para a RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Camada de saída final\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Retorna saída final (palavra escolhida), vetor de estado, e pesos do mecanismo de atenção (para visualização)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretando os modelos de Luong et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O artigo [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) por Luong et al. descreve outros modelos de atenção que possuem melhores resultados e algumas simplicações. Eles descrevem alguns modelos de \"atenção global\", que diferem do original na forma como as pontuações de atenção são calculadas.\n",
    "\n",
    "De maneira geral, o cálculo dos pesos de atenção depende do vetor de estado produzido pelo decodificador e de todos os vetores de estado produzidos pelo codificador. Uma normalização para que a soma dos pesos seja 1 é feita em seguida.\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "A função \"score\" que compara os dois estados pode ser: *dot*, o produto escalar entre esses dois vetores (*dot*); *general*, um produto escalar entre o vetor de estado do decodificador e uma transformação linear do vetor de estado do codificador; ou *concat*, um produto escalar entre um novo parâmetro $v_a$ e uma transformação linear dos vetores de estados concatenados.\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & dot \\\\\n",
    "h_t ^\\top \\textbf{W}_a \\bar h_s & general \\\\\n",
    "v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ] & concat\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A definição dessas funções nos dá a oportunidade de criar módulos de atenção específicos. A entrada para este módulo é sempre o vetor de estado do decoficador e as saídas do codificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.689476",
     "start_time": "2017-10-18T13:16:56.649298"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Cria variável para amarzenar os pesos calculados pelo mecanismo de atenção.\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calcula os pesos para cada saída do codificador.\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden.squeeze(0), encoder_outputs[i].squeeze(0))\n",
    "\n",
    "        # Normaliza os pesos para ficarem entre 0 e 1, redimensiona para 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos criar um decodificador que usa esse módulo de atenção após a chamada da RNN. Este módulo calculará os pesos e fará a soma ponderada das saídas do codificador usando esses pesos para obter o vetor de contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.720359",
     "start_time": "2017-10-18T13:16:56.691386"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Mantém os parâmetros para referências futuras.\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define as camadas.\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Escolhe o modelo de atenção\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Nota: rodamos essa função a cada passo\n",
    "        \n",
    "        # Transforma palavra atual em vetor (embedding)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combina o vetor da palavra com o último vetor de contexto e roda a RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calcula os pesos do mecanismo de atenção usando o vetor de estado atual e todas as saídas do codificador\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        # Aplica os pesos nos vetores do codificador\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Camada final (predição da próxima palavra) usando o vetor de estado da RNN e o vetor de contexto.\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Retorna a palavra selecionada, vetor de estado, e pesos do mecanismo de atenção (para visualização)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para garantir que o codificador e o decodificador estejam funcionando, iremos fazer um teste rápido com entradas artificiais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.768491",
     "start_time": "2017-10-18T13:16:56.722199"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(10, 10, num_layers=2)\n",
      ")\n",
      "AttnDecoderRNN (\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(20, 10, num_layers=2, dropout=0.1)\n",
      "  (out): Linear (20 -> 10)\n",
      "  (attn): Attn (\n",
      "    (attn): Linear (10 -> 10)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(10, 10, 2)\n",
    "decoder_test = AttnDecoderRNN('general', 10, 10, 2)\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(torch.LongTensor([1, 2, 3]))\n",
    "decoder_attns = torch.zeros(1, 3, 3)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)\n",
    "    print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
    "    decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo uma iteração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar, primeiramente entramos com a sentença original no codificador, palavra por palavra, e mantemos cada saída e o último vetor de estado. Em seguida, o último vetor de estado do codificador é usado como o primeiro vetor de estado do decodificador e o símbolo `<SOS>` (Start Of Sequence, início da sequência) é dado como primeira palavra de entrada. A partir daí, iremos iterar o decodificador para prever as próximas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado Supervisionado e Scheduled Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No aprendizado supervisionado (também chamado de \"Teacher Forcing\" ou maximum likelihood sampling) uma saída \"real\" é usada como sinal de treino para o modelo. Uma alternativa seria usar a própria predição do decodificador como próxima entrada.\n",
    "\n",
    "As saídas das redes treinadas com supervisão frequentemente geram traduções que são corretas gramaticalmente mas erradas do ponto de vista semântico. É como se a rede tivesse aprendido a prestar atenção nas instruções do professor mas não consegue criar frases próprias.\n",
    "\n",
    "Uma solução para este problema é método de [Scheduled Sampling](https://arxiv.org/abs/1506.03099), que alterna entre valores preditos pela rede e os sinais de supervisão durante o treinamento. Durante o treinameto, iremos escolher aleatoriamente quando usar um sinal supervisionado e quando seguir a própria predição da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.817781",
     "start_time": "2017-10-18T13:16:56.770442"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zerar os gradientes\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Obter o tamanho das sentenças de entrada e reais\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Codificar\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Preparar entradas para o decodificador\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Escolher se iremos usar sinal supervisionado (teacher forcing)\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: use a palavra \"real/verdadeira\" como próxima entrada.\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Sem teacher forcing: use a última predição da rede como próxima entrada.\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "            # Use a palavra com maior probabilidade como saída\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Palavra escolhida será a próxima entrada\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Pare caso o símbolo de EOS (End-Of-Sequence) foi escolhido.\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas funções auxiliares para imprimir os tempos de processamento e progresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.826201",
     "start_time": "2017-10-18T13:16:56.819790"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / max(0.0001,percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos iniciar o treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.977352",
     "start_time": "2017-10-18T13:16:56.828388"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Inicializa os modelos\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move os modelos para a GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Inicializa os otimizadores e função de custo (aqui chamada de criterion)\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura as variáveis de treino e monitoramento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:16:56.983809",
     "start_time": "2017-10-18T13:16:56.979015"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50000\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# mantenha o tempo gasto e custo médio por iteração\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0\n",
    "plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar, podemos chamar a função \"train\" várias vezes, que imprimirá um resumo cada vez que for interrompida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:34.085668",
     "start_time": "2017-10-18T13:16:56.985873"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Começa!\n",
    "try:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        # Amostra dados de treino para esta época.\n",
    "        training_pair = variables_from_pair(random.choice(pairs))\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        # Roda a função de treino.\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch == 0: continue\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando as curvas de custo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos o matplotlib e o array `plot_losses`, que foi criado enquanto treinávamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:45.829087",
     "start_time": "2017-10-18T13:17:45.448946"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b99160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDf0HxR+aRG5JPtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111eae748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # coloque ticks em intervalos regulares\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O procedimento para avaliação é quase o mesmo que o de treinamento, mas agora não existem senteças \"reais\" como referência. Neste caso, as predições do decodificador são sempre usadas como a próxima entrada. Ou seja, toda vez que o decodificador prediz uma palavra, essa palavra será adicionada na string de saída. Se o decodificador predizer o símbolo EOS, a execução será interrompida. Iremos também armazenar os pesos de atenção para visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:46.543612",
     "start_time": "2017-10-18T13:17:46.517333"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Codificação\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Cria os vetores iniciais para o decodificador.\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Decodificação\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Escolha a palavra com maior probabilidade\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # A próxima entrada será a palavra escolhida como saída deste passo.\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos avaliar sentenças escolhidas aleatóriamente do dataset de treino para fazermos uma análise qualitativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:47.355335",
     "start_time": "2017-10-18T13:17:47.349356"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:47.833705",
     "start_time": "2017-10-18T13:17:47.766763"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ela e bela .\n",
      "= she is beautiful .\n",
      "< is is is . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando o mecanismo de atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma propriedade útil do mecanismo de atenção são as saídas interpretáveis. Os pesos desse mecanismo podem ser usados para availar onde a rede prestou mais atenção em cada passo.\n",
    "\n",
    "Rode `plt.matshow(attentions)` para ver a saída mostrada como uma matriz, onde as colunas são passos de entrada e as linhas são os passos de saída:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:48.756341",
     "start_time": "2017-10-18T13:17:48.530539"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1113c5e48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAECCAYAAACmB/FKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111259048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\"eu estou com frio .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma melhor visualização, adicionamos títulos nos eixos e legenda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:49.420564",
     "start_time": "2017-10-18T13:17:49.408187"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Configura eixos\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Exibe legenda\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_attention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:50.222385",
     "start_time": "2017-10-18T13:17:49.838278"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = ela tem cinco anos a menos que eu .\n",
      "output = is is is . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAADxCAYAAADBVawCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1115e8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_show_attention(\"ela tem cinco anos a menos que eu .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:51.114492",
     "start_time": "2017-10-18T13:17:50.792591"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = o carro era muito grande .\n",
      "output = is is is . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEMCAYAAADOLq1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1126d9438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_show_attention(\"o carro era muito grande .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T13:17:51.622875",
     "start_time": "2017-10-18T13:17:51.264718"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = eu tenho medo de altura .\n",
      "output = is is is . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEMCAYAAADOLq1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGQVJREFUeJzt3X+0XWV95/H3h0gCCAO0QaX87qyADRYhCeBYoChiA0Wp\nS1YFrLRYpFjpdGYqhel0qeOPtg7jzNQRhMCEHzMK41iK0abF6lKjRUoSiYGgcbJwITfacYIMAi1C\ncj/zx94HTg4399x7zsndz9l+Xll73bN/3Od+CTff85xnP/v7yDYREdE+ezQdQERE7B5J8BERLZUE\nHxHRUknwEREtlQQfEdFSSfARES2VBB8R0VJJ8BERLZUEHxHRUknwEVE8Ve6U9AtNxzJOkuAjYhy8\nHjgRuKTpQMZJEnxEjIPfpkrub5D0oqaDGRdJ8BFRNEkLgWNt/zXwBeDXGg5pbCTBR0Tp3gbcVr++\niQzTzFgSfESU7u1UiR3ba4GDJR3WbEjjIQk+Iool6QDgY7a3dh1+N7CwoZDGirLgx3iQNB84ut7d\nbPvZJuOJwUjai+qG4bHAXp3jtt/eWFDRWunBjwFJpwP/G7gGuBb4jqTTGg2qJumNkv5jvb2h6XjG\nwH8HXgb8CvAV4FDgiUYjKpSkd0haVL+WpJsk/VjSRkknNB3fOEgPfgxIWg9caHtzvX80cJvtpQ3H\n9afAScAn6kMXAGtt/1FzUZVN0n22T5C00fZxkvYEvmr7VU3HVhpJDwAn2H5W0oXAH1DNhz8BeK/t\nUxsNcAykBz8e9uwkdwDb3wH2bDCejl8FzrS90vZKYDlwTsMxla4ztPb/JL0C2B94SYPxlGx711Dk\nOcCtth+1/QXgxQ3GNTaS4MfDOkk3Sjq93m4A1jUdVO2Artf7NxbF+Fgh6UDgj4FVwIPAh5sNqViT\nkg6u71ucQTUHvmPvhmIaK3kibDy8E3gX8C/r/a9SjcU37U+B+yR9CRBwGnBVsyGVS9IewI9tPwas\nAX6+4ZBK9x6qjsw8YJXtTQCSfhl4qMnAxkXG4GMokg6mqhECcK/tf2gyntJJWmd7WdNxjIu6LMF+\n9Zti59iLqXLXk81FNh6S4MeApF8C3gccQdenLtuN9AAlLZnuvO1vzFUs40bSnwHbgP8JPNU5bvtH\njQVVMEkvofr0emx9aBNwre3/01xU4yMJfgxI+jbwr4H1wI7OcduPNhTPl+qXewHLgG9SDdEcB6yz\n/S+aiGscSPruFIfd1Jt1yeqOzSeBm6l+9wGWAr8JvNX23zUU2thIgh8Dkv7e9slNx9FL0h1U09Xu\nr/dfAbzP9nnNRhZtIOke4J227+s5fjxwfYn/JkqTBF+wrqGQX6e60XQH8JPO+aaHQiRtsn1sv2Px\nPEkXTXXc9q1zHUvpJD1oe/Fsz8XzMoumbB/p2e++OWfgtXMYy1Q2SroR+B/1/luBjQ3GMw5O7Hrd\nmf73DSAJ/oUk6cDuG6z1wZ8hU7xnJD34GFg9P/mdVNMjoZr693HbTzcX1Xipi2ndbnt507GURtKl\nwDuoiot1Pq0upXpuYKXt65uKbVwkwY8BSQuANwNHsvMsmvc3FVOHpL2Bw7uftI2Zq0sVPGD7mKZj\nKZGkc4A/pJpFY6oHw662/dlGAxsTGaIZD58BHqeaSfCTPtfOGUlvBK4G5gNH1Te/3m/7jc1GVi5J\nn6VKVFANMywG/ldzEZXN9ueAzzUdx7hKD34MSHrA9iuajqNXXQTttcCXbZ9QH7vf9i82G1m56qcw\nO7YDD9ueaCqekkn6lO1fr19/2PaVXec+b/v1zUU3HnKjYjzcLanEpPms7cd7jqXHML2zbX+l3v7O\n9oSk1KKZ2qKu12f2nDtoLgMZV0nwXep60yt7t6bjAk4B1kvaXNfCvl9SCbNVNtVlXOdJWiTpvwJ3\nNx2UpKMlfbEuN4uk4yT9cdNx1XoTFcBZcx7FeJius9C6jkSdb37Y+b2d4rwkfVTSljoPTPtEOWQM\nvlf3WN9ewJuA7zcUS7dSE8DvAf+O6r7AJ4G7gA80GlHlBuAK4HoA2xslfRL4YFMBSXon8LvAz/e8\nOe8H5InMqe1TL+yxB7B3/Vr11sZqkjcDH2PXU2bPovpUswg4Gfh4/XWXMgY/jbr639dsv7qAWE4B\nFtm+SdJBwL62p3rsfS5jWkaV4I/k+c6CbR/XWFCApLW2T+wsrlEf22D7+AZj2h84kKoCZ3fFzSdS\nh2ZqXSUxpmT7NXMVy1yRdCTwuanuuUm6nup+1231/mbgdNs/2FV76cFPbxEFLMYg6b1UDzkdQ7W6\n/J5UDxf9UpNxUa3k9G7gAWCy4Vi6bZP0z6k/xks6D9jlP4I5Mg/4MVXhrJ1I+pkk+RcahwS+fPly\nb9u2re9169ev3wR0Px+ywvaKWf64Q4BHuvYn6mNJ8DMh6Ql2Htv7B6o5uE17E9UyZd8AsP19Sfs1\nGxIA/7fQ+cjvAlYAL5e0Ffgu1VO2TVrP879bmuJ1io1NoX7O4mjb3+w6djiww/bW5iKrbNu2jXXr\n+q+9I+npJspEJ8HvbH+qRHCU7ffXv0gvazgmgGdsW1KnR1rKcmXvrUsVfJGda+Tc0UQwkv5N1+5q\n4EtU47dPUT0o9p+aiAvA9lGd1/Wj9ouo7vPE9LYDd0g6znanvPKNwB8BjSd4gDkc5t4KHNa1fyh9\n/g6S4Hd2DdVQw2uB91Otdv8X7Fw/pAmfqsffDpD0DuDtVDcSm3Yx8HKqIaPOEI2piqI1ofOp5hiq\n/2efoeohvw24t6GYdiLpEuD3qf5xbgBeRTXz6Iwm4ypVveD2X1IV3Lup7nQdZLuIJSsN7Jics9HJ\nVcDlkm6nurn6+HTj75AE3+tk20sk3Qdg+zFJ85sOimrO76epxnCPoVrK7HWNRlQ5saRH7G3/ewBJ\na4Altp+o998H/FWDoXX7fao3n3tsv0bSy4E/aTim0t1INeR2E3BR/bUQxiOasSnpNuB0YKGkCeC9\nVJ0nbF9H9an0bGAL8I9UHaxpJcHv7FlJ83j+5txBlHHz8Mz6Kb6/7RyQ9BHgyl1/y5y4W9Ji2w82\nHEevlwLPdO0/Ux8rwdO2n5aEpAW2vy2pmDfJEtV/R5J0NHA+cGrTMT3HMDmiERrbF/Q5b6a4ST+d\nJPidfRT4S+Alkj4EnAc09oDMGMydfhWwQdUqRT+hvmHY9DRJqnnE99Yf7QF+jWqOcQkm6gqSdwJ/\nK+kx4OGGY9olSS8rZJ3d/0bVk7+/t3xw00qeap558D3qj8xnUCWrL9r+VoOxFD13WtIRUx233XjC\nqp/y6/T01vSuClSCui7N/sDf2H6m3/VNkPRXtn+1gDj2oZoO+GbbX2g6no4lS5f6a1//et/rXrxg\nwfomZtEkwUdEDGjJ0qX+6t39q3Psu9dejST4DNFERAzI9lzOopm1FBvbhXo1meKUGheUG1vimp1S\n44IyY7Pdd2tKEvyuFfeLVCs1Lig3tsQ1O6XGBQXG5hn8aUqGaCIiBmRGN01yd2hVgu88yl9qe6NS\nalxQbmyJa3ZGGdfSpUtH1RSHH344y5YtG0ls69ev32Z76IVDSp6o0qoEHxHlmUkxriZIGn46b+E3\nWZPgIyIGZNKDj4horckk+IiIdkoPPiKilZqdBtlPEnxExIA8wmqSu0MSfETEECYziyYion2qB53K\n7cInwUdEDCE3WSMi2shODz4ioq3Sg4+IaCEDO5LgIyLaKT34iIiWSoKPiGghF36TtcgVnST1X8U2\nIqIAJS/ZV2QP3varm44hImImSh6iKbUH/2T99WBJayRtkPSApFObji0ioqOaRTPZd2tKkT34LhcC\nd9n+kKR5wD5NBxQR0S3Fxga3FlgpaU/gTtsbei+QdCkFrrQeET8FGh5j76fIIZoO22uA04CtwM2S\nLprimhW2l9leNucBRsRPtc6SfbnJOgBJRwATtm+QtABYAtzacFgREc8peZpk0QkeOB24QtKzwJPA\nC3rwERFNKnmIpsgEb3vf+ustwC0NhxMRMSXb7MiCHxER7ZQ1WSMiWqrkaZJFz6KJiCjZKGfRSFou\nabOkLZKumuL8/pI+K+mbkjZJurhfm0nwERFDGEWCrx/kvAY4C1gMXCBpcc9l7wIetP1KqgkoH5E0\nf7p2M0QTETGo0d1kPQnYYvshAEm3A+cCD3b/NGA/SQL2BX4EbJ+u0ST4iIgBdYZoRuAQ4JGu/Qng\n5J5rPgasAr4P7Ae8xZ6+0E2GaCIihjBZ14SfbgMWSlrXtQ1SXuVXgA3AzwHHAx+T9M+m+4b04CMi\nhjDDaZLb+pRT2Qoc1rV/aH2s28XAn7n6yLBF0neBlwP37qrR9OAjIoZg999mYC2wSNJR9Y3T86mG\nY7p9DzgDQNJLgWOAh6ZrND34iIgBmdHUorG9XdLlwF3APGCl7U2SLqvPXwd8gKro4v2AgCttb5uu\n3ST4iIhBjbBUge3VwOqeY9d1vf4+8PrZtJkEHxExoBHOotktkuAjIoaQBB8R0VKpBx8R0UpONcmI\niDaaxTTIRiTBR0QMIQt+RES00Kjmwe8uSfAREUPILJqIiDaaxYIeTUiCj4gYRhJ8REQ7Te5Igo+I\naJ1qmmQSfEREKyXBR0S0Um6yRkS0lieT4CMiWidj8BERLeaUKoiIaKeCO/BlLrot6e6mY4iI6MvG\nk/23phTZg7f96qZjiIiYiZLH4EvtwT9Zfz1Y0hpJGyQ9IOnUpmOLiOjorMnab2tKkT34LhcCd9n+\nkKR5wD5NBxQR0a3kHnzpCX4tsFLSnsCdtjf0XiDpUuDSOY8sIsLGO8qdRVPkEE2H7TXAacBW4GZJ\nF01xzQrby2wvm/MAI+KnXoZoBiTpCGDC9g2SFgBLgFsbDisi4jkFj9CUneCB04ErJD0LPAm8oAcf\nEdGUzk3WUhWZ4G3vW3+9Bbil4XAiIqaWUgUREW1lJgu+yZoEHxExhPTgIyJaKNUkIyLaLAk+IqKd\nXO4QfBJ8RMQwMkQTEdFGNpNZ8CMion1Kf9Cp6Fo0ERFFMyNb8EPSckmbJW2RdNUurjm9Lp++SdJX\n+rWZHnxExDBG0IOvy6FfA5wJTABrJa2y/WDXNQcA1wLLbX9P0kv6tZsefETEwPpXkpzhEM5JwBbb\nD9l+BrgdOLfnmguBO2x/D8D2D/s1mgQfETGEyUn33YCFktZ1bb1rWBwCPNK1P1Ef63Y0cKCkL0ta\nP1X59F4ZoomIGJDrMfgZ2DaCNSteBCwFzgD2Br4u6R7b35nuGyIiYkAjmkWzFTisa//Q+li3CeBR\n208BT0laA7wS2GWCzxBNRMQQRjQGvxZYJOkoSfOB84FVPdd8BjhF0osk7QOcDHxrukbTg4+IGNho\nluSzvV3S5cBdwDxgpe1Nki6rz19n+1uS/gbYCEwCN9p+YLp2k+AjIgY1wmqStlcDq3uOXdezfzVw\n9UzbTIKPiBiQAe8o90nWJPiIiCGUXKogCT4iYlAzv4naiCT4iIghzLTWTBOS4CMihpAefEREC5Ve\nLjgJPiJiUDbOgh8REe2UNVkjIloqQzQREW00widZd4ck+IiIAeUma0REa5nJHeUOwifBR0QMqvAh\nmiLrwUu6u+kYIiJmxO6/NaTIHrztVzcdQ0TETBTcgS+2B/9k/fVgSWskbZD0gKRTm44tIqKjc5N1\nBCs67RZF9uC7XAjcZftDkuYB+/ReUK9O3rtCeUTE7jfzRbcbUXqCXwuslLQncKftDb0X2F4BrACQ\nVO7fdES0kJksuFRBkUM0HbbXAKdRrS5+s6SLGg4pImInGaIZkKQjgAnbN0haACwBbm04rIiI5xV8\nl7XoBA+cDlwh6VngSSA9+IgohjMGP3u2962/3gLc0nA4ERG7VHAHvswEHxExHrIma0REO5miZ9Ek\nwUdEDMhkDD4iorUyRBMR0UrNFhPrJwk+ImJQhZcLToKPiBjC5I4k+IiI1smSfRERbZUhmoiItsqD\nThERrZUEHxHRUiU/6FR0PfiIiJJ1qkn222ZC0nJJmyVtkXTVNNedKGm7pPP6tZkEHxExhFEs+FEv\nSXoNcBawGLhA0uJdXPdh4PMziS0JPiJiYP2T+wzH6E8Ctth+yPYzwO3AuVNc93vAXwA/nEmjSfAR\nEYMa3RDNIcAjXfsT9bHnSDoEeBPw8ZmGl5usERFDmGEPfaGkdV37K2yvmOWP+i/AlbYnJc3oG5Lg\nIyIGNIsnWbfZXjbN+a3AYV37h9bHui0Dbq+T+0LgbEnbbd+5q0aT4CMiBmY8mgU/1gKLJB1FldjP\nBy7c6SfZR3VeS7oZ+Nx0yR2S4CMiBmfwCPK77e2SLgfuAuYBK21vknRZff66QdpNgo+IGMKonmS1\nvRpY3XNsysRu+7dm0mYSfETEEFKqICKihVIuOCKirWwmd4zkJutukQQfETGM9OAjItrJJMFHRLSO\ns6JTRERbGY9iIvxukgQfETGE9OAjIlpqcjSlCnaLJPiIiAFV9d6T4CMi2ilDNLuPpEuBS5uOIyJ+\nOmWa5G5UF81fASCp3L/piGil3GSNiGglMzm5o+kgdikJPiJiQKU/6DQ2i25LWi3p55qOIyKiWzWT\nZvqtKWPTg7d9dtMxRET0KrkHPzYJPiKiPM40yYiItjJ50CkionXslCqIiGipZm+i9pMEHxExhNSi\niYhoqfTgIyJaKgk+IqKNnGmSERGtZGDSqUUTEdFCmUUTEdFaSfARES2VBB8R0ULVPdbMg4+IaCHj\nlCqIiGinrMkaEdFSGYOPiGglZww+IqKNsiZrRESLjWpNVknLJW2WtEXSVVOcf6ukjZLul3S3pFf2\nazM9+IiIIYxiwQ9J84BrgDOBCWCtpFW2H+y67LvAL9t+TNJZwArg5OnaTYKPiBiYYTRj8CcBW2w/\nBCDpduBc4LkEb/vuruvvAQ7t12iGaCIihuAZ/AEWSlrXtV3a08whwCNd+xP1sV35beCv+8WWHnxE\nxIBmcZN1m+1lo/iZkl5DleBP6XdtEnxExBBGNItmK3BY1/6h9bGdSDoOuBE4y/aj/RpNgo+IGNjI\n5sGvBRZJOooqsZ8PXNh9gaTDgTuAt9n+zkwaTYKPiBjCKGbR2N4u6XLgLmAesNL2JkmX1eevA94D\n/CxwrSSA7f2GfZLgIyIGNMoHnWyvBlb3HLuu6/UlwCWzaTMJPiJiYFmTNSKitUy5tWiGngcv6cv1\n47Ub6u3TXeculfTtertX0ild586RdJ+kb0p6UNLvDBtLRMRcG1Wpgt1hoB68pPnAnrafqg+91fa6\nnmvOAX4HOMX2NklLgDslnQQ8SvWY7Um2JyQtAI6sv+9A248N9p8TETGXPJKbrLvLrHrwkn5B0keA\nzcDRfS6/ErjC9jYA298AbgHeBexH9ebyaH3uJ7Y319/3FkkPSPoDSQfNJr6IiLnUWbKv39aUvgle\n0oslXSzpa8ANVLURjrN9X9dln+gaorm6PnYssL6nuXXAsbZ/BKwCHpZ0W10lbQ947q7xWcA+wBpJ\nn66rrKWsQkQUZ9yHaH4AbAQusf3tXVzzgiGafmxfIukXgdcB76aqovZb9blHgA9I+iBVsl9J9ebw\nxt526poOvXUdIiLmxLjXgz+P6smqOyS9R9IRM2z7QWBpz7GlwKbOju37bf9nquT+5u4L67H6a4GP\nAp8C/u1UP8T2CtvLRlXnISJi5twZp5l+a0jfBG/787bfApwKPA58RtIXJB3Z51v/A/BhST8LIOl4\nqh76tZL2lXR617XHAw/X171e0kbgg8CXgMW2/5XtTUREFGaG1SQbMeNZNHVhmz8H/rzuXe/oOv0J\nSf9Uv95m+3W2V0k6BLhbkoEngN+w/QNJ+wF/KOl64J+Ap6iHZ6huvL7B9sND/ZdFROxmNkxO7uh/\nYUNU8vjRbNVvJBFRkFJzjKT1ww7tzp+/l1/60iP7XjcxsXnonzWIPMkaETGEUt/AIAk+ImIoSfAR\nES3V5INM/STBR0QMquFpkP0kwUdEDMjAZHrwERHtlCGaiIhWarbWTD9J8BERQ0iCj4hooVGuybo7\nJMFHRAzMuOBSBUnwERFDaLKYWD9J8BERQ8gQTURESyXBR0S0ULUkX+bBz5Vt1AuHjMDCur3SlBoX\nlBtb4pqdkcYlaVRNwWhjm+nqdNNKD36O2D5oVG1JWlfiMoClxgXlxpa4ZqfUuKDM2CYn04OPiGin\n9OAjItrImPTgx9GKpgPYhVLjgnJjS1yzU2pcUFhspT/J2qo1WSMi5tIee8zzggV7973u6aefypqs\nERHjpuROchJ8RMTAzGRq0UREtE/pY/B7NB1ARMRY66zLOt02A5KWS9osaYukq6Y4L0kfrc9vlLSk\nX5tJ8BERA/OM/vQjaR5wDXAWsBi4QNLinsvOAhbV26XAx/u1mwQfETEEe7LvNgMnAVtsP2T7GeB2\n4Nyea84FbnXlHuAASQdP12gSfETEECYnJ/tuM3AI8EjX/kR9bLbX7CQ3WSMiBncXVQG0fvaStK5r\nf4Xt3f7QVhJ8RMSAbC8fUVNbgcO69g+tj832mp1kiCYionlrgUWSjpI0HzgfWNVzzSrgono2zauA\nx23/YLpG04OPiGiY7e2SLqca8pkHrLS9SdJl9fnrgNXA2cAW4B+Bi/u1m1o0EREtlSGaiIiWSoKP\niGipJPiIiJZKgo+IaKkk+IiIlkqCj4hoqST4iIiWSoKPiGip/w/fOGdQxGFWZwAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112fb4e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_show_attention(\"eu tenho medo de altura .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "447px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
