{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento - positivo ou negativo na Base de filmes IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise do sentimento é um problema de processamento de linguagem natural onde o texto é entendido e a intenção subjacente é prevista.\n",
    "\n",
    "Este notebook contém um exemplo de predição do sentimento das avaliações de filmes como positivo ou negativo\n",
    "acessando a based de filmes IMDB. Esta é uma base pública contendo 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "\n",
    "O problema de análise de sentimento consiste em analisar um texto de revisão de filme e classificá-lo como revisão positiva ou negativa.\n",
    "\n",
    "Houve uma competição no Kaggle, denominada \"*Bag of Words Meets Bags of Popcorn*\": https://www.kaggle.com/rochachan/bag-of-words-meets-bags-of-popcorn/data\n",
    "que trata justamente de análise de sentimento baseado neste mesmo dataset.\n",
    "\n",
    "Iremos utilizar a solução mis simples possível onde cada texto é codificado como um vetor da ocorrência ou não de\n",
    "cada palavra. Futuramente, esta solução irá utilizar outros modelos onde cada palavra será codificada com seus\n",
    "atributos latentes utilizando o conceito de *embedding*.\n",
    "\n",
    "O objetivo desse primeiro experimento é utilizar uma rede neural clássica para projetar um classificador\n",
    "binário simples (sentimento positivo ou negativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:29.863695Z",
     "start_time": "2017-10-19T09:33:29.833164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchvision import datasets, transforms, models\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras já possui este dataset para leitura. Ele é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. Cada amostra tem um rótulo\n",
    "associado com 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos textos de revisão e rótulos dos sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:42.801301Z",
     "start_time": "2017-10-19T09:33:35.951819Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data('/data/datasets/IMDB/imdb.npz',\n",
    "                                                      num_words=dictionary_size,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=dictionary_size-1,\n",
    "                                                      index_from=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos o número de amostras de treinamento e teste e 2 primeiros textos: o primeiro possui 218 palavras e sentimento positivo\n",
    "enquanto que o segundo possui 189 palavras e sentimento negativo.\n",
    "Observe que as palavras estão codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:42.814645Z",
     "start_time": "2017-10-19T09:33:42.804804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "texto: 0 ( 218 ) - 1 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 9999, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 9999, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 9999, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 9999, 8, 4, 107, 117, 5952, 15, 256, 4, 9999, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 9999, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "texto: 1 ( 189 ) - 0 : [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 9999, 4, 1153, 9, 194, 775, 7, 8255, 9999, 349, 2637, 148, 605, 9999, 8003, 15, 123, 125, 68, 9999, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 9999, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 9999, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 9999, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(x_test))\n",
    "for i in range(2):\n",
    "    print('texto:',i,'(',len(x_train[i]),') -',y_train[i],':',x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:43.036219Z",
     "start_time": "2017-10-19T09:33:42.816973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = [max(sequence) for sequence in x_train]\n",
    "max(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos índices das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras traz junto uma função que carrega o índice das palavras. Este índice é baseado nas palavras mais frequentes, quanto mais frequente a\n",
    "palavra, menor o seu índice. Isso facilita na hora de descartar palavras devido a um limite imposto no tamanho do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:45.251091Z",
     "start_time": "2017-10-19T09:33:45.058913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras no índice: 88584\n",
      "Quatro palavras mais frequentes: ['the', 'and', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index(path='/data/datasets/IMDB/imdb_word_index.json')\n",
    "print('Número de palavras no índice:', len(idx))\n",
    "idx2word = {v: k for k, v in idx.items()}\n",
    "print('Quatro palavras mais frequentes:',[idx2word[i] for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando o texto do primeiro comentário, que é positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito cuidado: a conversão do índice para palavras possui offset de -3:\n",
    "- 0 é reservado para padding;\n",
    "- 1 é reservado para início sequência;\n",
    "- 2 é reservado para palavras raras.\n",
    "Utilizar como verificação: 'french' é iD: 785 \n",
    "\n",
    "Entretanto, o texto não é utilizado no treinamento e predição da rede,\n",
    "ele serve apenas para certificarmos sobre a integridade da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:47.480232Z",
     "start_time": "2017-10-19T09:33:47.473747Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert edged is an amazing actor and now the same being director edged father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for edged and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also edged to the two little boy's that played the edged of norman and paul they were just brilliant children are often left out of the edged list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[0][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto do segundo comentário, negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:48.843550Z",
     "start_time": "2017-10-19T09:33:48.837682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal edged the hair is big lots of boobs edged men wear those cut edged shirts that show off their edged sickening that men actually wore them and the music is just edged trash that plays over and over again in almost every scene there is trashy music boobs and edged taking away bodies and the gym still doesn't close for edged all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[1][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o dataset para codificação on-hot das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe várias formas de preparar os dados para a rede neural.\n",
    "Iremos utilizar uma delas que é utilizar uma codificação on-hot das palavras em\n",
    "cada sequência (amostra). Assim, por exemplo, se uma amostra tiver as palavras\n",
    "5,8,10,543,10,282, o vetor terá 5 ums colocados nas posições 5,8,10,282 e 543."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:55.833515Z",
     "start_time": "2017-10-19T09:33:52.045602Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "        # create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "# our vectorized training data\n",
    "x_oh_train = vectorize_sequences(x_train)\n",
    "# our vectorized test data\n",
    "x_oh_test = vectorize_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe a primeira amostra como ficou. A lista abaixo consiste dos\n",
    "índices da primeira amostra em que os valores são diferentes de zero.\n",
    "Estes índices são justamentes os índices das palavras de cada amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:33:58.058578Z",
     "start_time": "2017-10-19T09:33:58.052355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   1,    4,    5,    6,    7,    8,    9,   12,   13,   14,   15,\n",
      "         16,   17,   18,   19,   21,   22,   25,   26,   28,   30,   32,\n",
      "         33,   35,   36,   38,   39,   43,   46,   48,   50,   51,   52,\n",
      "         56,   62,   65,   66,   71,   76,   77,   82,   87,   88,   92,\n",
      "         98,  100,  103,  104,  106,  107,  112,  113,  117,  124,  130,\n",
      "        134,  135,  141,  144,  147,  150,  167,  172,  173,  178,  192,\n",
      "        194,  215,  224,  226,  256,  283,  284,  297,  316,  317,  336,\n",
      "        381,  385,  386,  400,  407,  447,  458,  469,  476,  480,  515,\n",
      "        530,  546,  619,  626,  670,  723,  838,  973, 1029, 1111, 1247,\n",
      "       1334, 1385, 1415, 1622, 1920, 2025, 2071, 2223, 3766, 3785, 3941,\n",
      "       4468, 4472, 4536, 4613, 5244, 5345, 5535, 5952, 7486, 9999]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.nonzero(x_oh_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferindo o shape dos dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:34:00.527980Z",
     "start_time": "2017-10-19T09:34:00.522115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000) (25000, 10000)\n",
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_oh_train.shape, x_oh_test.shape)\n",
    "print(y_train.shape,    y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural clássica com uma única camada escondida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se aqui uma rede neural mínima, com uma única camada escondida e o embedding com 32 atributos a serem treinados, inicializados aleatóriamente.\n",
    "Lembrar que o embedding é uma forma de entrar com dados categóricos que são trocados pelos seus atributos latentes, a serem treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:34:04.023279Z",
     "start_time": "2017-10-19T09:34:03.997479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model (\n",
       "  (ln1): Linear (10000 -> 16)\n",
       "  (at1): ReLU ()\n",
       "  (ln2): Linear (16 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.ln1 = nn.Linear(10000,16)\n",
    "        self.at1 = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(16,2)\n",
    "        #self.at2 = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.at1(x)\n",
    "        x = self.ln2(x)\n",
    "        #x = self.at2(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:34:45.565302Z",
     "start_time": "2017-10-19T09:34:04.687752Z"
    }
   },
   "outputs": [],
   "source": [
    "xt_val   = torch.from_numpy(x_oh_test).type(torch.FloatTensor)\n",
    "xt_train = torch.from_numpy(x_oh_train).type(torch.FloatTensor)\n",
    "yt_train = torch.from_numpy(y_train)\n",
    "yt_val   = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:34:45.597317Z",
     "start_time": "2017-10-19T09:34:45.576834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 10000]) torch.Size([25000]) torch.FloatTensor torch.LongTensor\n",
      "torch.Size([25000, 10000]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "print(xt_train.size(), yt_train.size(), xt_train.type(), yt_train.type())\n",
    "print(xt_val.size(),   yt_val.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:34:45.610792Z",
     "start_time": "2017-10-19T09:34:45.599966Z"
    }
   },
   "outputs": [],
   "source": [
    "savebest = ptt.ModelCheckpoint('../../models/analisesentimentoIMDB_pt',reset=True, verbose=1)\n",
    "trainer = ptt.DeepNetTrainer(model,\n",
    "                             #criterion = nn.BCELoss(),\n",
    "                             criterion = nn.CrossEntropyLoss(),\n",
    "                             optimizer = torch.optim.RMSprop(model.parameters()),\n",
    "                             callbacks = [savebest, ptt.AccuracyMetric(), ptt.PrintCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T09:35:51.524995Z",
     "start_time": "2017-10-19T09:35:46.209079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 10 epochs\n",
      "  1:   2.3s   T: 0.84427 0.77944   V: 0.44536 0.85360 best\n",
      "  2:   2.4s   T: 0.35217 0.90312   V: 0.36655 0.87364 best\n",
      "  3:   2.4s   T: 0.28589 0.91632   V: 0.33841 0.87000 best\n",
      "  4:   2.4s   T: 0.21437 0.94144   V: 0.31922 0.87460 best\n",
      "  5:   2.3s   T: 0.17449 0.95384   V: 0.33755 0.87348 \n",
      "  6:   2.2s   T: 0.14827 0.96252   V: 0.35368 0.85412 \n",
      "  7:   2.3s   T: 0.13730 0.96188   V: 0.35989 0.87192 \n",
      "  8:   2.3s   T: 0.11076 0.97316   V: 0.37753 0.87060 \n",
      "  9:   2.3s   T: 0.09974 0.97540   V: 0.40845 0.84292 \n",
      " 10:   2.3s   T: 0.09559 0.97516   V: 0.38976 0.86968 \n",
      "Best model was saved at epoch 4 with loss 0.31922: ../../models/analisesentimentoIMDB_pt\n",
      "Stop training at epoch: 10/10\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(10,xt_train, yt_train, valid_data=(xt_val,yt_val),batch_size=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_state('../../models/analisesentimentoIMDB_pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T19:11:16.486604",
     "start_time": "2017-07-18T19:11:16.468704"
    }
   },
   "source": [
    "Observe que com esta rede densa de apenas uma camada escondida, com um total de 160 mil parâmetros,\n",
    "conseguimos uma acurácia de 87%, que é bastante razoável para uma rede simples.\n",
    "É possível conseguir da ordem de 95% de acurácia utilizando métodos estado-da-arte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aprendizados"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
