{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento - positivo ou negativo na Base de filmes IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise do sentimento é um problema de processamento de linguagem natural onde o texto é entendido e a intenção subjacente é prevista.\n",
    "\n",
    "Este notebook contém um exemplo de predição do sentimento das avaliações de filmes como positivo ou negativo\n",
    "acessando a based de filmes IMDB. Esta é uma base pública contendo 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "\n",
    "O problema de análise de sentimento consiste em analisar um texto de revisão de filme e classificá-lo como revisão positiva ou negativa.\n",
    "\n",
    "Houve uma competição no Kaggle, denominada \"*Bag of Words Meets Bags of Popcorn*\": https://www.kaggle.com/rochachan/bag-of-words-meets-bags-of-popcorn/data\n",
    "que trata justamente de análise de sentimento baseado neste mesmo dataset.\n",
    "\n",
    "Iremos utilizar a solução mis simples possível onde cada texto é codificado como um vetor da ocorrência ou não de\n",
    "cada palavra. Futuramente, esta solução irá utilizar outros modelos onde cada palavra será codificada com seus\n",
    "atributos latentes utilizando o conceito de *embedding*.\n",
    "\n",
    "O objetivo desse primeiro experimento é utilizar uma rede neural clássica para projetar um classificador\n",
    "binário simples (sentimento positivo ou negativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:48.245778Z",
     "start_time": "2017-10-18T22:00:43.140283Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchvision import datasets, transforms, models\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import lib.pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do Dataset IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:50.993630Z",
     "start_time": "2017-10-18T22:00:50.989826Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/data/datasets/IMDB/labeledTrainData.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras já possui este dataset para leitura. Ele é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. Cada amostra tem um rótulo\n",
    "associado com 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos textos de revisão e rótulos dos sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:58.872523Z",
     "start_time": "2017-10-18T22:00:52.130597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data('/data/datasets/IMDB/imdb.npz',\n",
    "                                                      num_words=dictionary_size,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=dictionary_size-1,\n",
    "                                                      index_from=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos o número de amostras de treinamento e teste e 2 primeiros textos: o primeiro possui 218 palavras e sentimento positivo\n",
    "enquanto que o segundo possui 189 palavras e sentimento negativo.\n",
    "Observe que as palavras estão codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:58.886560Z",
     "start_time": "2017-10-18T22:00:58.875146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "texto: 0 ( 218 ) - 1 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 9999, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 9999, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 9999, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 9999, 8, 4, 107, 117, 5952, 15, 256, 4, 9999, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 9999, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "texto: 1 ( 189 ) - 0 : [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 9999, 4, 1153, 9, 194, 775, 7, 8255, 9999, 349, 2637, 148, 605, 9999, 8003, 15, 123, 125, 68, 9999, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 9999, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 9999, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 9999, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(x_test))\n",
    "for i in range(2):\n",
    "    print('texto:',i,'(',len(x_train[i]),') -',y_train[i],':',x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:59.090067Z",
     "start_time": "2017-10-18T22:00:58.888726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = [max(sequence) for sequence in x_train]\n",
    "max(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos índices das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras traz junto uma função que carrega o índice das palavras. Este índice é baseado nas palavras mais frequentes, quanto mais frequente a\n",
    "palavra, menor o seu índice. Isso facilita na hora de descartar palavras devido a um limite imposto no tamanho do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:59.225498Z",
     "start_time": "2017-10-18T22:00:59.092635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras no índice: 88584\n",
      "Quatro palavras mais frequentes: ['the', 'and', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index(path='/data/datasets/IMDB/imdb_word_index.json')\n",
    "print('Número de palavras no índice:', len(idx))\n",
    "idx2word = {v: k for k, v in idx.items()}\n",
    "print('Quatro palavras mais frequentes:',[idx2word[i] for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando o texto do primeiro comentário, que é positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito cuidado: a conversão do índice para palavras possui offset de -3:\n",
    "- 0 é reservado para padding;\n",
    "- 1 é reservado para início sequência;\n",
    "- 2 é reservado para palavras raras.\n",
    "Utilizar como verificação: 'french' é iD: 785 \n",
    "\n",
    "Entretanto, o texto não é utilizado no treinamento e predição da rede,\n",
    "ele serve apenas para certificarmos sobre a integridade da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:01.534379Z",
     "start_time": "2017-10-18T22:01:01.528495Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert edged is an amazing actor and now the same being director edged father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for edged and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also edged to the two little boy's that played the edged of norman and paul they were just brilliant children are often left out of the edged list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[0][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto do segundo comentário, negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:02.915764Z",
     "start_time": "2017-10-18T22:01:02.908222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal edged the hair is big lots of boobs edged men wear those cut edged shirts that show off their edged sickening that men actually wore them and the music is just edged trash that plays over and over again in almost every scene there is trashy music boobs and edged taking away bodies and the gym still doesn't close for edged all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[1][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o dataset para codificação on-hot das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe várias formas de preparar os dados para a rede neural.\n",
    "Iremos utilizar uma delas que é utilizar uma codificação on-hot das palavras em\n",
    "cada sequência (amostra). Assim, por exemplo, se uma amostra tiver as palavras\n",
    "5,8,10,543,10,282, o vetor terá 5 ums colocados nas posições 5,8,10,282 e 543."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:10.479304Z",
     "start_time": "2017-10-18T22:01:07.071915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "        # create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "# our vectorized training data\n",
    "x_oh_train = vectorize_sequences(x_train)\n",
    "# our vectorized test data\n",
    "x_oh_test = vectorize_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe a primeira amostra como ficou. A lista abaixo consiste dos\n",
    "índices da primeira amostra em que os valores são diferentes de zero.\n",
    "Estes índices são justamentes os índices das palavras de cada amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:14.888306Z",
     "start_time": "2017-10-18T22:01:14.882156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   1,    4,    5,    6,    7,    8,    9,   12,   13,   14,   15,\n",
      "         16,   17,   18,   19,   21,   22,   25,   26,   28,   30,   32,\n",
      "         33,   35,   36,   38,   39,   43,   46,   48,   50,   51,   52,\n",
      "         56,   62,   65,   66,   71,   76,   77,   82,   87,   88,   92,\n",
      "         98,  100,  103,  104,  106,  107,  112,  113,  117,  124,  130,\n",
      "        134,  135,  141,  144,  147,  150,  167,  172,  173,  178,  192,\n",
      "        194,  215,  224,  226,  256,  283,  284,  297,  316,  317,  336,\n",
      "        381,  385,  386,  400,  407,  447,  458,  469,  476,  480,  515,\n",
      "        530,  546,  619,  626,  670,  723,  838,  973, 1029, 1111, 1247,\n",
      "       1334, 1385, 1415, 1622, 1920, 2025, 2071, 2223, 3766, 3785, 3941,\n",
      "       4468, 4472, 4536, 4613, 5244, 5345, 5535, 5952, 7486, 9999]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.nonzero(x_oh_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferindo o shape dos dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:16.474750Z",
     "start_time": "2017-10-18T22:01:16.468570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000) (25000, 10000)\n",
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_oh_train.shape, x_oh_test.shape)\n",
    "print(y_train.shape,    y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural clássica com uma única camada escondida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se aqui uma rede neural mínima, com uma única camada escondida e o embedding com 32 atributos a serem treinados, inicializados aleatóriamente.\n",
    "Lembrar que o embedding é uma forma de entrar com dados categóricos que são trocados pelos seus atributos latentes, a serem treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:01:20.222973Z",
     "start_time": "2017-10-18T22:01:20.202939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model (\n",
       "  (ln1): Linear (10000 -> 16)\n",
       "  (at1): ReLU ()\n",
       "  (ln2): Linear (16 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.ln1 = nn.Linear(10000,16)\n",
    "        self.at1 = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(16,2)\n",
    "        #self.at2 = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.at1(x)\n",
    "        x = self.ln2(x)\n",
    "        #x = self.at2(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:02:04.782822Z",
     "start_time": "2017-10-18T22:01:20.972222Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_val   = torch.FloatTensor(x_oh_test)\n",
    "xt_train = torch.FloatTensor(x_oh_train)\n",
    "yt_train = torch.unsqueeze(torch.LongTensor(y_train),1)\n",
    "yt_val   = torch.unsqueeze(torch.LongTensor(y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:02:04.816873Z",
     "start_time": "2017-10-18T22:02:04.792677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 10000]) torch.Size([25000, 1])\n",
      "torch.Size([25000, 10000]) torch.Size([25000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(xt_train.size(), yt_train.size())\n",
    "print(xt_val.size(),   yt_val.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:02:15.633444Z",
     "start_time": "2017-10-18T22:02:15.627159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = ptt.DeepNetTrainer(model,\n",
    "                             #criterion = nn.BCELoss(),\n",
    "                             criterion = nn.CrossEntropyLoss(),\n",
    "                             optimizer= torch.optim.RMSprop(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:02:23.487032Z",
     "start_time": "2017-10-18T22:02:22.484403Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /Users/soumith/miniconda2/conda-bld/pytorch_1503975723910/work/torch/lib/THNN/generic/ClassNLLCriterion.c:22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-622a846c832d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/robertoalotufo/mylocalprojects/rnap/PyTorch/lib/pytorch_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epochs, Xin, Yin, valid_data, valid_split, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/robertoalotufo/mylocalprojects/rnap/PyTorch/lib/pytorch_trainer.py\u001b[0m in \u001b[0;36mfit_loader\u001b[0;34m(self, n_epochs, train_data, valid_data)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mYpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 482\u001b[0;31m                                self.ignore_index)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maveraged\u001b[0m \u001b[0mover\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mignored\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /Users/soumith/miniconda2/conda-bld/pytorch_1503975723910/work/torch/lib/THNN/generic/ClassNLLCriterion.c:22"
     ]
    }
   ],
   "source": [
    "trainer.fit(1,xt_train, yt_train, valid_data=(xt_val,yt_val),batch_size=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando as perdas de treinamento e de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:14.526123Z",
     "start_time": "2017-10-18T23:59:11.744Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "epochs = range(len(loss_values))\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss_values, 'b-o')\n",
    "# g+ is for \"green crosses\"\n",
    "plt.plot(epochs, val_loss_values, 'g-+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando a acurácia dos dados de treinamento e de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:14.526499Z",
     "start_time": "2017-10-18T23:59:11.749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_values = history.history['acc']\n",
    "val_acc_values = history.history['val_acc']\n",
    "plt.plot(epochs, acc_values, 'b-o')\n",
    "plt.plot(epochs, val_acc_values, 'g-+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento final, agora com todos os dados de treinamento disponíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supondo que os dados de testes não são conhecidos na fase de projeto do classificador (treinamento),\n",
    "o melhor que podemos fazer é utilizar um treinamento final com todos os dados disponíveis e\n",
    "treinar apenas o número de épocas que deve dar o melhor resultado, baseado no exemplo de treinamento\n",
    "anterior feito com a validação de dados. Neste caso, vamos treinar por 4 épocas. Note que nesse\n",
    "caso não existem dados de validação e não temos como medir o quanto há de *overfitting* no treinamento\n",
    "do classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:14.526796Z",
     "start_time": "2017-10-18T23:59:11.753Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_oh_train, y_train, epochs=4, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do desempenho final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:14.527075Z",
     "start_time": "2017-10-18T23:59:11.757Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_oh_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T19:11:16.486604",
     "start_time": "2017-07-18T19:11:16.468704"
    }
   },
   "source": [
    "Observe que com esta rede densa de apenas uma camada escondida, com um total de 160 mil parâmetros,\n",
    "conseguimos uma acurácia de 88%, que é bastante razoável para uma rede simples.\n",
    "É possível conseguir da ordem de 95% de acurácia utilizando métodos estado-da-arte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando predições com dados novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-18T22:00:14.527366Z",
     "start_time": "2017-10-18T23:59:11.762Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(x_oh_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aprendizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
