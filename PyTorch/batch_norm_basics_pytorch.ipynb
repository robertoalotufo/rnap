{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Batch Normalization é uma técnica que permite normalizar automaticamente os valores que atravessam uma camada da rede neural. Normalmente o uso do BatchNorm permite um treinamento mais rápido e com menores cuidados na inicialização dos pesos. Permite que redes profundas sejam treinadas mais facilmente. \n",
    "\n",
    "A normalização é feita para que o resultado tenha média zero e variância unitária, porém, em seguida, o resultado é\n",
    "escalado pelo fator $\\gamma$ e somado ao fator $\\beta$. Estes dois fatores são parâmetros que serão também otimizados durante o treinamento do gradiente descendente.\n",
    "\n",
    "Atualmente a técnica de batch normalization é utilizada em todas as redes profundas. \n",
    "\n",
    "A camada de batch normalization é colocada entre a camada densa ou convolucional e antes da camada de ativação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Referência:**\n",
    "- Ioffe, S. and Szegedy, C. (2015), Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift., in Francis R. Bach & David M. Blei, ed., 'ICML' , JMLR.org, , pp. 448-456. \n",
    "    [PDF:arxiv](http://arxiv.org/pdf/1502.03167.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='../figures/batchnorm_neuronios.png', width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='../figures/batchnorm_equations.png',width=500></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Diferença entre fase de treinamento e fase de predição ou avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A normalização ocorre de forma distinta na fase de treinamento e na fase de avaliação:\n",
    "- em treinamento: a média e variância ($\\mu$ e $\\sigma^2$) são estimados a partir dos valores\n",
    "das amostras no mini-batch:\n",
    "    - `running_mean`\n",
    "    - `running_var`\n",
    "- em avaliação: a média e variância é calculada pela média móvel, definida pelo parâmetro momentum ($\\lambda$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\begin{align*} \n",
    "\\boldsymbol{m}_{t} &=  \\lambda \\boldsymbol{m}_{t-1} + (1 - \\lambda) \\boldsymbol{\\mu_t} &&\n",
    "\\boldsymbol{v}_{t} =  \\lambda \\boldsymbol{v}_{t-1} + (1 - \\lambda) \\boldsymbol{\\sigma^{2}_t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-03T14:27:18.839311",
     "start_time": "2017-10-03T14:27:18.595038"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "nr.seed(23456)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch Norm: parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### nn.BatchNorm1d - Entrada com dimensão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Documentação oficial: http://pytorch.org/docs/master/nn.html#normalization-layers\n",
    "\n",
    "Quando a entrada da camada tem duas dimensões (amostras e atributos) a normalização por atributo. Ou seja, calcula-se a estatística (média e variância) para cada coluna da matriz de dados, em cada *mini-batch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experimento 1 - aplicação BatchNorm tanto para treino como para teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Criação da camada BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model = torch.nn.BatchNorm1d(5)        # Rede formada de apenas uma camada BatchNorm\n",
    "\n",
    "x = Variable(torch.randn(4, 5)) * 100. # Entrada x\n",
    "model.training = True\n",
    "\n",
    "y = model(x)                           # Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Entrada x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-133.6756 -113.6071   23.2839 -172.4477   51.1140\n",
       " -61.2595   47.0261   65.2737   57.1934  170.6041\n",
       " 100.2897 -231.1064  110.7505  171.6769   70.1500\n",
       "  62.7425  117.5225    9.5521 -151.1595   12.5337\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parâmetros: weight e bias -> gamma e beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "[ 0.19  0.5   0.62  0.82  0.44]\n",
      "bias\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "running_mean\n",
      "[-0.8  -4.5   5.22 -2.37  7.61]\n",
      "running_var\n",
      "[ 1179.64  2475.06   209.37  2770.49   455.28]\n"
     ]
    }
   ],
   "source": [
    "param_dict = model.state_dict()\n",
    "for name,value in param_dict.items():\n",
    "    print(name)\n",
    "    print(value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Saída  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.2560 -0.2505 -0.4552 -0.8442 -0.1874\n",
       "-0.1085  0.3364  0.2055  0.4589  0.7086\n",
       " 0.2205 -0.6798  0.9209  1.1086 -0.0446\n",
       " 0.1440  0.5939 -0.6712 -0.7234 -0.4766\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Conferindo com código próprio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$  y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.2560 -0.2505 -0.4552 -0.8442 -0.1874\n",
      "-0.1085  0.3364  0.2055  0.4589  0.7086\n",
      " 0.2205 -0.6798  0.9209  1.1086 -0.0446\n",
      " 0.1440  0.5939 -0.6712 -0.7234 -0.4766\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma, beta, mv_mean, mv_var = model.state_dict().values()\n",
    "\n",
    "if model.training:\n",
    "    mean = x.mean(dim=0).data\n",
    "    var  = x.var(dim=0, unbiased=False).data\n",
    "else:\n",
    "    mean = mv_mean\n",
    "    var  = mv_var\n",
    "\n",
    "y2 = (x.data - mean) / torch.sqrt(var + model.eps) * gamma + beta\n",
    "\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experimento 2 - Verificando o cálculo das médias móveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (1): Linear (5 -> 1)\n",
       "  (2): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "momentum = 0.1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(5, momentum=momentum),\n",
    "        torch.nn.Linear(5, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "        )\n",
    "model\n",
    "#model = Sequential()\n",
    "#model.add(BatchNormalization(axis=1, input_shape=(5,), momentum=mom, epsilon=0.0001))\n",
    "#model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "[ 0.19  0.5   0.62  0.82  0.44]\n",
      "0.bias\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "0.running_mean\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "0.running_var\n",
      "[ 1.  1.  1.  1.  1.]\n",
      "1.weight\n",
      "[[ 0.1   0.26  0.24  0.25  0.32]]\n",
      "1.bias\n",
      "[-0.2]\n"
     ]
    }
   ],
   "source": [
    "param_dict = model.state_dict()\n",
    "for name,value in param_dict.items():\n",
    "    print(name)\n",
    "    print(value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -2.3211  -5.7813  27.0910 -12.8426  23.3973\n",
      "-13.2257  24.3948  -3.8164  -4.7489   3.5889\n",
      " -2.0350  25.5979 -16.1929   7.6010   2.1562\n",
      "-23.5506  -7.8922  -5.7136  23.8806  13.3141\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n",
      "mean do minibatch: [-10.28   9.08   0.34   3.47  10.61]\n",
      "var  do minibatch:  [  79.03  254.07  260.72  191.83   72.9 ]\n"
     ]
    }
   ],
   "source": [
    "#print([x.shape for x in model.get_weights()], end='\\n\\n')\n",
    "#model.set_weights([[ 1., 1., 1., 1., 1.],     # gamma\n",
    "#                   [ 0., 0., 0., 0., 0.],     # beta\n",
    "#                   [ 0., 0., 0., 0., 0.],     # moving_mean\n",
    "#                   [ 1., 1., 1., 1., 1.]])    # moving_variance\n",
    "\n",
    "gamma, beta, mv_mean, mv_var, w, b = model.state_dict().values()\n",
    "\n",
    "x = torch.randn(4, 5) * 20.\n",
    "y = torch.randn(4, 1)\n",
    "print(x)\n",
    "print('mean do minibatch:',x.mean(0).numpy())\n",
    "print('var  do minibatch: ',x.var(0,unbiased=False).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my running mean: [-9.25  8.17  0.31  3.13  9.55]\n",
      "my running var : [  71.22  228.77  234.75  172.74   65.71]\n"
     ]
    }
   ],
   "source": [
    "mv_mean_new = (1 - momentum) * x.mean(0) + momentum * mv_mean\n",
    "mv_var_new  = (1 - momentum) * x.var(0,unbiased=False) + momentum * mv_var\n",
    "print('my running mean:',mv_mean_new.numpy())\n",
    "print('my running var :',mv_var_new.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 epochs\n",
      "\n",
      "  1:   0.0s   T: 0.92809 best\n",
      "\n",
      "Model from epoch 1 saved as \"None.*\", loss = 0.92809\n"
     ]
    }
   ],
   "source": [
    "from lib.pytorch_utils import DeepNetTrainer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "net_trainer = DeepNetTrainer(model=model,\n",
    "                             optimizer = torch.optim.SGD(model.parameters(),lr=0.1),\n",
    "                             criterion = torch.nn.MSELoss()\n",
    "                            )\n",
    "train_data = DataLoader(TensorDataset(x,y),batch_size=4)\n",
    "net_trainer.fit(1, train_data,use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "[ 0.19  0.5   0.62  0.82  0.44]\n",
      "0.bias\n",
      "[-0.   -0.01 -0.01 -0.01 -0.01]\n",
      "0.running_mean\n",
      "[-1.03  0.91  0.03  0.35  1.06]\n",
      "0.running_var\n",
      "[ 11.44  34.78  35.66  26.48  10.62]\n",
      "1.weight\n",
      "[[ 0.1   0.25  0.24  0.26  0.32]]\n",
      "1.bias\n",
      "[-0.24]\n"
     ]
    }
   ],
   "source": [
    "param_dict = model.state_dict()\n",
    "for name,value in param_dict.items():\n",
    "    print(name)\n",
    "    print(value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A partir daqui era o código do Keras que pode ser retirado depois de conseguirmos fazer a parte de cima funcionar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5,), (5,), (5,), (5,), (5, 1), (1,)]\n",
      "\n",
      "[[  1.74   4.4    7.11  15.93   7.74]\n",
      " [ 15.79  16.52   3.59   8.45   0.57]\n",
      " [  6.19   1.2   19.06   9.33   6.74]\n",
      " [ 14.86   8.33  19.11  12.7   10.43]\n",
      " [  6.52  11.71   0.5   11.32  17.42]\n",
      " [  2.81   6.7   15.46  13.86  18.72]\n",
      " [ 19.9    5.29  15.4    6.41   5.52]\n",
      " [ 15.81   4.68   0.07   0.84  11.68]\n",
      " [ 13.75  17.97  17.19   6.23  12.38]\n",
      " [  1.05  18.93   1.34   6.03   2.17]] [ 9.84  9.57  9.88  9.11  9.34] [ 6.55  6.    7.67  4.25  5.67]\n",
      "\n",
      "lambda: [ 1.    0.98  0.98  1.    1.  ]\n",
      "beta:   [-0.03 -0.02  0.03  0.   -0.02]\n",
      "\n",
      "mv_mean: [ 7.87  7.66  7.91  7.29  7.47]\n",
      "         [ 7.87  7.66  7.91  7.29  7.47]\n",
      "mv_var:  [ 34.54  28.98  47.32  14.66  25.91]\n",
      "         [ 34.54  28.98  47.32  14.66  25.91]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd = SGD(0.5)\n",
    "model.compile(sgd, loss='mse')\n",
    "h = model.fit(x, y, batch_size=10, epochs=1, verbose=0)\n",
    "\n",
    "new_weights = model.get_weights()\n",
    "\n",
    "print('lambda:', new_weights[0])\n",
    "print('beta:  ', new_weights[1])\n",
    "print()\n",
    "print('mv_mean:', new_weights[2])\n",
    "print('        ', mv_mean_new)\n",
    "print('mv_var: ', new_weights[3])\n",
    "print('        ', mv_var_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1260px",
    "left": "0px",
    "right": "2348px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
