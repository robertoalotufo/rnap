{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 8 Review & Transfer Learning with word2vec\n",
    "Import various modules that we need for this notebook (now using Keras 1.0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Problem Set 8, Part 1\n",
    "Let's work through a solution to the first part of problem set 8, where you applied various techniques to the STL-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_in = \"../../../class_data/stl10/\"\n",
    "X_train = np.genfromtxt(dir_in + 'X_train_new.csv', delimiter=',')\n",
    "Y_train = np.genfromtxt(dir_in + 'Y_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt(dir_in + 'X_test_new.csv', delimiter=',')\n",
    "Y_test = np.genfromtxt(dir_in + 'Y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And construct a flattened version of it, for the linear model case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_flat = np.zeros(Y_train.shape[0])\n",
    "Y_test_flat  = np.zeros(Y_test.shape[0])\n",
    "for i in range(10):\n",
    "    Y_train_flat[Y_train[:,i] == 1] = i\n",
    "    Y_test_flat[Y_test[:,i] == 1]   = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) neural network\n",
    "We now build and evaluate a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, input_shape = (X_train.shape[1],)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 29s - loss: 0.6513 - acc: 0.8248    \n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 29s - loss: 0.3446 - acc: 0.9052    \n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 29s - loss: 0.2453 - acc: 0.9326    \n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 29s - loss: 0.1787 - acc: 0.9470    \n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 29s - loss: 0.1196 - acc: 0.9600    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4030eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 5s     \n",
      "Test classification rate 0.90900\n"
     ]
    }
   ],
   "source": [
    "test_rate = model.evaluate(X_test, Y_test)[1]\n",
    "print(\"Test classification rate %0.05f\" % test_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) support vector machine\n",
    "And now, a basic linear support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification rate 0.94088\n"
     ]
    }
   ],
   "source": [
    "svc_obj = SVC(kernel='linear', C=1)\n",
    "svc_obj.fit(X_train, Y_train_flat)\n",
    "\n",
    "pred = svc_obj.predict(X_test)\n",
    "pd.crosstab(pred, Y_test_flat)\n",
    "c_rate = sum(pred == Y_test_flat) / len(pred)\n",
    "print(\"Test classification rate %0.05f\" % c_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) penalized logistc model\n",
    "And finally, an L1 penalized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification rate 0.93712\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, Y_train_flat)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "pd.crosstab(pred, Y_test_flat)\n",
    "c_rate = sum(pred == Y_test_flat) / len(pred)\n",
    "print(\"Test classification rate %0.05f\" % c_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Problem Set 8, Part 2\n",
    "Now, let's read in the Chicago crime dataset and see how well we can get a neural network to perform on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_in = \"../../../class_data/chi_python/\"\n",
    "X_train = np.genfromtxt(dir_in + 'chiCrimeMat_X_train.csv', delimiter=',')\n",
    "Y_train = np.genfromtxt(dir_in + 'chiCrimeMat_Y_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt(dir_in + 'chiCrimeMat_X_test.csv', delimiter=',')\n",
    "Y_test = np.genfromtxt(dir_in + 'chiCrimeMat_Y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, built a neural network for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, input_shape = (434,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "337619/337619 [==============================] - 972s - loss: 0.7771 - acc: 0.7334   \n",
      "Epoch 2/10\n",
      "337619/337619 [==============================] - 984s - loss: 0.7164 - acc: 0.7506   \n",
      "Epoch 3/10\n",
      "337619/337619 [==============================] - 961s - loss: 0.7043 - acc: 0.7547   \n",
      "Epoch 4/10\n",
      "337619/337619 [==============================] - 953s - loss: 0.6943 - acc: 0.7577   \n",
      "Epoch 5/10\n",
      "337619/337619 [==============================] - 953s - loss: 0.6880 - acc: 0.7589   \n",
      "Epoch 6/10\n",
      "337619/337619 [==============================] - 956s - loss: 0.6825 - acc: 0.7603   \n",
      "Epoch 7/10\n",
      "337619/337619 [==============================] - 959s - loss: 0.6785 - acc: 0.7611   \n",
      "Epoch 8/10\n",
      "337619/337619 [==============================] - 958s - loss: 0.6745 - acc: 0.7620   \n",
      "Epoch 9/10\n",
      "337619/337619 [==============================] - 958s - loss: 0.6722 - acc: 0.7631   \n",
      "Epoch 10/10\n",
      "337619/337619 [==============================] - 961s - loss: 0.6699 - acc: 0.7631   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123f31400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downsample, if need be:\n",
    "num_sample = X_train.shape[0]\n",
    "\n",
    "model.fit(X_train[:num_sample], Y_train[:num_sample], batch_size=32,\n",
    "          nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174320/174320 [==============================] - 85s    \n",
      "Test classification rate 0.76034\n"
     ]
    }
   ],
   "source": [
    "test_rate = model.evaluate(X_test, Y_test)[1]\n",
    "print(\"Test classification rate %0.05f\" % test_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Transfer Learning IMDB Sentiment analysis\n",
    "Now, let's use the word2vec embeddings on the IMDB sentiment analysis corpus. This will allow us to use a significantly larger vocabulary of words. I'll start by reading in the IMDB corpus again from the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"../../../class_data/aclImdb/\"\n",
    "\n",
    "ff = [path + \"train/pos/\" + x for x in os.listdir(path + \"train/pos\")] + \\\n",
    "     [path + \"train/neg/\" + x for x in os.listdir(path + \"train/neg\")] + \\\n",
    "     [path + \"test/pos/\" + x for x in os.listdir(path + \"test/pos\")] + \\\n",
    "     [path + \"test/neg/\" + x for x in os.listdir(path + \"test/neg\")]\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "    \n",
    "input_label = ([1] * 12500 + [0] * 12500) * 2\n",
    "input_text  = []\n",
    "\n",
    "for f in ff:\n",
    "    with open(f) as fin:\n",
    "        pass\n",
    "        input_text += [remove_tags(\" \".join(fin.readlines()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll fit a significantly larger vocabular this time, as the embeddings are basically given for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "max_len = 400\n",
    "tok = Tokenizer(num_words)\n",
    "tok.fit_on_texts(input_text[:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = tok.texts_to_sequences(input_text[:25000])\n",
    "X_test  = tok.texts_to_sequences(input_text[25000:])\n",
    "y_train = input_label[:25000]\n",
    "y_test  = input_label[25000:]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test  = sequence.pad_sequences(X_test,  maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for iter in range(num_words):\n",
    "    words += [key for key,value in tok.word_index.items() if value==iter+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = \"/Users/taylor/files/word2vec_python/GoogleNews-vectors-negative300.bin\"\n",
    "w2v = word2vec.Word2Vec.load_word2vec_format(loc, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.zeros((num_words,300))\n",
    "for idx, w in enumerate(words):\n",
    "    try:\n",
    "        weights[idx,:] = w2v[w]\n",
    "    except KeyError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 300, input_length=max_len))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GRU(16,activation='relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.layers[0].set_weights([weights])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1349s - loss: 0.1542 - acc: 0.9439 - val_loss: 0.2739 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1347s - loss: 0.1450 - acc: 0.9486 - val_loss: 0.3019 - val_acc: 0.8990\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1355s - loss: 0.1371 - acc: 0.9499 - val_loss: 0.2853 - val_acc: 0.8960\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1359s - loss: 0.1303 - acc: 0.9527 - val_loss: 0.3348 - val_acc: 0.8966\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1383s - loss: 0.1214 - acc: 0.9565 - val_loss: 0.3109 - val_acc: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f5a3f60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, nb_epoch=5, verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
