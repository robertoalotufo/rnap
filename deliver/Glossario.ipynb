{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossário\n",
    "\n",
    "Descrição sucinta dos termos mais utilizados em redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regressão Linear - ajuste de uma reta de estimativa de valor a partir de uma ou mais variáveis\n",
    "* Regressão Softmax - classificação utilizando a regressão linear seguida de módulo Softmax para atribuir probabilidades às classes\n",
    "* Época - intervalo de treinamento onde se utilizam todos os dados de treinamento\n",
    "* Batch - número de dados de treinamento para o cálculo do gradiente descendente ou outro critério de minimização de função de perda\n",
    "* Dados de treinamento, validação e teste\n",
    "    - treinamento: utilizados para o ajuste dos parâmetros visando minimizar função de perda\n",
    "    - validação: utilizado para verificar se o treinamento está tendo *overfit*\n",
    "    - teste: utilizado para avaliar o desempenho do classificador\n",
    "* Momentum: variaçao do gradiente descendente, dando uma certa inércia no caminho do gradiente descendente\n",
    "* Formas de evitar o overfitting:\n",
    "    - Regularização L2:para evitar overfitting, A função de perda é somada proporcionalmente com fator\n",
    "      lambda de regularização que é multiplicado pelo W ao quadrado.\n",
    "    - Dropout: técnica de desligar alguns neurônios (de forma aleatória) durante o treinamento.\n",
    "    - Early stopping: técnica de detecção do overfitting. Exige o conjunto de dados de validação. Quando no final de cada época, \n",
    "      a avaliação no conjunto de validação for pior por n épocas (fator de paciência), o treinamento é terminado.\n",
    "* Hyperparâmetros: parâmetros de sintonia da rede neural, número de camadas, taxa de aprendizado, número de neurônios, critério de minimização, etc.\n",
    "* Rede neural convolucional: rede neural onde uma ou mais camadas são implementadas por convoluções\n",
    "* Taxa de aprendizagem (*learning rate*): fator de multiplicação do ajuste dos parâmetros na direção oposta do gradiente\n",
    "* Gradiente descendente: método numérico de minimização de função\n",
    "  * em lote: (*batch gradient descent*) a atualização do parâmetro se dá após o uso de todos os dados de treinamento\n",
    "  * estocástico: (*Stochastic gradient descend SGD*) a atualização do parâmetro se dá com cada amostra de treinamento\n",
    "  * mini-lote: (*mini-batch gradient descent*) a atualização do parâmetro se dá a cada conjunto de amostras de treinamento\n",
    "* Early-Stop: método de parada do laço de minimização onde se verifica se a função de perda está ficando pior nos dados de validação\n",
    "* Softmax: Função de ativação que calcula a probabilidade de cada classe de saída\n",
    "* Back Propagation: método eficiente de cálculo do gradiente da função de perda em redes neurais\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
