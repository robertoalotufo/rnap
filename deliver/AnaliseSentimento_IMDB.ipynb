{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento - positivo ou negativo na Base de filmes IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise do sentimento é um problema de processamento de linguagem natural onde o texto é entendido e a intenção subjacente é prevista.\n",
    "\n",
    "Este notebook contém um exemplo de predição do sentimento das avaliações de filmes como positivo ou negativo utilizando o Keras e\n",
    "acessando a based de filmes IMDB. Esta é uma base pública contendo 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "\n",
    "O problema de análise de sentimento consiste em analisar um texto de revisão de filme e classificá-lo como revisão positiva ou negativa.\n",
    "\n",
    "Houve uma competição no Kaggle, denominada \"*Bag of Words Meets Bags of Popcorn*\": https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "que trata justamente de análise de sentimento baseado neste mesmo dataset.\n",
    "\n",
    "Iremos utilizar 3 soluções:\n",
    "- rede neural clássica com embedding treinável e 1 camadas escondida\n",
    "- rede convolucional com embedding treinável, uma convolucional e 2 camadas densas\n",
    "- rede anterior, porém com embedding pré treinado com o word2vec Glove\n",
    "- rede multiconvolucional com embedding pré treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:10:17.845870",
     "start_time": "2017-06-11T17:10:17.833840"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.utils import get_file\n",
    "from keras.datasets import imdb\n",
    "\n",
    "sys.path.append('../src')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend:        tensorflow\n",
      "Data format:    channels_last\n",
      "Available GPUS: []\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "# K.set_image_data_format('channels_first')\n",
    "K.set_floatx('float32')\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "print('Available GPUS:', get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de apoio ao treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model, X_train, y_train, Xval, yval, \n",
    "                  model_name = None,\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  opt='rmsprop', batch_size=60, nepochs=100, patience=10, nr_seed=20170522, \n",
    "                  reset=False, ploss=1.0):\n",
    "\n",
    "    print('DEBUG:',opt)\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss, optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot)\n",
    "        \n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "        cb.patience = patience\n",
    "\n",
    "    past_epochs = cb.get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs...\".format(tr_epochs))\n",
    "    try:\n",
    "        h = model.fit(X_train, y_train, batch_size=60, epochs=tr_epochs, verbose=0, \n",
    "                      validation_data=(Xval, yval), callbacks=[cb])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return model, cb\n",
    "\n",
    "def test_network(model_name, X_test, y_test):\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    print('Model from epoch {}'.format(histo.best_epoch))\n",
    "    print(\"[INFO] evaluating in the test data set ...\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, batch_size=128, verbose=1)\n",
    "    print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do Dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras já possui este dataset para leitura. Ele é composto de 25 mil amostras de treinamento e 25 mil amostras de teste.\n",
    "Cada amostra possui um texto de tamanho que varia entre 11 e 2494 palavras. Cada amostra tem um rótulo\n",
    "associado com 1 para denominar sentimento positivo e 0 para sentimento negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos textos de revisão e rótulos dos sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:00:14.985214",
     "start_time": "2017-06-11T16:00:08.817902"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos o número de amostras de treinamento e teste e 2 primeiros textos: o primeiro possui 218 palavras e sentimento positivo\n",
    "enquanto que o segundo possui 189 palavras e sentimento negativo.\n",
    "Observe que as palavras estão codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:00:14.997640",
     "start_time": "2017-06-11T16:00:14.988874"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "texto: 0 ( 218 ) - 1 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "texto: 1 ( 189 ) - 0 : [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(x_test))\n",
    "for i in range(2):\n",
    "    print('texto:',i,'(',len(x_train[i]),') -',y_train[i],':',x_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos índices das palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras traz junto uma função que carrega o índice das palavras. Este índice é baseado nas palavras mais frequentes, quanto mais frequente a\n",
    "palavra, menor o seu índice. Isso facilita na hora de descartar palavras devido a um limite imposto no tamanho do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:00:08.732023",
     "start_time": "2017-06-11T16:00:08.648966"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras no índice: 88584\n",
      "Quatro palavras mais frequentes: ['the', 'and', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()\n",
    "print('Número de palavras no índice:', len(idx))\n",
    "idx2word = {v: k for k, v in idx.items()}\n",
    "print('Quatro palavras mais frequentes:',[idx2word[i] for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando o texto do primeiro comentário, que é positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito cuidado: a conversão do índice para palavras possui offset de -3:\n",
    "- 0 é reservado para padding;\n",
    "- 1 é reservado para início sequência;\n",
    "- 2 é reservado para palavras raras.\n",
    "Utilizar como verificação: 'french' é iD: 785 \n",
    "\n",
    "Entretanto, o texto não é utilizado no treinamento e predição da rede,\n",
    "ele serve apenas para certificarmos sobre a integridade da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:00:15.017288",
     "start_time": "2017-06-11T16:00:15.000608"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[0][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto do segundo comentário, negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o-3] for o in x_train[1][1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe dois ajustes necessários do dataset antes de colocá-lo na rede neural:\n",
    "1. Tornar os textos de cada amostra com mesmo comprimento\n",
    "2. Limitar o vocabulário "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textos com mesmo comprimento - truncar ou completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa-se que o comprimento mínimo de palavras é 11, o máximo é 2494 e o valor médio é 238.\n",
    "Assim, iremos utilizar um comprimento padrão de 500 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:00:17.673283",
     "start_time": "2017-06-11T16:00:17.661912"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2494, 238.71364)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, x_train)))\n",
    "(lens.min(), lens.max(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `pad_sequences` do Keras trunca o texto em 500 palavras e caso o texto tenha\n",
    "um número menor de palavras, ele é completado com os valores zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:01:50.651286",
     "start_time": "2017-06-11T16:01:49.953897"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 500\n",
    "\n",
    "x_train_pad = sequence.pad_sequences(x_train, maxlen=SEQ_LEN+1, value=0,\n",
    "                                     padding='post', \n",
    "                                     truncating='post')[:,1:] # eliminando primeiro código: 1\n",
    "x_test_pad  = sequence.pad_sequences(x_test,  maxlen=SEQ_LEN+1, value=0, \n",
    "                                     padding='post', \n",
    "                                     truncating='post')[:,1:] # eliminando primeiro código: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:02:00.024464",
     "start_time": "2017-06-11T16:02:00.012966"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   14    22    16    43   530   973  1622  1385    65   458  4468    66\n",
      "  3941     4   173    36   256     5    25   100    43   838   112    50\n",
      "   670 22665     9    35   480   284     5   150     4   172   112   167\n",
      " 21631   336   385    39     4   172  4536  1111    17   546    38    13\n",
      "   447     4   192    50    16     6   147  2025    19    14    22     4\n",
      "  1920  4613   469     4    22    71    87    12    16    43   530    38\n",
      "    76    15    13  1247     4    22    17   515    17    12    16   626\n",
      "    18 19193     5    62   386    12     8   316     8   106     5     4\n",
      "  2223  5244    16   480    66  3785    33     4   130    12    16    38\n",
      "   619     5    25   124    51    36   135    48    25  1415    33     6\n",
      "    22    12   215    28    77    52     5    14   407    16    82 10311\n",
      "     8     4   107   117  5952    15   256     4 31050     7  3766     5\n",
      "   723    36    71    43   530   476    26   400   317    46     7     4\n",
      " 12118  1029    13   104    88     4   381    15   297    98    32  2071\n",
      "    56    26   141     6   194  7486    18     4   226    22    21   134\n",
      "   476    26   480     5   144    30  5535    18    51    36    28   224\n",
      "    92    25   104     4   226    65    16    38  1334    88    12    16\n",
      "   283     5    16  4472   113   103    32    15    16  5345    19   178\n",
      "    32     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vantagem é que agora os dados de treinamento estão na forma de um array (NumPy) contendo\n",
    "25 mil amostras de 500 índices de palavras em cada amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitando o vocabulário de 88.584 para 5000 palavras mais utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe muitas palavras de pouco uso que muitas vezes podem tornar a análise mais difícil.\n",
    "É comum, portanto limitar o vocabulário, convertendo o código das palavras fora do vocabulário\n",
    "como o último código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:02:09.459004",
     "start_time": "2017-06-11T16:02:09.452896"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "\n",
    "x_train_clip = np.clip(x_train_pad,0,VOCAB_SIZE-1)\n",
    "x_test_clip  = np.clip(x_test_pad, 0,VOCAB_SIZE-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural clássica com uma única camada escondida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se aqui uma rede neural mínima, com uma única camada escondida e o embedding com 32 atributos a serem treinados, inicializados aleatóriamente.\n",
    "Lembrar que o embedding é uma forma de entrar com dados categóricos que são trocados pelos seus atributos latentes, a serem treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:03:55.601155",
     "start_time": "2017-06-11T16:03:55.471697"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, 32, input_length=SEQ_LEN),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:05:01.307427",
     "start_time": "2017-06-11T16:04:30.394921"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: adam\n",
      "[INFO] compiling model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compile() got multiple values for argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4e28388ea010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-12d5a82d906d>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(model, X_train, y_train, Xval, yval, model_name, loss, opt, batch_size, nepochs, patience, nr_seed, reset, ploss)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# initialize the optimizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] compiling model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# History, checkpoint, earlystop, plot losses:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compile() got multiple values for argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "opt_adam = Adam()\n",
    "fit_params = {\n",
    "    'model_name': '../../models/SA_IMDB_1',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'opt':        opt_adam,      #SGD(lr=0.001, momentum=0.9, nesterov=True), \n",
    "    'batch_size': 64, \n",
    "    'nepochs':    50,\n",
    "    'patience':   15,\n",
    "    'ploss':      1.5,\n",
    "    'reset':      True,\n",
    "}\n",
    "\n",
    "train_network(model, x_train_clip, y_train, x_test_clip, y_test, **fit_params);\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "#model.fit(x_train_clip, y_train, validation_data=(x_test_clip, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. So we're short of that, but on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Convolucional com max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:06:54.923980",
     "start_time": "2017-06-11T16:06:54.667683"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(VOCAB_SIZE, 32, input_length=SEQ_LEN),\n",
    "    keras.layers.SpatialDropout1D(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:06:57.655935",
     "start_time": "2017-06-11T16:06:57.562854"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:09:19.582989",
     "start_time": "2017-06-11T16:07:06.246129"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv1.fit(x_train_clip, y_train, validation_data=(x_test_clip, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's well past the Stanford paper's accuracy - another win for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1.save_weights(model_path + 'conv1.h5')\n",
    "#conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rede convolucional utilizando embedding com vetores pré treinados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:31:52.746843",
     "start_time": "2017-06-11T16:10:43.995099"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = 'glove.6B.zip'\n",
    "origin = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "fpath = get_file(filename,origin=origin,extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:25.012005",
     "start_time": "2017-06-11T16:53:16.090916"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "GLOVE_DIR = '../../../.keras/datasets/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word  = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:35.554516",
     "start_time": "2017-06-11T16:53:35.547180"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix: ')\n",
    "\n",
    "# prepare embedding matrix\n",
    "EMBEDDING_DIM = len(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:40.357361",
     "start_time": "2017-06-11T16:53:40.306514"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#embedding_matrix = np.random.rand(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for i in range(1+3, VOCAB_SIZE):\n",
    "    word = idx2word[i-3]\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "#embeddings_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Conferindo se o embedding está correto: \"french\" é 785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:44.390582",
     "start_time": "2017-06-11T16:53:44.382569"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(idx2word[785 - 3])\n",
    "print(embeddings_index['french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:45.680965",
     "start_time": "2017-06-11T16:53:45.672752"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix[785])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Listando as palavras não existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:46.781592",
     "start_time": "2017-06-11T16:53:46.774156"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in np.nonzero(embedding_matrix.max(axis=1) == 0.0)[0]:\n",
    "    if i>3:\n",
    "        print(idx2word[i-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:53:56.326494",
     "start_time": "2017-06-11T16:53:55.958518"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, \n",
    "              input_length=SEQ_LEN,\n",
    "              weights=[embedding_matrix], trainable=False),\n",
    "    keras.layers.SpatialDropout1D(0.25),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:54:00.040668",
     "start_time": "2017-06-11T16:53:59.938663"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:56:20.330143",
     "start_time": "2017-06-11T16:54:06.865028"
    },
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_clip, y_train, validation_data=(x_test_clip, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T16:57:27.077473",
     "start_time": "2017-06-11T16:57:27.073401"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True\n",
    "model.optimizer.lr=1e-4\n",
    "model.fit(x_train_clip, y_train, validation_data=(x_test_clip, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As expected, that's given us a nice little boost. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:01:08.309944",
     "start_time": "2017-06-11T17:01:08.208071"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede convolucional com kernels de vários tamanhos em paralelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' [excellent blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:01:23.767016",
     "start_time": "2017-06-11T17:01:23.763026"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functional API to create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:01:29.125472",
     "start_time": "2017-06-11T17:01:28.895851"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D(2)(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = keras.layers.concatenate(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:01:35.602063",
     "start_time": "2017-06-11T17:01:35.114917"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, \n",
    "              input_length=SEQ_LEN, \n",
    "              weights=[embedding_matrix]),\n",
    "    keras.layers.SpatialDropout1D(0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:01:37.249188",
     "start_time": "2017-06-11T17:01:37.147284"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-11T17:07:41.769725",
     "start_time": "2017-06-11T17:01:44.185249"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False\n",
    "model.fit(x_train_clip, y_train, validation_data=(x_test_clip, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, I found that in this case I got best results when I started the embedding layer as being trainable, and then set it to non-trainable after a couple of epochs. I have no idea why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True\n",
    "model.optimizer.lr=1e-5\n",
    "model.fit(x_train_clip, labels_train, validation_data=(x_test_clip, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more complex architecture has given us another boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aprendizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
