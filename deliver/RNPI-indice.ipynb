{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEE 0156 - Prática de Redes Neurais Profundas para Análise de Imagens\n",
    "\n",
    "Professores: *Roberto Lotufo, Rubens Machado, Letícia Rittner*<br>\n",
    "Auxiliares: *Roberto M Souza, Oeslle Lucena, Irene H Fantini*<br>\n",
    "Data: 20 a 23 de fevereiro de 2017, das 9h às 12h e das 14h às 17h<br>\n",
    "Local: Universidade Estadual de Campinas\n",
    "\n",
    "![capa](../figures/curso-Deep-Learning-Processamento-Imagens-Unicamp.png)\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "### I - Introdução ao curso\n",
    "\n",
    "1. Conteúdo e formato do curso \n",
    "2. Introdução a Aprendizagem de Máquina, Redes Neurais Profundas e Redes Convolucionais\n",
    "3. Ferramentas e Ambiente de trabalho durante o curso\n",
    "    - Conceitos novos: acesso ao Jupyter AWS; Usando o Jupyter; Programando Python/Numpy\n",
    "    * NB:[*Acessando o Jupyter da AWS*]() Precisa ser feito\n",
    "    * NB:[*Python e Numpy*]() Precisa ser feito\n",
    "    \n",
    "\n",
    "### II - Introdução a redes neurais\n",
    "\n",
    "#### Regressão Linear\n",
    "- Conceitos novos:\n",
    "- Teoria: Otimização Mínimos Quadrados usando gradiente descendente, notação matemática\n",
    "- Prática: Programação matricial Y = XW + B; Laço de gradiente descendente; Plotando gráficos; visualizando animação da otimização na função custo e no ajuste reta\n",
    "    * NB: [*Regressão Linear*](../dev/2017-02-12-RAL-Regressao-Linear.ipynb) \n",
    "\n",
    "#### Regressão Logística\n",
    "- Conceitos novos:\n",
    "- Teoria: Classificação binária; mesmo modelo anterior de otimização, função Sigmóide\n",
    "- Prática: visualização animação otimização espaço de parâmetros e espaço de atributos\n",
    "    * NB: [*Regressão Logística*]() Precisa ser feito\n",
    "\n",
    "#### Softmax\n",
    "- Conceitos novos:\n",
    "- Teoria: Softmax para classificação multiclasse\n",
    "- Prática: Explorando comportamento do Softmax, com parâmetros interativos\n",
    " * NB: [Softmax](softmax.ipynb)\n",
    " \n",
    "#### Regressão Logística multiclasse\n",
    "- Conceitos novos: \n",
    "- Teoria: Classificação multiclasse com Softmax; Custo entropia cruzada; one-hot-encoding; mesmo modelo anterior de otimização\n",
    "    * NB: [Regressão Logística Multiclasse Softmax - Iris 2D - Programação matricial](Regressao-Logistica-Softmax-Iris-2D-matricial.ipynb)\n",
    "\n",
    "#### Regressão Logística multiclasse, agora no Keras\n",
    "- Conceitos novos:\n",
    "- Prática: Introdução inicial do Keras, mostrando que executa o mesmo exemplo anterior; criando a rede, definindo otimizador\n",
    "    * NB: [Regressão Logística Multiclasse Softmax - Iris 2D - usando o Keras](Regressao-Logistica-Softmax-Iris-2D-Keras.ipynb)\n",
    "\n",
    "#### Regressão Logística multiclasse, dataset NMIST\n",
    "- Conceitos novos:\n",
    "- Prática: dataset NMIST, Visualização dos pesos na forma de template; não é possível mais visualizar espaço de atributos nem espaço de parâmetros. Visualiza-se a queda da função de perda\n",
    " * NB: [Regressão Logística Multiclasse Softmax - NMIST - Programação Matricial](Regressao-Logistica-Softmax-NMIST-matricial.ipynb)\n",
    "\n",
    "#### Regressão Logística multiclasse, dataset NMIST, agora no Keras\n",
    "- Conceitos novos:\n",
    "- Prática: extraindo os pesos aprendidos da rede no Keras\n",
    " * NB: [Regressão Logística Multiclasse Softmax - NMIST - usando o Keras](Regressao-Logistica-Softmax-NMIST-Keras.ipynb)\n",
    "\n",
    "#### Rede neural 2 camadas\n",
    "- Conceitos novos:\n",
    "- Teoria: concatenação de camadas com elemento não linear reLU; retropropagação;\n",
    "- Prática: Dificuldade de otimização, falta de atributos\n",
    " * NB: [Rede Neural 2 camadas com reLU - Iris - usando Keras](Duas-Camadas-Densas-reLU-Softmax-Iris-Keras.ipynb)\n",
    "\n",
    "#### Ilustrando overfitting e underfitting, regularização com exemplo de  ajuste de polinômio\n",
    "- Conceitos novos:\n",
    "- Teoria: Conceito de overfitting e underfitting, conceito de regularização, função de custo\n",
    "- Prática: Explorando parâmetros de ajustes e parâmetros de regularização\n",
    " * NB: [Ajuste curvas: Overfitting - Underfitting - Regularização](overfitting_regularization.ipynb)\n",
    " \n",
    "### Falta: \n",
    "- regularização para rede, \n",
    "- conjunto de dados: treino, validação e testes.\n",
    "- hyperparâmetros: Rubens\n",
    "\n",
    "### III - Redes neurais profundas - redes convolucionais\n",
    "\n",
    "#### Convolução Keras/Theano\n",
    "- Conceitos novos:\n",
    "- Teoria: Convolução, processamento borda, zero-padding, dimensionalidade dos arrays\n",
    "- Prática: rede convolucional no Keras, variação de número de imagens, kernels e filtros de saída\n",
    " * NB: [Convolução](Explorando-Convolucao-no-Keras-usando-Theano.ipynb)\n",
    "\n",
    "#### Introdução ao Keras\n",
    "- Conceitos novos:\n",
    "- Prática: Keras, criando o modelo, colocando pesos, retirando pesos e retirando camadas\n",
    " * NB: [*Introdução ao Keras*](2017-02-10-OASL-CNN-Intro.ipynb)\n",
    "\n",
    "#### CIFAR-10\n",
    "- Conceitos novos:\n",
    "- Prática: Keras - criando a rede, treinando e avaliando\n",
    " * NB: [CIFAR-10 e rede convolucional no Keras](../dev/2017-02-10-RMS-cifar10CNN.ipynb)\n",
    "\n",
    "#### Keras LeNet e MNIST\n",
    "- Conceitos novos:\n",
    "- Teoria: Rede convolucional LeNet\n",
    " * NB: [*KerasLenetMNIST*](../dev/2017-01-25-RCM-Keras-lenet-mnist.ipynb)\n",
    "\n",
    "#### Otimizadores\n",
    "- Conceitos novos: \n",
    "- Teoria: Diversos otimizadores: SGD com momento, SGD com momento Nesterov, SGD com queda de fator de aprendizagem, RMSprop, Adadelta, Adam\n",
    "- Prática: Observação do desempenho de diversos otimizadores\n",
    " * NB: [*Otimizadores*](../dev/2017-01-30-RCM-Keras-optimizers.ipynb)\n",
    "\n",
    "#### Autoencoder convolucional\n",
    "- Conceitos novos:\n",
    "- Teoria: autoencoders, autoencoders convolucionais\n",
    "- Prática: treinando autoencoders\n",
    "- NB:[Autoencoder convolucional](../dev/2017-02-03-RCM-CovolutionalAutoencoder.ipynb)\n",
    "\n",
    "\n",
    "#### Rede totalmente convolucional\n",
    "- Conceitos novos:\n",
    "- Teoria: Convertendo uma rede com camadas densas para totalmente convolucionais para otimizar varredura da imagem\n",
    "- Prática: Explorando o exemplo implementado\n",
    " * NB: [*Convertendo camadas densas para totalmente convolucionais*](../dev/2017-02-02-RCM-Keras-full-conv-01.ipynb)\n",
    " * NB: [*Aplicando rede totalmente convolucional*](../dev/2017-02-02-RCM-Keras-full-conv-02.ipynb)\n",
    " \n",
    "#### Aprendizado com poucos dados, transfer learning\n",
    "- Conceitos novos:\n",
    "- Teoria: Aprendizado com poucos dados, data augmentation, treina do zero 80%\n",
    " * NB: [*Aprendizado com poucos dados*](../dev/2017-02-10-RCM-cats-and-dogs-1.ipynb)\n",
    " - trasfer learning off-the-shelf feature extraction com VGG aprendida 90%\n",
    " * NB: [*Aprendizado com poucos dados*](../dev/2017-02-10-RCM-cats-and-dogs-2.ipynb)\n",
    " - transfer learning,fine tuning,\n",
    " * NB: [*Aprendizado com poucos dados*](../dev/2017-02-10-RCM-cats-and-dogs-3.ipynb)\n",
    " \n",
    "\n",
    "### IV - Aplicações\n",
    "\n",
    "#### Face spoofing\n",
    "- Conceitos novos:\n",
    "- Teoria: pre-processamento, uso do transfer learning\n",
    "- Prática: pré-processamento\n",
    " * NB:[Face antispoofing](../dev/2017-02-07-OASL-FASNet-Adessowiki.ipynb)\n",
    "\n",
    "\n",
    "#### Skull Stripping\n",
    " * NB: [*Skull Stripping 2D*](../dev/2017-02-10-OASL-CNN-Intro.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## Documentação\n",
    "\n",
    "- [Jupyter Notebook](http://jupyter-notebook.readthedocs.io/en/latest/notebook.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook RNPI-indice.ipynb to html\n",
      "[NbConvertApp] Writing 257899 bytes to RNPI-indice.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert RNPI-indice.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
