{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Redes Neurais\n",
    "## Preâmbulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=100, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (3918, 11) (980, 11) (3918, 1) (980, 1)\n",
      "Target: min=3.000, mean=5.878, max=9.000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# Wine Quality Data Set\n",
    "# ---------------------\n",
    "# [https://archive.ics.uci.edu/ml/datasets/Wine+Quality]\n",
    "#\n",
    "# 1. Title: Wine Quality \n",
    "\n",
    "# 2. Sources\n",
    "#    Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n",
    "   \n",
    "# 3. Past Usage:\n",
    "\n",
    "#   P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "#   Modeling wine preferences by data mining from physicochemical properties.\n",
    "#   In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "#   In the above reference, two datasets were created, using red and white wine samples.\n",
    "#   The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n",
    "#   (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n",
    "#   between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n",
    "#   these datasets under a regression approach. The support vector machine model achieved the\n",
    "#   best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n",
    "#   etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n",
    "#   analysis procedure).\n",
    " \n",
    "# 4. Relevant Information:\n",
    "\n",
    "#    The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n",
    "#    For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].\n",
    "#    Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n",
    "#    are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "#    These datasets can be viewed as classification or regression tasks.\n",
    "#    The classes are ordered and not balanced (e.g. there are munch more normal wines than\n",
    "#    excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n",
    "#    or poor wines. Also, we are not sure if all input variables are relevant. So\n",
    "#    it could be interesting to test feature selection methods. \n",
    "\n",
    "# 5. Number of Instances: red wine - 1599; white wine - 4898. \n",
    "\n",
    "# 6. Number of Attributes: 11 + output attribute\n",
    "  \n",
    "#    Note: several of the attributes may be correlated, thus it makes sense to apply some sort of\n",
    "#    feature selection.\n",
    "\n",
    "# 7. Attribute information:\n",
    "\n",
    "#    For more information, read [Cortez et al., 2009].\n",
    "\n",
    "#    Input variables (based on physicochemical tests):\n",
    "#    1 - fixed acidity\n",
    "#    2 - volatile acidity\n",
    "#    3 - citric acid\n",
    "#    4 - residual sugar\n",
    "#    5 - chlorides\n",
    "#    6 - free sulfur dioxide\n",
    "#    7 - total sulfur dioxide\n",
    "#    8 - density\n",
    "#    9 - pH\n",
    "#    10 - sulphates\n",
    "#    11 - alcohol\n",
    "#    Output variable (based on sensory data): \n",
    "#    12 - quality (score between 0 and 10)\n",
    "\n",
    "# 8. Missing Attribute Values: None\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine_data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv\"\n",
    "wine_data_file = '../data/winequality-{}.csv'\n",
    "\n",
    "for red_or_white in ['red', 'white']:\n",
    "    if not os.path.isfile(wine_data_file.format(red_or_white)):\n",
    "        # Read wine quality data from UCI website\n",
    "        req = requests.get(wine_data_url.format(red_or_white))\n",
    "        open(wine_data_file.format(red_or_white), 'w').write(req.text)\n",
    "\n",
    "df_white = pd.read_csv(\"../data/winequality-white.csv\", sep=';')\n",
    "dfw = df_white.as_matrix()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dfw[:,:-1])\n",
    "y = dfw[:,-1]\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "# X = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "Xtra, Xval, ytra, yval = train_test_split(X, y.reshape(-1, 1), train_size=0.8, random_state=20170421)\n",
    "\n",
    "print('Shapes:', Xtra.shape, Xval.shape, ytra.shape, yval.shape)\n",
    "print('Target: min={:.3f}, mean={:.3f}, max={:.3f}'.format(ytra.min(), ytra.mean(), ytra.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align*} \n",
    "\\mathbf{X}_{train} & = \\begin{bmatrix}\n",
    "1 & \\longleftarrow & (\\mathbf{x}^{(0)})^T & \\longrightarrow \\\\ \n",
    "1 & \\longleftarrow & (\\mathbf{x}^{(1)})^T & \\longrightarrow \\\\ \n",
    " &  & \\vdots  & \\\\ \n",
    "1 & \\longleftarrow & (\\mathbf{x}^{(m-1)})^T & \\longrightarrow \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_0^{(0)} & x_1^{(0)} & x_2^{(0)} & \\ldots & x_{11}^{(0)} \\\\\n",
    "1 & x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\ldots & x_{11}^{(1)} \\\\\n",
    " &  & \\vdots  & \\\\ \n",
    "1 & x_0^{(3917)} & x_1^{(3917)} & x_2^{(3917)} & \\ldots & x_{11}^{(3917)}\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\mathbf{y}_{train} & = \\begin{bmatrix}\n",
    "y^{(0)} \\\\\n",
    "y^{(1)} \\\\\n",
    "\\vdots  \\\\ \n",
    "y^{(3917)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementação matricial\n",
    "\n",
    "Nesta implementação da rede neural, o *bias* está incluido na matriz de pesos. A matriz de amostras é aumentada de forma a incluir uma primeira coluna contendo 1s. \n",
    "\n",
    "### Equações para regressão linear\n",
    "Considerando uma amostra apenas (SGD).\n",
    "\n",
    "#### Para a frente:\n",
    "\n",
    "\\begin{align*} \n",
    "\\sigma(\\mathbf{z}) & = \\frac{1}{1+e^{-\\mathbf{z}}} \\\\\n",
    "\\\\\n",
    "\\mathbf{a}_{(i)} & = \\begin{cases}\n",
    "\\mathbf{x}  & \\text{ if } i = 0 \\\\\n",
    "\\\\\n",
    "\\sigma(\\mathbf{W}_{(i)} \\cdot \\mathbf{a}_{(i-1)}) & \\text{ if } 1 \\leq i \\leq L-1 \\\\\n",
    "\\\\\n",
    "\\mathbf{W}_{(i)} \\cdot \\mathbf{a}_{(i-1)} & \\text{ if } i = L \n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\\\\n",
    "J & = \\frac{1}{2} (\\mathbf{a}_{(L)} - \\mathbf{y})^T \\cdot (\\mathbf{a}_{(L)} - \\mathbf{y})\n",
    "\\end{align*}\n",
    "\n",
    "#### Para trás:\n",
    "\n",
    "\\begin{align*} \n",
    "\\boldsymbol{\\delta}_{(i)} & = \\begin{cases}\n",
    "\\mathbf{a}_{(i)} - \\mathbf{y} & \\text{ if } i = L \\\\\n",
    "\\\\\n",
    "\\mathbf{W}_{(i+1)} \\cdot \\boldsymbol{\\delta}_{(i+1)} \\circ \\mathbf{a}_{(i)} (1 - \\mathbf{a}_{(i)}) & \\text{ if } 1 \\leq i \\leq L-1 \n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\\\\n",
    "\\mathbf{W}_{(i)} & = \\mathbf{W}_{(i)} - \\eta \\boldsymbol{\\delta}_{(i)} \\cdot \\mathbf{a}_{(i-1)}^{T}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Equações para regressão logística\n",
    "Considerando uma amostra apenas (SGD).\n",
    "\n",
    "#### Para a frente:\n",
    "\n",
    "\\begin{align*} \n",
    "\\sigma(\\mathbf{z}) & = \\frac{1}{1+e^{-\\mathbf{z}}} \\\\\n",
    "\\\\\n",
    "\\mathbf{a}_{(i)} & = \\begin{cases}\n",
    "\\mathbf{x}  & \\text{ if } i = 0 \\\\\n",
    "\\\\\n",
    "\\sigma(\\mathbf{W}_{(i)} \\cdot \\mathbf{a}_{(i-1)}) & \\text{ if } 1 \\leq i \\leq L \\\\\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\\\\n",
    "J & = - \\sum \\left (\\mathbf{y} \\log{(\\mathbf{a}_{(L)})} + (1 - \\mathbf{y}) \\log{(1 - \\mathbf{a}_{(L)})} \\right )\n",
    "\\end{align*}\n",
    "\n",
    "#### Para trás:\n",
    "\n",
    "\\begin{align*} \n",
    "\\boldsymbol{\\delta}_{(i)} & = \\begin{cases}\n",
    "\\mathbf{a}_{(i)} - \\mathbf{y} & \\text{ if } i = L \\\\\n",
    "\\\\\n",
    "\\mathbf{W}_{(i+1)} \\cdot \\boldsymbol{\\delta}_{(i+1)} \\circ \\mathbf{a}_{(i)} (1 - \\mathbf{a}_{(i)}) & \\text{ if } 1 \\leq i \\leq L-1 \n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\\\\n",
    "\\mathbf{W}_{(i)} & = \\mathbf{W}_{(i)} - \\eta \\boldsymbol{\\delta}_{(i)} \\cdot \\mathbf{a}_{(i-1)}^{T}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### O código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BackPropNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layer_sizes=[], is_classifier=True):\n",
    "        self.L = len(layer_sizes)\n",
    "        self.s = layer_sizes\n",
    "        self.W = None\n",
    "        self.classifier = is_classifier\n",
    "\n",
    "    def init_weights(self, epsilon=None):\n",
    "        from numpy.random import rand\n",
    "        self.W = []\n",
    "        for i in range(self.L-1):\n",
    "            if epsilon is None:\n",
    "                eps = np.sqrt(6.0 / (self.s[i] + self.s[i+1]))\n",
    "            else:\n",
    "                eps = epsilon\n",
    "            self.W.append(2*eps*rand(self.s[i+1], self.s[i]+1) - eps)\n",
    "\n",
    "    def sgd(self, X, y, lr, batch, nepochs):\n",
    "        r = nepochs / 10\n",
    "        m, n = X.shape\n",
    "        n_batches = int(np.ceil(m / batch))\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            for ii in range(n_batches):\n",
    "                kk = batch * ii\n",
    "                X_batch, y_batch = X[kk:kk+batch], y[kk:kk+batch]\n",
    "                \n",
    "                cost, G = self.compute_cost_and_gradient(self.W, X, y)\n",
    "                for i in range(len(self.W)):\n",
    "                    self.W[i] -= lr * G[i]\n",
    "                    \n",
    "            if not epoch % r:\n",
    "                print(('{:4d} Cost: {:.5f}'.format(epoch, cost)))\n",
    "        print(('{:4d} Cost: {:.5f}'.format(epoch, cost)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        a = self.compute_activations(self.W, X)\n",
    "        return a[-1]\n",
    "    \n",
    "    def compute_cost_and_gradient(self, W, X, y):\n",
    "        M, N  = X.shape\n",
    "        # Gradients\n",
    "        G = [None for n in self.s[:-1]]\n",
    "        # Forward propagation\n",
    "        a = self.compute_activations(W, X)\n",
    "        # Back propagation\n",
    "        d = self.compute_errors(W, a, y)\n",
    "        # Cost computation\n",
    "        if self.classifier:\n",
    "            # classifier: binary cross-entropy\n",
    "            J = - (y * np.log(a[-1]) + (1 - y) * np.log(1 - a[-1])).sum() / M\n",
    "        else:\n",
    "            # regressor: mean squared error\n",
    "            J = 0.5 * np.square(y - a[-1]).sum() / M\n",
    "        for j in range(self.L-1):\n",
    "            # gradient computation\n",
    "            G[j] = np.dot(d[j+1].T, a[j]) / M\n",
    "        return J, G\n",
    "\n",
    "    def compute_activations(self, W, X):\n",
    "        a = [None for n in self.s]\n",
    "        a[0] = X\n",
    "        for j in range(1, self.L):\n",
    "            a[j-1] = np.insert(a[j-1], 0, 1, 1)\n",
    "            z = np.dot(a[j-1], W[j-1].T)\n",
    "            if j == self.L-1 and not self.classifier:\n",
    "                a[j] = z\n",
    "            else:\n",
    "                a[j] = self.logistic(z)\n",
    "        return a            \n",
    "\n",
    "    def compute_errors(self, W, a, y):\n",
    "        d = [None for n in self.s]\n",
    "        d[-1] = a[-1] - y\n",
    "        for j in range(self.L-2, 0, -1):\n",
    "            d[j] = np.dot(d[j+1], W[j]) * a[j] * (1 - a[j])\n",
    "            d[j] = d[j][:,1:]\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic(z):\n",
    "        z = np.asarray(z)\n",
    "        z = np.minimum(z,  15)\n",
    "        z = np.maximum(z, -15)\n",
    "        return np.ones(z.shape)/(1.0 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Teste com gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 Cost: 19.81936\n",
      " 100 Cost: 0.29999\n",
      " 200 Cost: 0.28705\n",
      " 300 Cost: 0.28495\n",
      " 400 Cost: 0.28396\n",
      " 500 Cost: 0.28314\n",
      " 600 Cost: 0.28233\n",
      " 700 Cost: 0.28149\n",
      " 800 Cost: 0.28061\n",
      " 900 Cost: 0.27968\n",
      " 999 Cost: 0.27869\n",
      "\n",
      "Trained in 16.796289s\n",
      "\n",
      "MSE: 0.536059832829\n",
      "\n",
      "y_hat [ 5.641  6.569  6.229  5.35   6.035  5.511  6.47   5.11   6.244  6.014]\n",
      "y_val [ 6.  6.  7.  5.  6.  8.  7.  5.  8.  6.]\n",
      "\n",
      "Accuracy: 56.12%\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [n, 40, 20, 1]\n",
    "nepochs = 1000\n",
    "\n",
    "nnet = BackPropNeuralNetwork(layer_sizes, is_classifier=False)\n",
    "nnet.init_weights();\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    nnet.sgd(Xtra, ytra, 0.2, 4128, nepochs)\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "yhat = nnet.predict(Xval)\n",
    "mse = np.square(yhat - yval).mean()\n",
    "print('\\nMSE:', mse)\n",
    "print()\n",
    "print('y_hat', yhat[:10,0])\n",
    "print('y_val', yval[:10,0])\n",
    "print()\n",
    "acc = 100.0 * np.where(np.abs(yhat - yval) < 0.5, 1, 0).sum() / yval.shape[0]\n",
    "print('Accuracy: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 40)                480       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00312: early stopping\n",
      "\n",
      "Trained in 3.534593s\n",
      "\n",
      "MSE: 0.520362105834\n",
      "\n",
      "y_hat [ 5.811  6.795  6.884  5.403  5.63   6.778  6.96   5.481  6.318  6.481]\n",
      "y_val [ 6.  6.  7.  5.  6.  8.  7.  5.  8.  6.]\n",
      "\n",
      "Accuracy: 54.69%\n"
     ]
    }
   ],
   "source": [
    "estop = EarlyStopping(patience=200, verbose=1)\n",
    "\n",
    "def build():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_sizes[1], activation='sigmoid', input_shape=(layer_sizes[0],)))\n",
    "    model.add(Dense(layer_sizes[2], activation='sigmoid'))\n",
    "    model.add(Dense(layer_sizes[3], activation=None))\n",
    "    return model\n",
    "\n",
    "nnet = build()\n",
    "nnet.summary()\n",
    "\n",
    "opt = SGD(lr=0.1, momentum=0.9)\n",
    "nnet.compile(loss=\"mean_squared_error\", optimizer=opt)    \n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    histo2 = nnet.fit(Xtra, ytra, batch_size=516, epochs=1000, verbose=0, \n",
    "                      validation_data=(Xval, yval), callbacks=[estop])\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "print()\n",
    "yhat = nnet.predict(Xval, verbose=0)\n",
    "mse = np.square(yhat - yval).mean()\n",
    "print('MSE:', mse)\n",
    "print()\n",
    "print('y_hat', yhat[:10,0])\n",
    "print('y_val', yval[:10,0])\n",
    "print()\n",
    "acc = 100.0 * np.where(np.abs(yhat - yval) < 0.5, 1, 0).sum() / yval.shape[0]\n",
    "print('Accuracy: {:.2f}%'.format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                440       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 147       \n",
      "=================================================================\n",
      "Total params: 1,407\n",
      "Trainable params: 1,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00233: early stopping\n",
      "\n",
      "Trained in 5.114188s\n",
      " 32/980 [..............................] - ETA: 0s\n",
      "\n",
      "Loss     = 1.23137\n",
      "Accuracy = 56.633\n"
     ]
    }
   ],
   "source": [
    "yy = np.vstack((ytra, yval)).astype(np.int32)\n",
    "ymin, ymax = yy.min(), yy.max()\n",
    "ncat = ymax - ymin + 1\n",
    "\n",
    "ytra_cat = np_utils.to_categorical(ytra.astype(np.int32) - ymin, ncat)\n",
    "yval_cat = np_utils.to_categorical(yval.astype(np.int32) - ymin, ncat)\n",
    "\n",
    "estop = EarlyStopping(patience=200, verbose=1)\n",
    "tb = TensorBoard(log_dir='../logs')\n",
    "\n",
    "def build():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, activation='relu', input_shape=(layer_sizes[0] - 1,)))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(ncat, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "nnet = build()\n",
    "nnet.summary()\n",
    "\n",
    "opt = SGD(lr=0.1, momentum=0.9)\n",
    "nnet.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "Xt = Xtra[:, 1:]\n",
    "Xv = Xval[:, 1:]\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    histo2 = nnet.fit(Xtra[:,1:], ytra_cat, batch_size=516, epochs=1000, verbose=0, \n",
    "                      validation_data=(Xval[:,1:], yval_cat), callbacks=[estop, tb])\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "loss, acc = nnet.evaluate(Xv, yval_cat)\n",
    "print('\\n\\nLoss     = {:.5f}\\nAccuracy = {:.3f}'.format(loss, 100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "697px",
    "left": "0px",
    "right": "1189px",
    "top": "106px",
    "width": "251px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
