{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Redes Neurais\n",
    "## Preâmbulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=100, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (3918, 12) (980, 12) (3918, 1) (980, 1)\n",
      "Target: 3.0 9.0 5.87790935076\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# Wine Quality Data Set\n",
    "# ---------------------\n",
    "# [https://archive.ics.uci.edu/ml/datasets/Wine+Quality]\n",
    "#\n",
    "# 1. Title: Wine Quality \n",
    "\n",
    "# 2. Sources\n",
    "#    Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n",
    "   \n",
    "# 3. Past Usage:\n",
    "\n",
    "#   P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "#   Modeling wine preferences by data mining from physicochemical properties.\n",
    "#   In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "#   In the above reference, two datasets were created, using red and white wine samples.\n",
    "#   The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n",
    "#   (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n",
    "#   between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n",
    "#   these datasets under a regression approach. The support vector machine model achieved the\n",
    "#   best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n",
    "#   etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n",
    "#   analysis procedure).\n",
    " \n",
    "# 4. Relevant Information:\n",
    "\n",
    "#    The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n",
    "#    For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].\n",
    "#    Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n",
    "#    are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "#    These datasets can be viewed as classification or regression tasks.\n",
    "#    The classes are ordered and not balanced (e.g. there are munch more normal wines than\n",
    "#    excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n",
    "#    or poor wines. Also, we are not sure if all input variables are relevant. So\n",
    "#    it could be interesting to test feature selection methods. \n",
    "\n",
    "# 5. Number of Instances: red wine - 1599; white wine - 4898. \n",
    "\n",
    "# 6. Number of Attributes: 11 + output attribute\n",
    "  \n",
    "#    Note: several of the attributes may be correlated, thus it makes sense to apply some sort of\n",
    "#    feature selection.\n",
    "\n",
    "# 7. Attribute information:\n",
    "\n",
    "#    For more information, read [Cortez et al., 2009].\n",
    "\n",
    "#    Input variables (based on physicochemical tests):\n",
    "#    1 - fixed acidity\n",
    "#    2 - volatile acidity\n",
    "#    3 - citric acid\n",
    "#    4 - residual sugar\n",
    "#    5 - chlorides\n",
    "#    6 - free sulfur dioxide\n",
    "#    7 - total sulfur dioxide\n",
    "#    8 - density\n",
    "#    9 - pH\n",
    "#    10 - sulphates\n",
    "#    11 - alcohol\n",
    "#    Output variable (based on sensory data): \n",
    "#    12 - quality (score between 0 and 10)\n",
    "\n",
    "# 8. Missing Attribute Values: None\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine_data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv\"\n",
    "wine_data_file = '../data/winequality-{}.csv'\n",
    "\n",
    "for red_or_white in ['red', 'white']:\n",
    "    if not os.path.isfile(wine_data_file.format(red_or_white)):\n",
    "        # Read wine quality data from UCI website\n",
    "        req = requests.get(wine_data_url.format(red_or_white))\n",
    "        open(wine_data_file.format(red_or_white), 'w').write(req.text)\n",
    "\n",
    "df_white = pd.read_csv(\"../data/winequality-white.csv\", sep=';')\n",
    "dfw = df_white.as_matrix()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dfw[:,:-1])\n",
    "y = dfw[:,-1]\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "Xtra, Xval, ytra, yval = train_test_split(X, y.reshape(-1, 1), train_size=0.8, random_state=20170421)\n",
    "\n",
    "print('Shapes:', Xtra.shape, Xval.shape, ytra.shape, yval.shape)\n",
    "print('Target:', y.min(), y.max(), y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing()\n",
    "\n",
    "# print('Description')\n",
    "# print('-----------')\n",
    "# print(housing.DESCR)\n",
    "# print('Features')\n",
    "# print('--------')\n",
    "# print(housing.feature_names)\n",
    "# print()\n",
    "# print('Shapes')\n",
    "# print('------')\n",
    "# print(housing.data.shape, housing.target.shape)\n",
    "# print()\n",
    "\n",
    "# m, n = housing.data.shape\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "\n",
    "# housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "# X = housing_data_plus_bias / housing_data_plus_bias.max(0)\n",
    "# y = housing.target\n",
    "\n",
    "# Xtra, Xval, ytra, yval = train_test_split(X, y.reshape(-1, 1), train_size=0.8, random_state=20170412)\n",
    "\n",
    "# print('Shapes:', Xtra.shape, Xval.shape, ytra.shape, yval.shape)\n",
    "# print('Target:', y.min(), y.max(), y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementação matricial\n",
    "### O código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57690004466e-10\n",
      "6.20300197406e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs, fmin_ncg, fmin_tnc\n",
    "from scipy.optimize.tnc import RCSTRINGS\n",
    "\n",
    "class BackPropNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layer_sizes=[], is_classifier=True, lamb=0.0):\n",
    "        self.L = len(layer_sizes)\n",
    "        self.s = layer_sizes\n",
    "        self.Theta = None\n",
    "        self.classifier = is_classifier\n",
    "        self.set_lambda(lamb)\n",
    "\n",
    "    def save(self, filename):\n",
    "        import pickle as pickle\n",
    "        data = {\n",
    "            'L':      self.L,\n",
    "            's':      self.s,\n",
    "            'Theta' : self.Theta,\n",
    "            'lambda': self.lambda_,\n",
    "        }\n",
    "        pickle.dump(data, open(filename, 'w'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        import pickle as pickle\n",
    "        data = pickle.load(open(filename, 'r'))\n",
    "        self.L = data['L']\n",
    "        self.s = data['s']\n",
    "        self.Theta = data['Theta']\n",
    "        self.lambda_ = data['lambda']\n",
    "        \n",
    "    def init_theta(self, epsilon=None):\n",
    "        from numpy.random import rand\n",
    "        self.Theta = []\n",
    "        for i in range(self.L-1):\n",
    "            if epsilon is None:\n",
    "                eps = np.sqrt(6.0 / (self.s[i] + self.s[i+1]))\n",
    "            else:\n",
    "                eps = epsilon\n",
    "            self.Theta.append(2*eps*rand(self.s[i+1], self.s[i]+1) - eps)\n",
    "        return self._unroll(self.Theta)\n",
    "\n",
    "    def set_lambda(self, value=None):\n",
    "        if value is not None:\n",
    "            self.lambda_ = value\n",
    "        return self.lambda_\n",
    "        \n",
    "    def tr_newton(self, X, y, nfeval=200, pgtol=-1, xtol=-1, ftol=-1, disp=1):\n",
    "        thetas = self._unroll(self.Theta)\n",
    "        thetas, nfeval, rc = fmin_tnc(self.compute_cost_and_gradient,\n",
    "                                      thetas, fprime=None, args=(X, y),\n",
    "                                      disp=disp, maxfun=nfeval, xtol=xtol,\n",
    "                                      ftol=ftol, pgtol=pgtol)\n",
    "        self.Theta = self._roll(thetas)\n",
    "        return nfeval, RCSTRINGS[rc]\n",
    "\n",
    "    def sgd(self, X, y, lr, momentum, batch, nepochs):\n",
    "        r = nepochs / 10\n",
    "        m, n = X.shape\n",
    "        n_batches = int(np.ceil(m / batch))\n",
    "        \n",
    "        Velocity = [np.zeros_like(th) for th in self.Theta]\n",
    "        for epoch in range(nepochs):\n",
    "            for ii in range(n_batches):\n",
    "                kk = batch * ii\n",
    "                X_batch, y_batch = X[kk:kk+batch], y[kk:kk+batch]\n",
    "                \n",
    "                cost, Delta = self._compute_cost_and_gradient(self.Theta, X, y)\n",
    "                for i in range(len(self.Theta)):\n",
    "                    V = momentum * Velocity[i] - lr * Delta[i]\n",
    "                    Velocity[i] = V\n",
    "                    self.Theta[i] += V\n",
    "                    \n",
    "            if not epoch % r:\n",
    "                print(('{:4d} Cost: {:.5f}'.format(epoch, cost)))\n",
    "        print(('{:4d} Cost: {:.5f}'.format(epoch, cost)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        a = self._compute_activations(self.Theta, X)\n",
    "        return a[-1]\n",
    "    \n",
    "    def compute_cost_and_gradient(self, thetas, X, y):\n",
    "        Theta = self._roll(thetas)\n",
    "        J, Delta = self._compute_cost_and_gradient(Theta, X, y)\n",
    "        return J, self._unroll(Delta)\n",
    "\n",
    "    def _compute_cost_and_gradient(self, Theta, X, y):\n",
    "        M, N  = X.shape\n",
    "        Delta = [None for n in self.s[:-1]]\n",
    "        # Forward propagation\n",
    "        a = self._compute_activations(Theta, X)\n",
    "        # Back propagation\n",
    "        d = self._compute_errors(Theta, a, y)\n",
    "        # Cost computation\n",
    "        if self.classifier:\n",
    "            # classifier: binary cross-entropy\n",
    "            J = - (y * np.log(a[-1]) + (1 - y) * np.log(1 - a[-1])).sum() / M\n",
    "        else:\n",
    "            # regressor: mean squared error\n",
    "            J = 0.5 * np.square(y - a[-1]).sum() / M\n",
    "        for j in range(self.L-1):\n",
    "            # ... add regularization to cost\n",
    "            J += 0.5 * self.lambda_ * (Theta[j][:,1:] * Theta[j][:,1:]).sum() / M\n",
    "            # gradient computation\n",
    "            Delta[j] = np.dot(np.transpose(d[j+1]), a[j]) / M\n",
    "            # ... add regularization to gradient\n",
    "            Delta[j][:,1:] += self.lambda_ * Theta[j][:,1:] / M\n",
    "        return J, Delta\n",
    "\n",
    "    def _compute_activations(self, Theta, X):\n",
    "        a = [None for n in self.s]\n",
    "        a[0] = X\n",
    "        for j in range(1, self.L):\n",
    "            a[j-1] = np.insert(a[j-1], 0, 1, 1)\n",
    "            z = np.dot(a[j-1], np.transpose(Theta[j-1]))\n",
    "            if j == self.L-1 and not self.classifier:\n",
    "                a[j] = z\n",
    "            else:\n",
    "                a[j] = logistic(z)\n",
    "        return a            \n",
    "\n",
    "    def _compute_errors(self, Theta, a, y):\n",
    "        d = [None for n in self.s]\n",
    "        d[-1] = a[-1] - y\n",
    "        for j in range(self.L-2, 0, -1):\n",
    "            d[j] = np.dot(d[j+1], Theta[j]) * a[j] * (1 - a[j])\n",
    "            d[j] = d[j][:,1:]\n",
    "        return d\n",
    "        \n",
    "    def _unroll(self, Theta):\n",
    "        thetas = np.concatenate([t.flat for t in Theta])\n",
    "        return thetas\n",
    "    \n",
    "    def _roll(self, thetas):\n",
    "        Theta, m = [], 0\n",
    "        for l in range(self.L-1):\n",
    "            h, w = self.s[l+1], self.s[l] + 1\n",
    "            n = w * h\n",
    "            Theta.append(thetas[m:m+n].reshape((h,w)))\n",
    "            m += n\n",
    "        return Theta\n",
    "\n",
    "    def compute_approx_gradient(self, thetas, X, y, eps=0.0001):\n",
    "        agrads = np.zeros_like(thetas)    \n",
    "        for i in range(thetas.shape[0]):\n",
    "            tplus = thetas.copy()\n",
    "            tplus[i] += eps\n",
    "            tminus = thetas.copy()\n",
    "            tminus[i] -= eps\n",
    "            Jplus,  G = self.compute_cost_and_gradient(tplus, X, y)\n",
    "            Jminus, G = self.compute_cost_and_gradient(tminus, X, y)\n",
    "            agrads[i] = (Jplus - Jminus) / (2*eps)\n",
    "        return agrads\n",
    "    \n",
    "    def learning_curve(self, Xtrain, ytrain, Xval, yval, lambd, incr=10, nfeval=100, prnt=1):\n",
    "        m, n = Xtrain.shape\n",
    "        J = np.zeros((m/incr,3), np.float64)\n",
    "        for i, n in enumerate(range(incr, m, incr)):\n",
    "            self.init_theta()\n",
    "            self.set_lambda(lambd)\n",
    "            self.train(Xtrain[:n,:], ytrain[:n,:], nfeval=nfeval, disp=0)\n",
    "            J[i,0], G = self._compute_cost_and_gradient(self.Theta, Xtrain[:n,:], ytrain[:n,:])\n",
    "            self.set_lambda(0)\n",
    "            J[i,1], G = self._compute_cost_and_gradient(self.Theta, Xtrain[:n,:], ytrain[:n,:])\n",
    "            J[i,2], G = self._compute_cost_and_gradient(self.Theta, Xval, yval)\n",
    "            if prnt: print((i, n, J[i]))\n",
    "        return J\n",
    "\n",
    "def logistic(z):\n",
    "    z = np.asarray(z)\n",
    "    z = np.minimum(z,  15)\n",
    "    z = np.maximum(z, -15)\n",
    "    return np.ones(z.shape)/(1.0 + np.exp(-z))\n",
    "\n",
    "def gradient_check(lamb, classif):\n",
    "    def norm(a):\n",
    "        return np.sqrt(np.sum(a*a))\n",
    "    nn = BackPropNeuralNetwork([4, 5, 3], lamb=lamb, is_classifier=classif)\n",
    "    thetas = nn.init_theta()\n",
    "    X = np.sin(np.arange(32)).reshape((8, 4)) / 10\n",
    "    y = np.zeros((8, 3))\n",
    "    for i, k in enumerate(np.mod(np.arange(8), 3)):\n",
    "        y[i,k] = 1.0\n",
    "    agrad = nn.compute_approx_gradient(thetas, X, y)\n",
    "    J, grad = nn.compute_cost_and_gradient(thetas, X, y)\n",
    "    diff = norm(agrad-grad) / norm(agrad+grad)\n",
    "    return diff\n",
    "\n",
    "print(gradient_check(0, True))\n",
    "print(gradient_check(0, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [n+1, 40, 20, 1]\n",
    "nepochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Teste com otimizador avançado (*Truncated Newton*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained in 17.602465s\n",
      "\n",
      "MSE: 0.739397318578\n",
      "\n",
      "[ 5.687  6.286  6.781  5.804  5.628  7.941  6.423  6.08   7.18   6.145]\n",
      "[ 6.  6.  7.  5.  6.  8.  7.  5.  8.  6.]\n"
     ]
    }
   ],
   "source": [
    "nnet = BackPropNeuralNetwork(layer_sizes, is_classifier=False)\n",
    "nnet.init_theta();\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    nnet.tr_newton(Xtra, ytra, nepochs)\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "yhat = nnet.predict(Xval)\n",
    "mse = np.square(yhat - yval).mean()\n",
    "print('\\nMSE:', mse)\n",
    "print()\n",
    "print(yhat[:10,0])\n",
    "print(yval[:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Teste com gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 Cost: 16.10587\n",
      " 100 Cost: 0.28633\n",
      " 200 Cost: 0.27427\n",
      " 300 Cost: 0.25655\n",
      " 400 Cost: 0.24620\n",
      " 500 Cost: 0.23956\n",
      " 600 Cost: 0.23460\n",
      " 700 Cost: 0.23040\n",
      " 800 Cost: 0.22677\n",
      " 900 Cost: 0.22361\n",
      " 999 Cost: 0.22084\n",
      "\n",
      "Trained in 17.153527s\n",
      "\n",
      "MSE: 0.480271455107\n",
      "\n",
      "[ 5.628  6.85   6.771  5.363  5.773  6.739  6.748  4.918  6.555  6.607]\n",
      "[ 6.  6.  7.  5.  6.  8.  7.  5.  8.  6.]\n"
     ]
    }
   ],
   "source": [
    "nnet = BackPropNeuralNetwork(layer_sizes, is_classifier=False)\n",
    "nnet.init_theta();\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    nnet.sgd(Xtra, ytra, 0.2, 0.9, 4128, nepochs)\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "yhat = nnet.predict(Xval)\n",
    "mse = np.square(yhat - yval).mean()\n",
    "print('\\nMSE:', mse)\n",
    "print()\n",
    "print(yhat[:10,0])\n",
    "print(yval[:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 40)                480       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00189: early stopping\n",
      "\n",
      "Trained in 2.556202s\n",
      "\n",
      "MSE: 0.485530365783\n",
      "\n",
      "[ 5.475  6.827  6.693  5.29   5.684  6.715  6.773  5.09   6.454  6.409]\n",
      "[ 6.  6.  7.  5.  6.  8.  7.  5.  8.  6.]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "estop = EarlyStopping(patience=50, verbose=1)\n",
    "\n",
    "layer_sizes = [n+1, 40, 20, 1]\n",
    "nepochs = 1000\n",
    "\n",
    "def build():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, activation='sigmoid', input_shape=(n,)))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation=None))\n",
    "    return model\n",
    "\n",
    "nnet = build()\n",
    "nnet.summary()\n",
    "\n",
    "opt = SGD(lr=0.1, momentum=0.9)\n",
    "nnet.compile(loss=\"mse\", optimizer=opt)    \n",
    "\n",
    "Xt = Xtra[:, 1:]\n",
    "Xv = Xval[:, 1:]\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    histo2 = nnet.fit(Xtra[:,1:], ytra, batch_size=516, epochs=nepochs, verbose=0, \n",
    "                      validation_data=(Xval[:,1:], yval), callbacks=[estop])\n",
    "    t1 = time.time()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('\\nTrained in {:2f}s'.format(t1-t0))\n",
    "print()\n",
    "yhat = nnet.predict(Xv, verbose=0)\n",
    "mse = np.square(yhat - yval).mean()\n",
    "print('MSE:', mse)\n",
    "print()\n",
    "print(yhat[:10,0])\n",
    "print(yval[:10,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 54.80%\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "acc = 100.0 * np.where(np.abs(yhat - yval) < 0.5, 1, 0).sum() / yval.shape[0]\n",
    "print('Accuracy: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Equações\n",
    "### Backpropagation\n",
    "----\n",
    "\n",
    "Para a última camada, temos:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}^{(L)}} = \\frac{\\partial J}{\\partial a_i^{(L)}} \\cdot \\frac{\\partial a_i^{(L)}}{\\partial z_i^{(L)}} \\cdot \\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}}$$\n",
    "\n",
    "$$\\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} = a_j^{(L-1)}$$\n",
    "\n",
    "$$\\delta_i^{(L)} = \\frac{\\partial J}{\\partial a_i^{(L)}} \\cdot \\frac{\\partial a_i^{(L)}}{\\partial z_i^{(L)}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}^{(L)}} = \\delta_i^{(L)} \\cdot a_j^{(L-1)}$$\n",
    "\n",
    "Passando para a próxima camada:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}^{(L-1)}} = \\frac{\\partial J}{\\partial a_i^{(L)}} \\cdot \\frac{\\partial a_i^{(L)}}{\\partial z_i^{(L)}} \\cdot \\frac{\\partial z_i^{(L)}}{\\partial a_i^{(L-1)}}  \\cdot \\frac{\\partial a_i^{(L-1)}}{\\partial z_i^{(L-1)}} \\cdot \\frac{\\partial z_i^{(L-1)}}{\\partial w_{ij}^{(L-1)}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}^{(L-1)}} = \\delta_i^{(L)} \\cdot \\frac{\\partial z_i^{(L)}}{\\partial a_i^{(L-1)}} \\cdot \\frac{\\partial a_i^{(L-1)}}{\\partial z_i^{(L-1)}} \\cdot \\frac{\\partial z_i^{(L-1)}}{\\partial w_{ij}^{(L-1)}}$$\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "Para a regressão linear, temos:\n",
    "\n",
    "$$J = \\frac{1}{2} \\sum_{k}^{} (\\hat{y}_k - y_k)^2  \\Rightarrow   \\frac{\\partial J}{\\partial a_i} = a_i - y_i$$ \n",
    "$$a_i = z_i \\Rightarrow \\frac{\\partial a_i}{\\partial z_i} = 1$$\n",
    "\n",
    "Para a regressão logística:\n",
    "\n",
    "$$J = -  \\sum_{k}^{} (y_k \\log{\\hat{y}_k} + (1 - y_k) \\log{(1 - \\hat{y}_k)}) \\Rightarrow \\frac{\\partial J}{\\partial a_i} = \\frac{a_i - y_i}{a_i (1 - a_i)}$$ \n",
    "$$a_i = \\frac{1}{1 + e^{-z_i}} \\Rightarrow \\frac{\\partial a_i}{\\partial z_i} = a_i (1 - a_i)$$\n",
    "\n",
    "Enfim:\n",
    "\n",
    "$$\\delta = a_i - y_i  \\Rightarrow  \\frac{\\partial J}{\\partial w_{ij}} = \\delta x_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Propagação para trás\n",
    "\n",
    "---- \n",
    "\\begin{align*} \n",
    "\\frac{\\partial J}{\\partial W^{(L)}} & = \\frac{\\partial J}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial W^{(L)}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial z^{(L)}}{\\partial W^{(L)}} & = (a^{(L-1)})^T \\\\\n",
    "\\\\\n",
    "\\text{Definindo   } \\delta^{(L)} & = \\frac{\\partial J}{\\partial z^{(L)}} \\\\\n",
    " & = \\frac{\\partial J}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\\\\n",
    " & = \\frac{a^{(L)} - y}{ (1 - a^{(L)}) \\cdot a^{(L)}} \\cdot (1 - a^{(L)}) \\cdot a^{(L)} \\\\\n",
    "\\\\\n",
    "\\delta^{(L)} & = a^{(L)} - y \\\\\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial W^{(L)}} & = \\delta^{(L)} \\cdot (a^{(L-1)})^T\n",
    "\\end{align*}\n",
    "-----\n",
    "\n",
    "Passando para a próxima camada:\n",
    "-----\n",
    "\\begin{align*} \n",
    "\\frac{\\partial J}{\\partial W^{(L-1)}} & = \\frac{\\partial J}{\\partial z^{(L)}} \\cdot \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}  \\cdot \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} \\cdot \\frac{\\partial z^{(L-1)}}{\\partial W^{(L-1)}} \\\\\n",
    " & = (W^{(L)})^T \\cdot \\delta^{(L)} \\circ  (1 - a^{(L-1)}) a^{(L-1)} \\cdot (a^{(L-2)})^T \\\\\n",
    "\\\\\n",
    "\\delta^{(L-1)} & = (W^{(L)})^T \\cdot \\delta^{(L)} \\circ (1 - a^{(L-1)}) a^{(L-1)} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial W^{(L-1)}} & = \\delta^{(L-1)} \\cdot a^{(L-2)}\n",
    "\\end{align*}\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Propagação para trás (Backpropagation)\n",
    "\n",
    "Uma descrição mais detalhada do processo de *backpropagation* pode ser encontrada em \n",
    "<a href=\"https://medium.com/becoming-human/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\">neste artigo</a>\n",
    "\n",
    "Para a última camada, temos:\n",
    "\n",
    "\\begin{align*} \n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L)}} &= \\frac{\\partial J}{\\partial a_i^{(L)}} \\cdot \\frac{\\partial a_i^{(L)}}{\\partial z_i^{(L)}} \\cdot \\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} &= a_j^{(L-1)} \\\\\n",
    "\\\\\n",
    "\\delta_i^{(L)} &= \\frac{\\partial J}{\\partial a_i^{(L)}} \\cdot \\frac{\\partial a_i^{(L)}}{\\partial z_i^{(L)}} = \\frac{a_i^{(L)} - y_i}{a_i^{(L)} (1 - a_i^{(L)})} \\cdot a_i^{(L)} (1 - a_i^{(L)}) \\\\\n",
    "\\\\\n",
    "\\delta_i^{(L)} &= a_i^{(L)} - y_i \\\\\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L)}} &= \\delta_i^{(L)} \\cdot a_j^{(L-1)}\n",
    "\\end{align*}\n",
    "\n",
    "Passando para a próxima camada:\n",
    "\n",
    "\\begin{align*} \n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L-1)}} &= \\frac{\\partial J}{\\partial a_i^{(L-1)}} \\cdot \\frac{\\partial a_i^{(L-1)}}{\\partial z_i^{(L-1)}}  \\cdot  \\frac{\\partial z_i^{(L-1)}}{\\partial w_{ij}^{(L-1)}}\n",
    "\\\\\n",
    "\\frac{\\partial z_i^{(L-1)}}{\\partial w_{ij}^{(L-1)}} &= a_j^{(L-2)}\n",
    "\\\\\n",
    "\\frac{\\partial a_i^{(L-1)}}{\\partial z_i^{(L-1)}} &= a_i^{(L-1)} (1 - a_i^{(L-1)})\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial a_{i}^{(L-1)}} &= \\sum_{k = 0}^{n_L} \\frac{\\partial J}{\\partial z_{k}^{(L)}} \\cdot \\frac{\\partial z_k^{(L)}}{\\partial a_{i}^{(L-1)}} = \\sum_{k = 0}^{n_L} \\delta_k^{L} \\cdot w_{ki}^{L}\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L-1)}} &= \\sum_{k = 0}^{n_L} \\delta_k^{L} \\cdot w_{ik}^{L} \\cdot a_i^{(L-1)} (1 - a_i^{(L-1)}) \\cdot a_j^{(L-2)} \\\\\n",
    "\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "697px",
    "left": "0px",
    "right": "1189px",
    "top": "106px",
    "width": "251px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
